O(''n'') iterations, each costing O(''n'') flops. Thus, the first step is more expensive, and the overall cost is O(''mn''<sup>2</sup>) flops {{harv|Trefethen|Bau III|1997|loc=Lecture 31}}. The first step can be done using [[Householder reflection]]s for a cost of 4''mn''<sup>2</sup> &minus; 4''n''<sup>3</sup>/3 flops, assuming that only the singular values are needed and not the singular vectors. If ''m'' is much larger than ''n'' then it is advantageous to first reduce the matrix ''M'' to a triangular matrix with the [[QR decomposition]] and then use Householder reflections to further reduce the matrix to bidiagonal form; the combined cost is 2''mn''<sup>2</sup> + 2''n''<sup>3</sup> flops {{harv|Trefethen|Bau III|1997|loc=Lecture 31}}. The second step can be done by a variant of the [[QR algorithm]] for the computation of eigenvalues, which was first described by {{harvtxt|Golub|Kahan|1965}}. The [[LAPACK]] subroutine [http://www.netlib.org/lapack/double/dbdsqr.f DBDSQR] implements this iterative method, with some modifications to cover the case where the singular values are very small {{harv|Demmel|Kahan|1990}}. Together with a first step using Householder reflections and, if appropriate, QR decomposition, this forms the [http://www.netlib.org/lapack/double/dgesvd.f DGESVD] routine for the computation of the singular value decomposition. The same algorithm is implemented in the [[GNU Scientific Library]] (GSL). The GSL also offers an alternative method, which uses a one-sided [[Jacobi orthogonalization]] in step 2 {{harv|GSL Team|2007}}. This method computes the SVD of the bidiagonal matrix by solving a sequence of 2-by-2 SVD problems, similar to how the [[Jacobi eigenvalue algorithm]] solves a sequence of 2-by-2 eigenvalue methods {{harv|Golub|Van Loan|1996|loc=§8.6.3}}. Yet another method for step 2 uses the idea of [[divide-and-conquer eigenvalue algorithm]]s {{harv|Trefethen|Bau III|1997|loc=Lecture 31}}. ==Reduced SVDs== In applications it is quite unusual for the full SVD, including a full unitary decomposition of the null-space of the matrix, to be required. Instead, it is often sufficient (as well as faster, and more economical for storage) to compute a reduced version of the SVD. The following can be distinguished for an ''m''&times;''n'' matrix ''M'' of rank ''r'': ===Thin SVD=== :<math>M = U_n \Sigma_n V^{*}. \,</math> Only the ''n'' column vectors of ''U'' corresponding to the row vectors of ''V*'' are calculated. The remaining column vectors of ''U'' are not calculated. This is significantly quicker and more economical than the full SVD if ''n''<<''m''. The matrix ''U''<sub>n</sub> is thus ''m''&times;''n'', Σ<sub>n</sub> is ''n''&times;''n'' diagonal, and ''V'' is ''n''&times;''n''. The first stage in the calculation of a thin SVD will usually be a [[QR decomposition]] of ''M'', which can make for a significantly quicker calculation if ''n''<<''m''. ===Compact SVD=== :<math>M = U_r \Sigma_r V_r^*.</math> Only the ''r'' column vectors of ''U'' and ''r'' row vectors of ''V*'' corresponding to the non-zero singular values Σ<sub>r</sub> are calculated. The remaining vectors of ''U'' and ''V*'' are not calculated. This is quicker and more economical than the thin SVD if ''r''<<''n''. The matrix ''U''<sub>r</sub> is thus ''m''&times;''r'', Σ<sub>r</sub> is ''r''&times;''r'' diagonal, and ''V''<sub>r</sub>* is ''r''&times;''n''. ===Truncated SVD=== :<math>\tilde{M} = U_t \Sigma_t V_t^*.</math> Only the ''t'' column vectors of ''U'' and ''t'' row vectors of ''V*'' corresponding to the ''t'' largest singular values Σ<sub>t</sub> are calculated. The rest of the matrix is discarded. This can be much quicker and more economical than the compact SVD if ''t''<<''r''. The matrix ''U''<sub>t</sub> is thus ''m''&times;''t'', Σ<sub>t</sub> is ''t''&times;''t'' diagonal, and ''V''<sub>t</sub>* is ''t''&times;''n'. Of course the truncated SVD is no longer an exact decomposition of the original matrix ''M'', but as discussed below, the approximate matrix <math>\tilde{M}</math> is in a very useful sense the closest approximation to ''M'' that can be achieved by a matrix of rank ''t''. ==Norms== === Ky Fan norms === The sum of the ''k'' largest singular values of ''M'' is a [[matrix norm]], the [[Ky Fan]] ''k''-norm of ''M'' The first of the Ky Fan norms, the Ky Fan 1-norm is the same as the [[operator norm]] of ''M'' as a linear operator with respect to the Euclidean norms of ''K''<sup>''m''</sup> and ''K''<sup>''n''</sup>. In other words, the Ky Fan 1-norm is the operator norm induced by the standard ''l''<sup>2</sup> Euclidean inner product. For this reason, it is also called the operator 2-norm. One can easily verify the relationship between the Ky Fan 1-norm and singular values. It is true in general, for a bounded operator ''M'' on (possibly infinite dimensional) Hilbert spaces :<math>\| M \| = \| M^* M \|^{\frac{1}{2}}.</math> But, in the matrix case, ''M*M''<sup>½</sup> is a [[normal matrix]], so ||''M* M''||<sup>½</sup> is the largest eigenvalue of ''M* M''<sup>½</sup>, i.e. the largest singular value of ''M''. The last of the Ky Fan norms, the sum of all singular values, is the [[trace class|trace norm]] (also known as the 'nuclear norm'), defined by ||''M''|| = Tr[(''M*M'')<sup>½</sup>] (the diagonal entries of ''M* M'' are the squares of the singular values). === Hilbert–Schmidt norm{{Anchor|Hilbert–Schmidt norm|Hilbert-Schmidt norm|Hilbert–Schmidt|Hilbert-Schmidt}} === The singular values are related to another norm on the space of operators. Consider the [[Hilbert–Schmidt operator|Hilbert–Schmidt]] inner product on the ''n'' &times; ''n'' matrices, defined by <math>\langle M,N\rangle=\operatorname{trace}(N^*M)</math>. So the induced norm is <math>\|M\|=\langle M,M\rangle^{1/2} = \operatorname{trace}(M^*M)^{1/2}</math>. Since trace is invariant under unitary equivalence, this shows :<math>\| M \| = (\sum s_i ^2)^{\frac{1}{2}}</math> where ''s<sub>i</sub>'' are the singular values of ''M''. This is called the '''[[Frobenius norm]]''', '''Schatten 2-norm''', or '''Hilbert–Schmidt norm''' of ''M''. Direct calculation shows that if :<math>\, M = (m_{ij}),</math> the Frobenius norm of ''M'' coincides with :<math>( \sum_{ij} | m_{ij} | ^2 )^{\frac{1}{2}}.</math> == Tensor SVD == There exist two types of tensor decompositions which generalise SVD to multi-way arrays. One decomposition decomposes a tensor into a sum of rank-1 tensors, see [[Candecomp]]-[[PARAFAC]] (CP) algorithm. The CP algorithm should not be confused with a rank-''R'' decomposition but, for a given ''N'', it decomposes a tensor into a sum of ''N'' rank-1 tensors that optimally fit the original tensor. The second type of decomposition computes the orthonormal subspaces associated with the different axes or modes of a tensor (orthonormal row space, column space, fiber space, etc.). This decomposition is referred to in the literature as the [[Tucker decomposition|Tucker3/TuckerM]], ''M''-mode SVD, multilinear SVD and sometimes referred to as a [[Higher-order singular value decomposition|higher-order SVD]] (HOSVD). == 