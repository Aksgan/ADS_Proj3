language)]] **[[sed]] **[[AWK]] ***[[Perl]] (also under C) ==Eiffel based== *[[Eiffel (programming language)|Eiffel]] **[[Cobra (programming language from Cobra Language LLC)|Cobra]] <small>(design by contract)</small> **[[Sather]] **[[Ubercode]] ==Forth based== *[[Forth (programming language)|Forth]] **[[InterPress]] ***[[PostScript]] **[[Joy (programming language)|Joy]] ***[[Factor (programming language)|Factor]] ***[[Cat (programming language)|Cat]] **[[RPL (programming language)|RPL]] (also under Lisp) ==Fortran based== *[[Fortran]] **[[Fortran#FORTRAN II|Fortran II]] ***[[BASIC]] (see also [[Generational_list_of_programming_languages#BASIC_based|BASIC based languages]]) **[[Fortran#FORTRAN IV|Fortran IV]] ***[[Port-a-punch FORTRAN|PORTRAN]] ***[http://csgwww.uwaterloo.ca/sdtp/watfor.html WATFOR] ***[[WATFIV (programming language)|WATFIV]] **[[Fortran#FORTRAN 66|Fortran 66]] ***[[FORMAC (programming language)|FORMAC]] ***[[Ratfor]] **[[Fortran#FORTRAN 77|Fortran 77]] ***WATFOR-77 ***[[Ratfiv]] **[[Fortran#Fortran 90|Fortran 90]] **[[Fortran#Fortran 95|Fortran 95]] ***[[F (programming language)|F]] **[[Fortran#Fortran 2003|Fortran 2003]] **[[ALGOL]] (see also [[Generational_list_of_programming_languages#ALGOL_based|ALGOL based languages]]) ==FP based== *[[FP (programming language)|FP (Function Programming)]] **[[FL (programming language)|FL (Function Level)]] ***[[J (programming language)|J (also under APL)]] **[[FPr (programming language)|FPr]] (also under [[Lisp (programming language)|Lisp]] and [[object-oriented programming]]) ==HyperTalk based== *[[HyperTalk]] **[[AppleScript]] **[[Revolution (programming language)|Revolution]] ==JAVA based== *[[OptimJ]] *[[Ateji PX]] *[[Groovy (programming language)|Groovy]] *[[Scala (programming language)|Scala]] ==JOSS based== *[[JOSS]] **[[CAL (Joss family)|CAL]] **[[TELCOMP]] **[[FOCAL (programming language)|FOCAL]] **[[MUMPS]] ***[[Caché ObjectScript]] ==Lisp based== *[[Lisp (programming language)|Lisp]] **[[Arc (programming language)|Arc]] **[[AutoLISP]] **[[Clojure]] **[[Common Lisp]] **[[Emacs Lisp]] **[[K (programming language)|K]] (also under APL) **[[Logo (programming language)|Logo]] **[[Nu (programming language)|Nu programming language]] **[[REBOL]] **[[RPL (programming language)|RPL]] (also under Forth) **[[S (programming language)|S]] ***[[R (programming language)|R]] ****[[PCASTL]] (also under ALGOL) **[[Scheme (programming language)|Scheme]] ***[[GNU Guile]] ***[[Racket (programming language)|Racket]] ***[[Hop (software)|Hop]] ***[[Pico (programming language)|Pico]] ***[[T (programming language)|T]] ***[[Lua (programming language)|Lua]] (also under Modula and SNOBOL) ==ML based== *[[ML (programming language)|ML]] **[[Standard ML]] (SML) ***[[Alice (programming language)|Alice]] **[[Caml]] ***[[Objective Caml]] (OCaml) ***[[F Sharp (programming language)|F#]] **[[MCPL]] (also under BCPL) **[[Mythryl]] ==PL based== *[[PL/I]] **[[PL/M]] ***[[PL/M-86 (programming language)|PL/M-86]] **[[PL/C]] **[[REXX]] **[[SP/k]] ==Prolog based== *[[Prolog]] **[[PROLOG II]], III, IV **[[CLP(R)]], CLP(FD) **[[Mercury (programming language)|Mercury]] **[[Erlang (programming language)|Erlang]] **[[Logtalk]] ==SASL Based== *[[SASL (programming language)|SASL]] **[[Kent Recursive Calculator]] **[[Miranda (programming language)|Miranda]] ***[[Haskell (programming language)|Haskell]] ==SETL based== *[[SETL]] **[[ABC (programming language)|ABC]] ***[[Python (programming language)|Python]] ****[[Boo (programming language)|Boo]] ****[[Cobra (programming language from Cobra Language LLC)|Cobra]] <small>(syntax and features)</small> ==sh based== *[[Bourne shell|Sh]] **[[Bash (Unix shell)|bash]] **[[C shell|csh]] **[[tcsh]] (also under C) **[[z shell|zsh]] **[[Korn shell|ksh]] ***[[Windows PowerShell]] (also under C#, DCL and Perl) ==Simula based== *[[Simula]] (also under ALGOL 60) **[[C++]] (also under C) **[[Smalltalk]] ***[[Objective-C]] <small>(hybrid of C and Smalltalk)</small> ****[[Cobra (programming language from Cobra Language LLC)|Cobra]] <small>(support both dynamic and static types)</small> ***[[Ruby (programming language)|Ruby]] ***[[Self (programming language)|Self]] ****[[JavaScript]] (originally LiveScript) *****[[ActionScript]] *****[[JavaScript OSA]] *****[[ECMAScript]] ******[[JScript]] ******[[HaXe]] ****[[NewtonScript]] *****[[Io (programming language)|Io]] ****[[Slate (programming language)|Slate]] **[[BETA]] ==Tcl based== *[[Tcl]] **[[Tea (programming language)|Tea]] ==Today based== *[[Today (programming language)|Today]] **[[BuildProfessional]] ==Others== *[[Assembly language|Assembly]] *[[Coral 66|CORAL]] *[[Corn (programming language)|Corn]] *[[Curl (programming language)|Curl]] *[[LabVIEW]] *[[occam (programming language)|occam]] *[[Progress (programming language)|Progress]] *[[REFAL]] *[[Seed7]] *[[Text Editor and Corrector|TECO]] *[[TUTOR (programming language)|TUTOR]] *[[XMLmosaic]] ==External links== * [http://merd.sourceforge.net/pixel/language-study/diagram.html Diagram & history of programming languages] {{DEFAULTSORT:Generational List Of Programming Languages}} [[Category:Lists of programming languages]] [[lo:ລາຍຊື່ພາສາຄອມພິວເຕີຕາມເຄົ້າ]] [[ru:Генеалогический список языков программирования]]</text> </page> <page> <id>15006</id> <title>Generative topographic map</title> <text>'''Generative topographic map (GTM)''' is a [[machine learning]] method that is a probabilistic counterpart of the [[self-organizing map]] (SOM), is provably convergent and does not require a shrinking [[Neighbourhood (mathematics)| neighborhood]] or a decreasing step size. It is a [[generative model]]: the data is assumed to arise by first probabilistically picking a point in a low-dimensional space, mapping the point to the observed high-dimensional input space (via a smooth function), then adding noise in that space. The parameters of the low-dimensional probability distribution, the smooth map and the noise are all learned from the training data using the [[expectation-maximization]] (EM) algorithm. GTM was introduced in 1996 in a paper by [[Christopher M. Bishop]], Markus Svensen, and Christopher K. I. Williams. == Details of the algorithm == The approach is strongly related to [[density networks]] which use [[importance sampling]] and a [[multi-layer perceptron]] to form a non-linear [[latent variable model]]. In the GTM the latent space is a discrete grid of points which is assumed to be non-linearly projected into data space. A [[Gaussian noise]] assumption is then made in data space so that the model becomes a constrained [[Gaussian_mixture_model|mixture of Gaussians]]. Then the model's likelihood can be maximized by EM. In theory, an arbitrary nonlinear parametric deformation could be used. The optimal parameters could be found by gradient descent etc. The suggested approach to the nonlinear mapping is to use a [[radial basis function network]] (RBF) to create a nonlinear mapping between the latent space and the data space. The nodes of the RBF network then form a feature space and the nonlinear mapping can then be taken as a [[linear transform]] of this feature space. This approach has the advantage over the suggested density network approach that it can be optimised analytically. == Uses == In data analysis, GTMs are like a nonlinear version of [[principal components analysis]], which allows high dimensional data to be modelled as resulting from Gaussian noise added to sources in lower-dimensional latent space. For example, to locate stocks in plottable 2D space based on their hi-D time-series shapes. Other applications may want to have fewer sources than data points, for example mixture models. In generative [[deformational modelling]], the latent and data spaces have the same dimensions, for example, 2D images or 1 audio sound waves. Extra 'empty' dimensions are added to the source (known as the 'template' in this form of modelling), for example locating the 1D sound wave in 2D space. Further nonlinear dimensions are then added, produced by combining the original dimensions. The enlarged latent space is then projected back into the 1D data space. The probability of a given projection is, as before, given by the product of the likelihood of the data under the Gaussian noise model with the prior on the deformation parameter. Unlike conventional spring-based deformation modelling, this has the advantage of being analytically optimizable. The disadvantage is that it is a 'data-mining' approach, ie. the shape of the deformation prior is unlikely to be meaningful as an explanation of the possible deformations, as it is based on a very high, artificial- and arbitrarily constructed nonlinear latent space. For this reason the prior is learned from data rather than created by a human expert, as is 