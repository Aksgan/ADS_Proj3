f) = \int_a^b \alpha f(x)\, dx</math> ::<math>= \alpha\int_a^b f(x)\, dx = \alpha I(f).</math> === Evaluation === Let ''P''<sub>n</sub> denote the vector space of real-valued polynomials of degree ≤''n'' defined on an interval [''a'',''b'']. If ''c'' &isin; [''a'', ''b''], then let ''ev''<sub>''c''</sub> : ''P''<sub>n</sub> &rarr; '''R''' be the '''evaluation functional''': :<math>ev_c f = f(c).</math> The mapping &fnof; &rarr; &fnof;(''c'') is linear since :<math>(f+g)(c) = f(c) + g(c)</math> :<math>(\alpha f)(c) = \alpha f(c).</math> If ''x''<sub>0</sub>, ..., ''x''<sub>n</sub> are ''n''+1 distinct points in [''a'',''b''], then the evaluation functionals ''ev''<sub>x<sub>i</sub></sub>, ''i''=0,1,...,''n'' form a [[basis of a vector space|basis]] of the dual space of ''P''<sub>n</sub>. ({{harvtxt|Lax|1996}} proves this last fact using [[Lagrange interpolation]].) === Application to quadrature === The integration functional ''I'' defined above defines a linear functional on the [[linear subspace|subspace]] ''P''<sub>n</sub> of polynomials of degree ≤ ''n''. If ''x''<sub>0</sub>, &hellip;, ''x''<sub>''n''</sub> are ''n''+1 distinct points in [''a'', ''b''], then there are coefficients ''a''<sub>0</sub>, &hellip;, ''a''<sub>''n''</sub> for which :<math>I(f) = a_0 f(x_0) + a_1 f(x_1) + \dots + a_n f(x_n)</math> for all ''&fnof;'' &isin; ''P''<sub>''n''</sub>. This forms the foundation of the theory of [[numerical quadrature]]. This follows from the fact that the linear functionals ''ev''<sub>x<sub>i</sub></sub> : ''&fnof;'' &rarr; ''&fnof;''(''x''<sub>''i''</sub>) defined above form a [[basis of a vector space|basis]] of the dual space of ''P''<sub>''n''</sub> {{harv|Lax|1996}}. === Linear functionals in quantum mechanics === Linear functionals are particularly important in [[quantum mechanics]]. Quantum mechanical systems are represented by [[Hilbert space]]s, which are [[antilinear|anti]]-[[linear isomorphism|isomorphic]] to their own dual spaces. A state of a quantum mechanical system can be identified with a linear functional. For more information see [[bra-ket notation]]. === Distributions === In the theory of [[generalized function]]s, certain kinds of generalized functions called [[distribution (mathematics)|distributions]] can be realized as linear functionals on spaces of [[test function]]s. == Properties == * Any linear functional is either trivial (equal to 0 everywhere) or [[surjective]] onto the scalar field. Indeed, this follows since the image of a vector [[linear subspace|subspace]] under a linear transformation is a subspace, so is the image of ''V'' under ''L''. But the only subspaces (i.e., ''k''-subspaces) of ''k'' are {0} and ''k'' itself. * A linear functional is continuous if and only if its [[Kernel (linear operator)|kernel]] is closed {{harv|Rudin|1991|loc=Theorem 1.18}}. * Linear functionals with the same kernel are proportional. == Dual vectors and bilinear forms == {{See also|Hodge dual}} Every non-degenerate [[bilinear form]] on a finite-dimensional space gives rise to an [[isomorphism]] from ''V'' to ''V''*. Specifically, denoting the bilinear form on ''V'' by ( , ) (for instance in [[Euclidean space]] (''v'',''w'') = ''v''•''w'' is the [[dot product]] of ''v'' and ''w''), then there is a natural isomorphism <math>V\to V^*:v\mapsto v^*</math> given by : <math> v^*(w) := \langle v, w\rangle. \, </math> The inverse isomorphism is given by <math>V^* \to V : f \mapsto f^* </math> where &fnof;* is the unique element of ''V'' for which for all ''w'' &isin; ''V'' : <math> \langle f^*, w\rangle = f(w). \, </math> The above defined vector ''v''* &isin; ''V''* is said to be the '''dual vector''' of ''v'' &isin; ''V''. In an infinite dimensional [[Hilbert space]], analogous results hold by the [[Riesz representation theorem]]. There is a mapping ''V'' &rarr; ''V''* into the ''continuous dual space'' ''V''*. However, this mapping is [[antilinear]] rather than linear. ==Visualizing linear functionals== In finite dimensions, a linear function can be visualized in terms of its [[level set]]s. In three dimensions, the level sets of a linear functional are a family of mutually parallel planes; in higher dimensions, they are parallel [[hyperplane]]s. This method of visualizing linear functionals is sometimes introduced in [[general relativity]] texts, such as {{harvtxt|Misner|Thorne|Wheeler|1973}}. ==Bases in finite dimensions== ===Basis of the dual space in finite dimensions=== Let the vector space ''V'' have a basis <math>\vec{e}_1, \vec{e}_2,\dots,\vec{e}_n</math>, not necessarily [[orthogonal]]. Then the [[dual space]] ''V''* has a basis <math>\tilde{\omega}^1,\tilde{\omega}^2,\dots,\tilde{\omega}^n</math> called the [[dual basis]] defined by the special property that :<math> \tilde{\omega}^i (\vec e_j) = \left\{\begin{matrix} 1 &\mathrm{if}\ i=j\\ 0 &\mathrm{if}\ i\not=j.\end{matrix}\right. </math> Or, more succinctly, :<math> \tilde{\omega}^i (\vec e_j) = \delta^i_j </math> where δ is the [[Kronecker delta]]. Here the superscripts of the basis functionals are not exponents but are instead [[covariance and contravariance|contravariant]] indices. A linear functional <math>\tilde{u}</math> belonging to the dual space <math>\tilde{V}</math> can be expressed as a [[linear combination]] of basis functionals, with coefficients ("components") ''u<sub>i</sub>'' , :<math>\tilde{u} = \sum_{i=1}^n u_i \, \tilde{\omega}^i. </math> Then, applying the functional <math>\tilde{u}</math> to a basis vector ''e<sub>j</sub>'' yields :<math>\tilde{u}(\vec e_j) = \sum_{i=1}^n (u_i \, \tilde{\omega}^i) \vec e_j = \sum_i u_i (\tilde{\omega}^i (\vec e_j)) </math> due to linearity of scalar multiples of functionals and pointwise linearity of sums of functionals. Then :<math> \tilde{u}({\vec e}_j) = \sum_i u_i (\tilde{\omega}^i ({\vec e}_j)) = \sum_i u_i \delta^i {}_j = u_j </math> that is :<math>\tilde{u} (\vec e_j) = u_j.</math> This last equation shows that an individual component of a linear functional can be extracted by applying the functional to a corresponding basis vector. === The dual basis and inner product === When the space ''V'' carries an [[inner product]], then it is possible to write explicitly a formula for the dual basis of a given basis. Let ''V'' have (not necessarily orthogonal) basis <math>\vec{e}_1,\dots, \vec{e}_n</math>. In three dimensions (''n'' = 3), the dual basis can be written explicitly :<math> \tilde{\omega}^i(\vec{v}) = {1 \over 2} \, \left\langle { \sum_{j=1}^3\sum_{k=1}^3\epsilon^{ijk} \, (\vec e_j \times \vec e_k) \over \vec e_1 \cdot \vec e_2 \times \vec e_3} , \vec{v} \right\rangle.</math> for ''i''=1,2,3, where <math>\epsilon\,\!</math> is the [[Levi-Civita symbol]] and <math>\langle,\rangle</math> the inner product (or [[dot product]]) on ''V''. In higher dimensions, this generalizes as follows :<math> \tilde{\omega}^i(\vec{v}) = \left\langle \frac{\underset{{}^{1\le i_2<i_3<\dots<i_n\le n}}{\sum}\epsilon^{ii_2\dots i_n}(\star \vec{e}_{i_2}\wedge\dots\wedge\vec{e}_{i_n})}{\star(\vec{e}_1\wedge\dots\wedge\vec{e}_n)}, \vec{v} \right\rangle </math> where <math>\star</math> is the [[Hodge star operator]]. ==See also== *[[Discontinuous linear map]] *[[Positive linear functional]] *[[Bilinear form]] ==References== *{{citation|first1=Richard|last1=Bishop|first2=Samuel|last2=Goldberg|year=1980|title=Tensor Analysis on Manifolds|publisher=Dover Publications|chapter=Chapter 4|isbn=0-486-64039-6}} * {{citation|first=Paul|last=Halmos|authorlink=Paul Halmos|title=Finite dimensional vector spaces|year=1974|publisher=Springer|isbn=0387900934}} * {{citation|authorlink=Peter Lax|first=Peter|last=Lax|title=Linear algebra|year=1996|publisher=Wiley-Interscience|isbn=978-0471111115}} * {{Citation | first=Charles W. | last=Misner | first2=Kip. S. | last2=Thorne | first3=John A. | last3=Wheeler | title=Gravitation | publisher= W. H. Freeman | year=1973 | isbn=0-7167-0344-0 }} *{{Citation | last1=Rudin | 