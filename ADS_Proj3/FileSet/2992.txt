moment of a function, without further explanation, usually refers to the above expression with ''c'' = 0. Usually, except in the special context of the [[Moment problem|problem of moments]], the function ''f''(''x'') will be a [[probability density function]]. The ''n''<sup>th</sup> moment about zero of a probability density function ''f''(''x'') is the [[expected value]] of ''X''<sup>''n''</sup> and is called a ''raw moment'' or ''crude moment''<ref>http://mathworld.wolfram.com/RawMoment.html Raw Moments at Math-world</ref>. The moments about its mean μ are called [[central moment|''central'' moments]]; these describe the shape of the function, independently of [[translation (geometry)|translation]]. If ''f'' is a [[probability density function]], then the value of the integral above is called the ''n''th moment of the [[probability distribution]]. More generally, if ''F'' is a [[cumulative distribution function|cumulative probability distribution function]] of any probability distribution, which may not have a density function, then the ''n''th moment of the probability distribution is given by the [[Riemann–Stieltjes integral]] :<math>\mu'_n = \operatorname{E}(X^n)=\int_{-\infty}^\infty x^n\,dF(x)\,</math> where ''X'' is a [[random variable]] that has this distribution and '''E''' the [[expectation operator]] or mean. When :<math>\operatorname{E}(|X^n|) = \int_{-\infty}^\infty |x^n|\,dF(x) = \infty,\,</math> then the moment is said not to exist. If the ''n''th moment about any point exists, so does (''n'' &minus; 1)th moment, and all lower-order moments, about every point. ===Variance=== The second [[central moment]] about the mean is the [[variance]], the positive square root of which is the [[standard deviation]] ''σ''. ==== Normalized moments ==== The ''normalized'' ''n''th central moment or [[standardized moment]] is the ''n''th central moment divided by ''σ''<sup>''n''</sup>; the normalized ''n''th central moment of ''x'' = E((''x'' &minus; μ)<sup>''n''</sup>)/σ<sup>''n''</sup>. These normalized central moments are [[dimensionless number|dimensionless quantities]], which represent the distribution independently of any linear change of scale. ===Skewness=== The third central moment is a measure of the lopsidedness of the distribution; any symmetric distribution will have a third central moment, if defined, of zero. The normalized third central moment is called the [[skewness]], often γ. A distribution that is skewed to the left (the tail of the distribution is heavier on the left) will have a negative skewness. A distribution that is skewed to the right (the tail of the distribution is heavier on the right), will have a positive skewness. For distributions that are not too different from the [[normal distribution]], the [[median]] will be somewhere near μ &minus; γσ/6; the [[Mode (statistics)|mode]] about μ &minus; γσ/2. ===Kurtosis=== The fourth central moment is a measure of whether the distribution is tall and skinny or short and squat, compared to the normal distribution of the same variance. Since it is the expectation of a fourth power, the fourth central moment, where defined, is always non-negative; and except for a [[degenerate probability distribution|point distribution]], it is always strictly positive. The fourth central moment of a normal distribution is 3σ<sup>4</sup>. The [[kurtosis]] κ is defined to be the normalized fourth central moment minus 3. (Equivalently, as in the next section, it is the fourth [[cumulant]] divided by the square of the variance.) Some authorities<ref name="CasellaBerger">{{cite book | last1 = Casella | first1 = George | last2 = Berger | first2 = Roger L. | authorlink1 = George Casella | authorlink2 = Roger L. Berger | title = Statistical Inference | publisher = [[Duxbury]] | location = Pacific Grove | year = 2002 | edition = 2 | isbn = 0534243126 }}</ref><ref name="BalandaMacGillivray88">{{cite journal | last1 = Ballanda | first1 = Kevin P. | last2 = MacGillivray | first2 = H. L. | title = Kurtosis: A Critical Review | journal = The American Statistician | volume = 42 | issue = 2 | pages = 111–119 | year = 1988 | doi = 10.2307/2684482 | url = http://jstor.org/stable/2684482 | publisher = American Statistical Association}}</ref> do not subtract three, but it is usually more convenient to have the normal distribution at the origin of coordinates. If a distribution has a peak at the mean and long tails, the fourth moment will be high and the kurtosis positive (leptokurtic); and conversely; thus, bounded distributions tend to have low kurtosis (platykurtic). The kurtosis can be positive without limit, but κ must be greater than or equal to γ<sup>2</sup> &minus; 2; equality only holds for [[Bernoulli distribution|binary distributions]]. For unbounded skew distributions not too far from normal, κ tends to be somewhere in the area of γ<sup>2</sup> and 2γ<sup>2</sup>. The inequality can be proven by considering :<math>\operatorname{E} ((T^2 - aT)^2)\,</math> where ''T'' = (''X'' &minus; μ)/σ. This is the expectation of a square, so it is non-negative whatever ''a'' is; on the other hand, it's also a [[quadratic equation]] in ''a''. Its [[discriminant]] must be non-positive, which gives the required relationship. === Mixed moments === '''Mixed moments''' are moments involving multiple variables. Some examples are [[covariance]], co-skewness and co-kurtosis. While there is a unique covariance, there are multiple co-skewnesses and co-kurtoses. === Higher moments === '''High-order moments''' are moments beyond 4th-order moments. The higher the moment, the harder it is to estimate, in the sense that larger samples are required in order to obtain estimates of similar quality.{{Citation needed|date=September 2010}} ==Cumulants== :{{main|cumulant}} The first moment and the second and third ''unnormalized central'' moments are additive in the sense that if ''X'' and ''Y'' are [[statistical independence|independent]] random variables then :<math>\mu_1(X+Y)=\mu_1(X)+\mu_1(Y)\,</math> and :<math>\operatorname{Var}(X+Y)=\operatorname{Var}(X) + \operatorname{Var}(Y)</math> and :<math>\mu_3(X+Y)=\mu_3(X)+\mu_3(Y).\,</math> (These can also hold for variables that satisfy weaker conditions than independence. The first always holds; if the second holds, the variables are called [[correlation|uncorrelated]]). In fact, these are the first three cumulants and all cumulants share this additivity property. == Sample moments == The moments of a population can be estimated using the sample ''k''-th moment :<math>\frac{1}{n}\sum_{i = 1}^{n} X^k_i\,\!</math> applied to a sample ''X''<sub>1</sub>,''X''<sub>2</sub>,..., ''X''<sub>''n''</sub> drawn from the population. It can be shown that the expected value of the sample moment is equal to the ''k''-th moment of the population, if that moment exists, for any sample size ''n''. It is thus an unbiased estimator. ==Problem of moments== The '''[[problem of moments]]''' seeks characterizations of sequences { ''μ''&prime;<sub>''n''</sub> : ''n'' = 1, 2, 3, ... } 