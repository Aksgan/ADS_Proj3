robot</title> <text>{{Wikify|date=July 2010}} A '''rescue robot''' is a [[robot]] that has been designed for the purpose of aiding rescue workers.<ref name="sptimes">[http://www.sptimes.com/2003/03/02/Floridian/Robots_to_the_rescue.shtml Robots to the Rescue] St. Petersburg Times Online - Florida</ref> Common situations that employ rescue robots are mining accidents, urban disasters, hostage situations, and explosions. One notable use of rescue robots was in the search for victims and survivors in the remnants of the [[World Trade Center]] .<ref name="aaai">[http://www.aaai.org/Pressroom/Releases/release-02-0910.php In the Aftermath of September 11 What Roboticists Learned from the Search and Rescue Efforts] A AAAI press release.</ref> The benefits of rescue robots to these operations include reduced personnel requirements, reduced fatigue, and access to otherwise unreachable areas. == References == {{reflist}} == External links == * [http://www.crasar.org/ Center for Robot Assisted Search & Rescue] [[Category:Robots]] [[Category:Robotics]] [[Category:Robotics competitions]] {{Robo-stub}} [[ja:レスキューロボット]]</text> </page> <page> <id>31890</id> <title>Reservation stations</title> <text>{{Unreferenced|date=August 2008}} '''Reservation Stations''' are decentralized features of the [[microarchitecture]] of a [[Central processing unit|CPU]] that allow for [[register renaming]], and are used by the [[Tomasulo algorithm]] for dynamic instruction scheduling. Reservation stations permit the CPU to fetch and re-use a data value as soon as it has been computed, rather than waiting for it to be stored in a register and re-read. When instructions are issued, they can designate the reservation station from which they want their input to read. When multiple instructions need to write to the same register, all can proceed and only the (logically) last one need actually be written. It checks if the operands are available ([[Hazard_(computer_architecture)|RAW]]) and if execution unit is free ([[Structural hazard]]) before starting execution. Instruction are stored with available parameters, and executed when ready. Results are identified by the unit that will execute the corresponding instruction. Implicitly register renaming solves [[Hazard_(computer_architecture)|WAR]] and [[Hazard_(computer_architecture)|WAW]] hazards. Since this is a fully-associative structure, it has a very high cost in comparators (need to compare all results returned from processing units with all stored addresses). In Tomasulo's algorithm, instructions are issued in sequence to Reservation Stations which buffer the instruction as well as the operands of the instruction. If the operand is not available, the Reservation Station listens on a Common Data Bus for the operand to become available. When the operand becomes available, the Reservation Station buffers it, and the execution of the instruction can begin. Functional Units (such as an adder or a multiplier), each have their own corresponding Reservation Station. The output of the Functional Unit connects to the Common Data Bus, where Reservation Stations are listening for the operands they need. == External links == * [http://www.cs.iastate.edu/~prabhu/Tutorial/title.html] == Bibliography == * ''[http://books.google.com/books?id=pqYl3SWkA64C&pg=PR15&dq=advanced+computer+architecture&hl=en&ei=wwPrTP-mMMfXcb_f9J4P&sa=X&oi=book_result&ct=result&resnum=9&ved=0CFIQ6AEwCA#v=onepage&q=advanced%20computer%20architecture&f=false]'',A Quantitative approach for Computer Architechture [[Category:Instruction processing]] {{Comp-eng-stub}} {{Compu-hardware-stub}} [[ja:Reservation Station]] [[it:Reservation station]]</text> </page> <page> <id>31906</id> <title>Residual sum of squares</title> <text>In [[statistics]], the '''residual sum of squares (RSS)''' is the [[sum]] of squares of [[errors and residuals in statistics|residuals]]. It is also known as the '''sum of squared residuals''' or the '''sum of squared errors of prediction (SSE)'''. It is a measure of the discrepancy between the data and an estimation model. A small RSS indicates a tight fit of the model to the data. In general, [[total sum of squares]] = [[explained sum of squares]] + '''residual sum of squares'''. For a proof of this in the multivariate OLS case, see [[Explained sum of squares#Partitioning in the general OLS model|partitioning in the general OLS model]]. ==One explanatory variable== In a model with a single explanatory variable, RSS is given by :<math>RSS = \sum_{i=1}^n (y_i - f(x_i))^2, </math> where ''y''<sub>''i''</sub> is the ''i'' <sup>th</sup> value of the variable to be predicted, ''x''<sub>''i''</sub> is the ''i'' <sup>th</sup> value of the explanatory variable, and <math>f(x_i)</math> is the predicted value of ''y''<sub>''i''</sub>. In a standard linear simple [[regression model]], <math>y_i = a+bx_i+\varepsilon_i\,</math>, where ''a'' and ''b'' are [[coefficient]]s, ''y'' and ''x'' are the [[regressand]] and the [[regressor]], respectively, and &epsilon; is the [[errors and residuals in statistics|error term]]. The sum of squares of residuals is the sum of squares of [[estimator|estimates]] of &epsilon;<sub>''i''</sub>; that is :<math>RSS = \sum_{i=1}^n (y_i - (\alpha + \beta x_i))^2, </math> where <math>\alpha</math> is the estimated value of the constant term <math>a</math> and <math>\beta</math> is the estimated value of the slope coefficient ''b''. ==Matrix expression for the OLS residual sum of squares== The general regression model with ''n'' observations and ''k'' explanators, the first of which is a constant unit vector whose coefficient is the regression intercept, is :<math> y = X \beta + e</math> where ''y'' is an ''n'' × 1 vector of dependent variable observations, each column of the ''n'' × ''k'' matrix ''X'' is a vector of observations on one of the ''k'' explanators, <math>\beta </math> is a ''k'' × 1 vector of true coefficients, and ''e'' is an ''n''× 1 vector of the true underlying errors. The [[ordinary least squares]] estimator for <math>\beta</math> is :<math> \hat \beta = (X^T X)^{-1}X^T y.</math> The residual vector <math>\hat e</math> is <math>y - X \hat \beta = y - X (X^T X)^{-1}X^T y</math>, so the residual sum of squares <math>\hat e ^T \hat e</math> is, after simplification, :<math> RSS = y^T y - y^T X(X^T X)^{-1} X^T y = y^T [I - X(X^T X)^{-1} X^T] y.</math> ==See also== *[[Sum of squares]] *[[Squared deviations]] *[[Errors and residuals in statistics]] *[[Lack-of-fit sum of squares]] [[Category:Regression analysis]] [[Category:Least squares]] {{statistics-stub}} [[it:Somma dei quadrati residui]]</text> </page> <page> <id>31910</id> <title>Resilient Packet Ring</title> <text>'''Resilient Packet Ring''' ('''RPR'''), also known as '''[[IEEE]] 802.17''', is a standard designed for the optimized transport of data traffic over optical fiber ring networks. It is designed to provide the [[resilience (network)|resilience]] found in [[SONET]]/[[SDH]] networks (50 ms protection) but, instead of setting up circuit oriented connections, provides a packet based transmission, in order to increase the efficiency of [[Ethernet]] and [[Internet Protocol|IP]] services. RPR works on a concept of dual counter rotating rings called ringlets. These ringlets are set up by creating RPR stations at nodes where traffic is supposed to drop, per flow (a flow is the ingress and egress of data traffic). RPR uses MAC ([[Media Access Control]] protocol) messages 