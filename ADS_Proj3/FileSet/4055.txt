related to [[principal component analysis]] and to [[Correspondence analysis]], and in [[signal processing]] and [[pattern recognition]]. It is also used in output-only [[modal analysis]], where the non-scaled [[mode shape]]s can be determined from the singular vectors. Yet another usage is [[latent semantic indexing]] in natural language text processing. The SVD also plays a crucial role in the field of [[Quantum information]], in a form often referred to as the [[Schmidt decomposition]]. Through it, states of two quantum systems are naturally decomposed, providing a necessary and sufficient condition for them to be [[entangled]] : if the rank of the <math>\Sigma</math> matrix is larger than one. One application of SVD to rather large matrices is in [[numerical weather prediction]], where [[Lanczos algorithm|Lanczos method]]s are used to estimate the most linearly quickly growing few perturbations to the central numerical weather prediction over a given initial forward time period &mdash; i.e. the singular vectors corresponding to the largest singular values of the linearized propagator for the global weather over that time interval. The output singular vectors in this case are entire weather systems. These perturbations are then run through the full nonlinear model to generate an [[ensemble forecasting|ensemble forecast]], giving a handle on some of the uncertainty that should be allowed for around the current central prediction. Another application of SVD for daily life is that point in perspective view can be unprojected in a photo using the calculated SVD matrix, this application leads to measuring length (a.k.a. the distance of two unprojected points in perspective photo) by marking out the 4 corner points of known-size object in a single photo. PRuler is a demo to implement this application by taking a photo of a regular credit card == Relation to eigenvalue decomposition == The singular value decomposition is very general in the sense that it can be applied to any ''m'' &times; ''n'' matrix whereas [[eigenvalue decomposition]] can only be applied to certain classes of square matrices. Nevertheless, the two decompositions are related. Given an SVD of ''M'', as described above, the following two relations hold: :<math> M^{*} M = V \Sigma^{*} U^{*}\, U \Sigma V^{*} = V (\Sigma^{*} \Sigma) V^{*}\, </math> :<math> M M^{*} = U \Sigma V^{*} \, V \Sigma^{*} U^{*} = U (\Sigma \Sigma^{*}) U^{*}.\, </math> The right hand sides of these relations describe the eigenvalue decompositions of the left hand sides. Consequently, the squares of the non-zero singular values of ''M'' are equal to the non-zero eigenvalues of either <math>M^{*}M</math> or <math>MM^{*}</math>. Furthermore, the columns of ''U'' (left singular vectors) are eigenvectors of <math>MM^{*}</math> and the columns of ''V'' (right singular vectors) are eigenvectors of <math>M^{*}M</math>. In the special case that ''M'' is a [[normal matrix]], which by definition must be square, the [[Spectral theorem#Finite-dimensional case|spectral theorem]] says that it can be [[Unitary transform|unitarily]] [[Diagonalizable matrix|diagonalized]] using a basis of [[eigenvector]]s, so that it can be written <math>M = U D U^*</math> for a unitary matrix ''U'' and a diagonal matrix ''D''. When ''M'' is a [[Positive-definite matrix|positive semi-definite]], the decomposition <math>M=UDU^*</math> is also a singular value decomposition. However, the eigenvalue decomposition and the singular value decomposition differ for all other matrices ''M'': the eigenvalue decomposition is <math>M=UDU^{-1}</math> where ''U'' is not necessarily unitary and ''D'' is not necessarily positive semi-definite, while the SVD is <math>M=U\Sigma V^*</math> where ''Σ'' is a diagonal positive semi-definite, and ''U'' and ''V'' are unitary matrices that are not necessarily related except through the matrix ''M''. == Existence == An eigenvalue ''λ'' of a matrix is characterized by the algebraic relation ''M u'' = ''λ u''. When ''M'' is [[Hermitian matrix|Hermitian]], a variational characterization is also available. Let ''M'' be a real ''n'' &times; ''n'' [[symmetric matrix]]. Define ''f'' :'''R'''<sup>''n''</sup> → '''R''' by ''f''(''x'') = ''x<sup>T</sup> M x''. By the [[extreme value theorem]], this continuous function attains a maximum at some ''u'' when restricted to the closed unit sphere {||''x''|| ≤ 1}. By the [[Lagrange multipliers]] theorem, ''u'' necessarily satisfies :<math>\nabla f = \nabla \; x^T M x = \lambda \cdot \nabla \; x^T x,</math> where the nabla symbol, <math>\nabla</math>, is the [[del]] operator. A short calculation shows the above leads to ''M u'' = ''λ u'' (symmetry of ''M'' is needed here). Therefore ''λ'' is the largest eigenvalue of ''M''. The same calculation performed on the orthogonal complement of ''u'' gives the next largest eigenvalue and so on. The complex Hermitian case is similar; there ''f''(''x'') = ''x* M x'' is a real-valued function of 2''n'' real variables. Singular values are similar in that they can be described algebraically or from variational principles. Although, unlike the eigenvalue case, Hermiticity, or symmetry, of ''M'' is no longer required. This section gives these two arguments for existence of singular value decomposition. === Based on the spectral theorem === Let ''M'' be an ''m''-by-''n'' matrix with complex entries. ''M*M'' is positive semidefinite and Hermitian. By the [[spectral theorem]], there exists a unitary ''n''-by-''n'' matrix ''V'' such that :<math>V^* M^* M V = \begin{bmatrix} D & 0 \\ 0 & 0\end{bmatrix}</math> where ''D'' is diagonal and positive definite. Partition ''V'' appropriately so we can write :<math>\begin{bmatrix} V_1 ^* \\ V_2 ^*\end{bmatrix} M^* M \begin{bmatrix} V_1 & V_2 \end{bmatrix} = \begin{bmatrix} V_1 ^* M^* M V_1 & V_1 ^* M^* M V_2 \\ V_2 ^* M^* M V_1 & V_2 ^* M^* M V_2 \end{bmatrix} = \begin{bmatrix} D & 0 \\ 0 & 0\end{bmatrix}. </math> Therefore ''V<sub>1</sub>*M*MV<sub>1</sub>'' = ''D'' and ''V<sub>2</sub>*M*MV<sub>2</sub>'' = ''0''. The latter means ''MV<sub>2</sub>'' = ''0''. Also, since ''V'' is unitary, ''V<sub>1</sub>*V<sub>1</sub>'' = ''I'', ''V<sub>2</sub>*V<sub>2</sub>'' = ''I'' and ''V<sub>1</sub>V<sub>1</sub>*'' + ''V<sub>2</sub>V<sub>2</sub>*'' = ''I''. Define :<math>U_1 = M V_1 D^{-1/2}.\!</math> Then :<math>U_1 D^{1/2} V_1^* = M V_1 D^{-1/2} D^{1/2} V_1^* = M . \,</math> We see that this is almost the desired result, except that ''U''<sub>1</sub> and ''V''<sub>1</sub> are not unitary in general, but merely [[Isometry|isometries]]. To finish the argument, one simply has to "fill out" these matrices to obtain unitaries. For example, one can choose ''U''<sub>2</sub> such that :<math>U = \begin{bmatrix} U_1 & U_2 \end{bmatrix}</math> 