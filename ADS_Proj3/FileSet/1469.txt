that all neurons have the same threshold, usually 1). The numbers that annotate arrows represent the weight of the inputs. This net assumes that if the threshold is not reached, zero (not -1) is output. Note that the bottom layer of inputs is not always considered a real neural network layer]] This class of networks consists of multiple layers of computational units, usually interconnected in a feed-forward way. Each neuron in one layer has directed connections to the neurons of the subsequent layer. In many applications the units of these networks apply a sigmoid function as an activation function. The ''[[universal approximation theorem]]'' for neural networks states that every continuous function that maps intervals of real numbers to some output interval of real numbers can be approximated arbitrarily closely by a multi-layer perceptron with just one hidden layer. This result holds only for restricted classes of activation functions, e.g. for the sigmoidal functions. Multi-layer networks use a variety of learning techniques, the most popular being ''[[back-propagation]]''. Here, the output values are compared with the correct answer to compute the value of some predefined error-function. By various techniques, the error is then fed back through the network. Using this information, the algorithm adjusts the weights of each connection in order to reduce the value of the error function by some small amount. After repeating this process for a sufficiently large number of training cycles, the network will usually converge to some state where the error of the calculations is small. In this case, one would say that the network has ''learned'' a certain target function. To adjust weights properly, one applies a general method for non-linear [[Optimization (mathematics)|optimization]] that is called [[gradient descent]]. For this, the derivative of the error function with respect to the network weights is calculated, and the weights are then changed such that the error decreases (thus going downhill on the surface of the error function). For this reason, back-propagation can only be applied on networks with differentiable activation functions. In general, the problem of teaching a network to perform well, even on samples that were not used as training samples, is a quite subtle issue that requires additional techniques. This is especially important for cases where only very limited numbers of training samples are available.<ref name=Balabin_2007>{{cite journal |journal=[[Chemometrics and intelligent laboratory systems|Chemometr Intell Lab]] |volume = 88 |issue = 2 |pages = 183–188 |doi=10.1016/j.chemolab.2007.04.006 |title=Comparison of linear and nonlinear calibration models based on near infrared (NIR) spectroscopy data for gasoline properties prediction |year=2007 |author=Roman M. Balabin, Ravilya Z. Safieva, and Ekaterina I. Lomakina}}</ref> The danger is that the network [[overfitting|overfits]] the training data and fails to capture the true statistical process generating the data. [[Computational learning theory]] is concerned with training classifiers on a limited amount of data. In the context of neural networks a simple [[heuristic]], called [[early stopping]], often ensures that the network will generalize well to examples not in the training set. Other typical problems of the back-propagation algorithm are the speed of convergence and the possibility of ending up in a [[local minimum]] of the error function. Today there are practical solutions <!-- examples? --> that make back-propagation in multi-layer perceptrons the solution of choice for many [[machine learning]] tasks. ==ADALINE== ADALINE stands for '''Ada'''ptive '''Lin'''ear '''E'''lement. It was developed by Professor [[Bernard Widrow]] and his graduate student [[Ted Hoff]] at [[Stanford University]] in 1960. It is based on the McCulloch-Pitts model and consists of a weight, a bias and a summation function. Operation: <math>y_i=wx_i+b</math> Its adaptation is defined through a cost function (error metric) of the residual <math>e=d_i-(b+wx_i)</math> where <math>d_i</math> is the desired input. With the [[Mean squared error|MSE]] error metric <math>E=\frac{1}{2N}\sum_i^N e_i^2</math> the adapted weight and bias become: <math>b=\frac{\sum_i x_i^2\sum_i d_i - \sum_i x_i \sum_i x_i d_i}{N(\sum_i(x_i - \bar x)^2)}</math> and <math>w=\frac{\sum_i(x_i - \bar x)(d_i - \bar d)}{\sum_i(x_i - \bar x)^2}</math> The Adaline has practical applications in the controls area. A single neuron with tap delayed inputs (the number of inputs is bounded by the lowest frequency present and the Nyquist rate) can be used to determine the higher order transfer function of a physical system via the bi-linear z-transform. This is done as the Adaline is, functionally, an adaptive FIR filter. Like the single-layer perceptron, ADALINE has a counterpart in statistical modelling, in this case [[least squares]] [[Regression analysis|regression]]. There is an extension of the Adaline, called the Multiple Adaline (MADALINE) that consists of two or more adalines serially connected. == See also == * [[Feed-forward]] * [[Artificial neural network]] * [[Backpropagation]] * [[Rprop]] ==References== {{reflist|2}} * {{cite journal | first = Peter | last = Auer | coauthors = Harald Burgsteiner; Wolfgang Maass | url = http://www.igi.tugraz.at/harry/psfiles/biopdelta-07.pdf | title = A learning rule for very simple universal approximators consisting of a single layer of perceptrons | journal = Neural Networks | volume = 21 | issue = 5 | pages = 786–795 | year = 2008 | doi = 10.1016/j.neunet.2007.12.036 | pmid = 18249524}} ==External links== * [http://www.emilstefanov.net/Projects/NeuralNetworks.aspx Feedforward neural networks tutorial] * [http://cse.stanford.edu/class/sophomore-college/projects-00/neural-networks/Architecture/feedforward.html Stanford: What's so cool about feed-forward networks?] * [http://wiki.syncleus.com/index.php/DANN:Backprop_Feedforward_Neural_Network Feedforward Neural Network: Example] * [http://media.wiley.com/product_data/excerpt/19/04713491/0471349119.pdf Feedforward Neural Networks: An Introduction] [[Category:Neural networks]]</text> </page> <page> <id>13420</id> <title>Feigenbaum constants</title> <text>{| class="infobox" style ="width: 370px;" | colspan="2" align="center" | {{Irrational numbers}} |- |- |- |- |} The '''Feigenbaum constants''' are two [[mathematical constant]]s named after the mathematician [[Mitchell Feigenbaum]]. Both express ratios in a [[bifurcation diagram]]. The first Feigenbaum constant {{OEIS|id=A006890}}, : <math>\delta = </math> 4.66920160910299067185320382... is the limiting [[ratio]] of each bifurcation interval to the next, or between the diameters of successive circles on the real axis of the [[Mandelbrot set]]. Feigenbaum originally related this number to the period-doubling bifurcations in the [[logistic map]], but also showed it to hold for all one-dimensional maps with a single quadratic maximum. As a consequence of this generality, every chaotic system that corresponds to this description will bifurcate at the same rate. Feigenbaum's constant can be used to predict when chaos will arise in such systems before it ever 