frequently formalized in [[Ontology (computer science)|ontologies]]. Alternatively, ''closed-domain'' might refer to a situation where only a limited type of questions are accepted, such as questions asking for [[descriptive knowledge|descriptive]] rather than [[procedural knowledge|procedural]] information. * ''[[Open domain#References|Open-domain]]'' question answering deals with questions about nearly everything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer. QA is regarded as requiring more complex [[natural language processing]] (NLP) techniques than other types of information retrieval such as [[document retrieval]], thus [[natural language search engine]]s are sometimes regarded as the next step beyond current [[search engine]]s {{Citation needed|date={{CURRENTMONTHNAME}} {{CURRENTYEAR}}}}. ==Architecture== The first QA systems were developed in the 1960s and they were basically natural-language interfaces to [[expert system]]s that were tailored to specific domains. In contrast, current QA systems use text documents as their underlying knowledge source and combine various [[natural language processing]] techniques to search for the answers. Current QA systems <ref>Hirschman, L. & Gaizauskas, R. (2001) [http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=96167 Natural Language Question Answering. The View from Here]. Natural Language Engineering (2001), 7:4:275-300 Cambridge University Press.</ref> typically include a '''question classifier''' module that determines the type of question and the type of answer. After the question is analysed, the system typically uses several modules that apply increasingly complex NLP techniques on a gradually reduced amount of text. Thus, a '''document retrieval module''' uses [[search engine]]s to identify the documents or paragraphs in the document set that are likely to contain the answer. Subsequently a '''filter''' preselects small text fragments that contain strings of the same type as the expected answer. For example, if the question is "Who invented Penicillin" the filter returns text that contain names of people. Finally, an '''answer extraction''' module looks for further clues in the text to determine if the answer candidate can indeed answer the question. ==Question answering methods== QA is very dependent on a good search [[text corpus|corpus]] - for without documents containing the answer, there is little any QA system can do. It thus makes sense that larger collection sizes generally lend well to better QA performance, unless the question domain is orthogonal to the collection. The notion of [[data redundancy]] in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents <ref>Lin, J. (2002). The Web as a Resource for Question Answering: Perspectives and Challenges. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC 2002).</ref>, leading to two benefits: :'''(1)''' By having the right information appear in many forms, the burden on the QA system to perform complex NLP techniques to understand the text is lessened. :'''(2)''' Correct answers can be filtered from [[false positive]]s by relying on the correct answer to appear more times in the documents than instances of incorrect ones. ===Shallow=== Some methods of QA use [[keyword (internet search)|keyword]]-based techniques to locate interesting passages and sentences from the retrieved documents and then filter based on the presence of the desired answer type within that candidate text. Ranking is then done based on [[syntactic]] features such as word order or location and similarity to query. When using massive collections with good data redundancy, some systems use templates to find the final answer in the hope that the answer is just a reformulation of the question. If you posed the question "What is a dog?", the system would detect the substring "What is a X" and look for documents which start with "X is a Y". <!-- wasn't there this really cute thing that searches Google for "X is..."? --> This often works well on simple "[[factoid]]" questions seeking factual tidbits of information such as names, dates, locations, and quantities. ===Deep=== However, in the cases where simple question reformulation or keyword techniques will not suffice, more sophisticated syntactic, semantic and contextual processing must be performed to extract or construct the answer. These techniques might include [[named entity recognition|named-entity recognition]], relation detection, [[coreference]] resolution, [[synonym|syntactic alternations]], [[word sense disambiguation]], [[logic form]] transformation, logical [[inference]]s ([[abductive reasoning|abduction]]) and [[commonsense reasoning]], temporal or spatial reasoning and so on. These systems will also very often utilize world knowledge that can be found in [[ontologies]] such as [[WordNet]], or the [[Suggested Upper Merged Ontology]] (SUMO) to augment the available reasoning resources through semantic connections and definitions. A good example of utilizing semantic connections is the use of [[VerbNet]]'s thematic roles to model verb arguments. The [[Conceptual Graph]] Formalism (CGF), a powerful semantic knowledge representation methodology, is used to model knowledge in documents and questions, and then comparing them using the [[projection]] algorithm defined in CGF <ref>Salloum, W. (2009) [http://adsabs.harvard.edu/abs/2009kam..confE...1S A Question Answering System based on Conceptual Graph Formalism]. In: 2nd International Symposium on Knowledge Acquisition and Modeling (KAM 2009), IEEE CS Press, New Your. [http://wsalloum.com/publications/qa_based_on_cgf.pdf PDF] </ref>. More difficult queries such as ''Why'' or ''How'' questions, hypothetical postulations, spatially or temporally constrained questions, [[Dialog system|dialog]] queries, badly worded or ambiguous questions will all need these types of deeper understanding of the question. Complex or ambiguous document passages likewise need more NLP techniques applied to understand the text. Statistical QA, which introduces statistical question processing and answer extraction modules, is also growing in popularity in the research community. Many of the lower-level NLP tools used, such as [[part-of-speech tagging]], [[parsing]], named-entity detection, sentence boundary detection, and [[document retrieval]], are already available as probabilistic applications. AQ (Answer Questioning) Methodology; introduces a working cycle to the QA methods. This method may be used in conjunction with any of the known or newly founded methods. AQ Method may be used upon perception of a posed question or answer. The means by which it is utilized can be manipulated beyond its primary usage; however, the primary usage is taking an answer and questioning it turning that very answer into a question. Example; A"I like sushi." Q"(Why do) I like sushi(?)" A"The flavor." Q"(What about) the flavor of sushi 