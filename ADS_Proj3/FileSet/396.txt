instead of conditioning an adjusted expectation is computed by a rule that is a generalization of Bayes' rule that is based upon expectation. The use of the word linear in the title refers to de Finetti's arguments that probability theory is a linear theory (de Finetti argued against the more common measure theory approach). ==Example== In Bayes linear statistics, the probability model is only partially specified it is not possible to calculate conditional probability by Bayes' rule. Instead Bayes linear suggests the calculation of an Adjusted Expectation. To conduct a Bayes linear analysis it is necessary to identify some values that you expect to know shortly by making measurements ''D'' and some future value which you would like to know ''B''. Here ''D'' refers to a vector containing data and ''B'' to a vector containing quantities you would like to predict. For the following example ''B'' and ''D'' are taken to be two-dimensional vectors i.e. :<math>B = (Y_1,Y_2),~ D = (X_1,X_2).</math> In order to specify a Bayes linear model it is necessary to supply expectations for the vectors ''B'' and ''D'', and to also specify the correlation between each component of ''B'' and each component of ''D''. For example the expectations are specified as: : <math>E(Y_1)=5,~E(Y_2)=3,~E(X_1)=5,~E(X_2)=3</math> and the covariance matrix is specified as : : <math> \begin{matrix} & X_1 & X_2 & Y_1 & Y_2 \\ X_1 & 1 & u & \gamma & \gamma \\ X_2 & u & 1 & \gamma & \gamma \\ Y_1 & \gamma & \gamma & 1 & v \\ Y_2 & \gamma & \gamma & v & 1 \\ \end{matrix}. </math> The repetition in this matrix, has some interesting implications to be discussed shortly. An adjusted expectation is a linear estimator of the form : <math>c_0 + c_1X_1 + c_2X_2</math> where <math>c_0, c_1</math> and <math>c_2</math> are chosen to minimise the prior expected loss for the observations i.e. <math>Y_1, Y_2</math> in this case. That is for <math>Y_1</math> : <math>E([Y_1 - c_0 - c_1X_1 - c_2X_2]^2)\,</math> where : <math>c_0, c_1, c_2\,</math> are chosen in order to minimise the prior expected loss in estimating <math>Y_1</math> In general the adjusted expectation is calculated with : <math>E_D(X) = \sum^k_{i=0} h_iD_i .</math> Setting <math>h_0, \dots, h_k</math> to minimise : <math>E\left(\left[X-\sum^k_{i=0}h_iD_i\right]^2\right). </math> From a proof provided in (Goldstein and Wooff 2007) it can be shown that: : <math>E_D(X) = E(X) + Cov(X,D)Var(D)^{-1}(D-E(D)) . \,</math> For the case where Var(''D'') is not invertible the [[Moore–Penrose pseudoinverse]] should be used instead. ==See also== *[[Imprecise probability]] == External links== * [http://maths.dur.ac.uk/stats/bayeslin/ Bayes Linear Methods] == References == * Goldstein, M. (1981) ''Revising Previsions: a Geometric Interpretation (with Discussion)''. [[Journal of the Royal Statistical Society]], Series B, 43(2), 105-130 * Goldstein, M. (2006) ''Subjectivism principles and practice''. Bayesian Analysis][http://ba.stat.cmu.edu/journal/2006/vol01/issue03/goldstein.pdf] * Michael Goldstein, David Wooff (2007) ''Bayes Linear Statistics, Theory & Methods'', Wiley. ISBN 978-0-470-01562-9 * de Finetti, B. (1931) "Probabilism: A Critical Essay on the Theory of Probability and on the Value of Science," (translation of 1931 article) in ''Erkenntnis,'' volume 31, September 1989. The entire double issue is devoted to de Finetti's philosophy of probability. * de Finetti, B. (1937) “La Prévision: ses lois logiques, ses sources subjectives,” Annales de l'Institut Henri Poincaré, : - "Foresight: its Logical Laws, Its Subjective Sources," (translation of the [http://www.numdam.org/item?id=AIHP_1937__7_1_1_0 1937 article] in French) in H. E. Kyburg and H. E. Smokler (eds), ''Studies in Subjective Probability,'' New York: Wiley, 1964. * de Finetti, B. (1974) ''Theory of Probability'', (translation by A Machi and [[AFM Smith]] of 1970 book) 2 volumes, New York: Wiley, 1974-5. <!-- marking as stub, as basic explanation of topic not quite right yet --> {{DEFAULTSORT:Bayes Linear Statistics}} [[Category:Bayesian statistics]] [[Category:Probability interpretations]]</text> </page> <page> <id>3476</id> <title>Bayesian average</title> <text>{{Cleanup|date=June 2009}} {{Unreferenced|date=December 2009}} A '''Bayesian average''' is a method of estimating the [[mean]] of a population consistent with [[Bayesian probability|Bayesian interpretation]], where instead of estimating the mean strictly from the available data set, other existing information related to that data set may also be incorporated into the calculation in order to minimize the impact of large deviations, or to assert a default value when the data set is small. For example, in a calculation of an average review score of a book where only two reviews are available, both giving scores of 10, a normal average score would be 10. However, as only two reviews are available, 10 may not represent the true average had more reviews been available. The review site may instead calculate a Bayesian average of this score by adding the average review score of all books in the store to the calculation. For example, by adding five scores of 7 each, the Bayesian average becomes 7.86 instead of 10, which the review site would hope that it will better represent the quality of the book. Note that the additional information incorporated into the mean calculation does not have to be the true prior mean of the larger population, but rather a value subjectively determined by the person calculating the average as relevant and serving the purpose of the calculation. Therefore, the quality of the Bayesian average (in term of representing the data set) is dependent on the judgment of the person doing the calculation. ==Calculation== Calculating the Bayesian average uses the prior mean ''m'' and a constant ''C''. ''C'' is assigned a value that is proportional to the typical data set size. The value is larger when the expected variation between data sets (within the larger population) is small. It is smaller, when the data sets are expected to vary substantially from one another. : <math> \bar{x} = {Cm + \sum_{i=1}^n{x_i} \over C + n} </math> In cases where the averages' relative values are the only result of importance, ''m'' can be replaced with zero. ''C'' can be calculated based on the priors regarding variance between data sets. In circumstances where that kind of rigor is desired, other more expressive measures of [[statistical power]] are likely to be used. As a result, ''C'' is usually assigned a value in an [[ad-hoc]] 