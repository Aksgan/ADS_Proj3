inference]], '''information geometry''' is the study of [[probability]] and [[information theory|information]] by way of [[differential geometry]]. It reached maturity through the work of [[Shun'ichi Amari]] in the 1980s. The book ''Methods of information geometry'',<ref name="AmariBook"/> is currently the ''de facto'' reference book due to its broad coverage of most of the developments attained using the methods of information geometry up to the year 2000. == Introduction == The main tenet of information geometry is that many important structures in [[probability theory]], [[information theory]] and [[statistics]] can be treated as structures in [[differential geometry]] by regarding a [[space of probability distributions]] as a [[differentiable manifold]] endowed with a [[Riemannian metric]] and a family of [[affine connection]]s distinct from the [[Levi-Civita connection|canonical affine connection]].{{Citation needed|date=February 2010}} The e-affine connection and m-affine connection geometrize{{Clarify|date=February 2010}} expectation and maximization, as in the [[expectation-maximization algorithm]]. For example, * The [[Fisher information metric]] is a [[Riemannian metric]].<ref name="AmariBook"/> * The [[Kullback-Leibler divergence]] is one of a family of divergences related to [[dual affine connection]]s.<ref name="AmariBook"> Shun'ichi Amari, Hiroshi Nagaoka - ''Methods of information geometry'', Translations of mathematical monographs; v. 191, American Mathematical Society, 2000 (ISBN 978-0821805312) </ref> * An [[exponential family]] is [[flat submanifold]] under the [[e-affine connection]].<ref name="AmariBook"/> * The [[maximum likelihood]] estimate may be obtained through [[projection (mathematics)|projection]] onto the chosen statistical model using the [[m-affine connection]].<ref name="Amari1985"> Shun'ichi Amari - ''Differential-geometrical methods in statistics'', Lecture notes in statistics, Springer-Verlag, Berlin, 1985. </ref><ref name="AmariBook"/> * The unique existence of maximum likelihood estimate on exponential families is the consequence of the e- and m- connections being dual affine.{{Citation needed|date=February 2010}} * The '''em-algorithm,''' (em here stands for: e-projection, m-projection) which behaves similarly to the canonical [[EM algorithm]] for most cases is, under broad conditions, an iterative dual projection method via the e-connection and m-connection.<ref name="Amari1995"> S. Amari, Information geometry of the EM and em algorithms for neural networks, Neural Networks, vol. 8, 1995, pp. 1379-1408. </ref> * The concepts of accuracy of estimators, in particular the first and third order efficiency of estimators, can be represented in terms of imbedding curvatures of the manifold representing the statistical model and the manifold of representing the estimator (the second order always equals zero after bias correction).{{Citation needed|date=February 2010}} *The higher order asymptotic power of statistical test can be represented using geometric quantities.{{Citation needed|date=February 2010}} The importance of studying statistical structures as geometrical structures lies in the fact that geometric structures are invariant under coordinate transforms. For example, the Fisher information metric is invariant under coordinate transformation.<ref name="AmariBook"/> The statistician [[Ronald Fisher|Fisher]] recognized in the 1920s that there is an intrinsic measure of amount of information for statistical estimators. The [[Fisher information matrix]] was shown by Cramer and Rao to be a Riemannian metric on the space of probabilities,{{Citation needed|date=February 2010}} and became known as Fisher information metric. The mathematician Cencov (Chentsov) proved in the 1960s and 1970s that on the space of probability distributions on a sample space containing at least three points, * There exists a unique intrinsic metric. It is the Fisher information metric.{{Citation needed|date=February 2010}} * There exists a unique one parameter family of affine connections. It is the family of <math>\alpha</math>-affine connections later popularized by Amari.{{Citation needed|date=February 2010}} Both of these uniqueness are, of course, up to the multiplication by a constant. Amari and Nagaoka's study in the 1980s brought all these results together, with the introduction of the concept of [[dual-affine connection]]s, and the interplay among [[Metric (mathematics)|metric]], [[affine connection]] and [[divergence(information geometry)|divergence]].{{Citation needed|date=February 2010}} In particular, * Given a Riemannian metric ''g'' and a family of dual affine connections <math>\Gamma_\alpha</math>, there exists a unique set of dual divergences <math>D_\alpha</math> defined by them. * Given the family of dual divergences <math>D_\alpha</math>, the metric and affine connections can be uniquely determined by second order and third order differentiations. Also, Amari and Kumon showed that asymptotic efficiency of estimates and tests can be represented by geometrical quantities.{{Citation needed|date=February 2010}} == Basic concepts == * Statistical manifold: space of probability distribution, [[statistical model]]. * Point on the manifold: probability distribution. * Coordinates: parameters in the statistical model. * Tangent vector: Fisher score function. * Riemannian metric: Fisher information metric. * Affine connections. * Curvatures: associated with information loss * Information divergence. == Fisher information metric as a Riemannian metric == {{Main|Fisher information metric}} Information geometry makes frequent use of the [[Fisher information metric]]: :<math>g_{jk}=\int \frac{\partial \log p(x,\theta)}{\partial \theta_j} \frac{\partial \log p(x,\theta)}{\partial \theta_k} p(x,\theta)\, dx.</math> Substituting ''i'' = &minus;log(''p'') from [[information theory]], the formula becomes: :<math>g_{jk}=\int \frac{\partial i(x,\theta)}{\partial \theta_j} \frac{\partial i(x,\theta)}{\partial \theta_k} p(x,\theta)\, dx.</math> == History == The history of information geometry is associated with the discoveries of at least the following people, and many others * [[Sir Ronald Aylmer Fisher]] * [[Harald Cramér]] * [[Calyampudi Radhakrishna Rao]] * [[Solomon Kullback]] * [[Richard Leibler]] * [[Claude Shannon]] * [[Imre Csiszár]] * Cencov * [[Bradley Efron]] * Vos * [[Shun'ichi Amari]] * [[Hiroshi Nagaoka]] * Kass * [[Shinto Eguchi]] * [[Ole Barndorff-Nielsen]] * [[Giovanni Pistone]] * [[Bernard Hanzon]] * [[Damiano Brigo]] == Some applications == ===Natural gradient=== An important concept on information geometry is the [[natural gradient]]. The concept and theory of the natural gradient suggests an adjustment to the [[Lyapunov function|energy function]] of a [[learning rule]]. This adjustment takes into account the [[curvature]] of the (prior) [[statistical differential manifold]], by way of the Fisher information metric. This concept has many important applications in [[blind signal separation]], [[neural network]]s, [[artificial intelligence]], and other engineering problems that deal with information.{{Citation needed|date=February 2010}} Experimental results have shown that application of the concept leads to substantial performance gains.{{Citation needed|date=February 2010}} ===Nonlinear filtering=== Other applications concern statistics of stochastic processes and approximate finite dimensional solutions of the [[filtering problem (stochastic processes)]]. As the [[nonlinear filter]]ing problem admits an infinite dimensional solution in general, one can use a geometric structure in the space of probability distributions to project the infinite dimensional filter into an approximate finite dimensional one, leading to the projection filters introduced in 1987 by [[Bernard Hanzon]].{{Citation needed|date=February 2010}} ==See 