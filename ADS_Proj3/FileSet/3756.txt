in their [[compiler]]'s [[register allocation]], making sure it wisely used the larger set available in the MIPS [[instruction set]]. This resulted in reduced complexity of the chip, with one half the total number of registers, while offering potentially higher performance in those cases where a single procedure could make use of the larger visible register space. In the end, with modern compilers, the MIPS design makes better use of its register space even during procedure calls.{{Fact|date=December 2008}} ==See also== * [[Register renaming]] == References == * {{Cite conference | first1 = Mike | last1 = Frantzen | first2 = Mike | last2 = Shuey | title = StackGhost: Hardware Facilitated Stack Protection | booktitle = Proceedings of the 10th Usenix Security Symposium | pages = 55–66 | publisher = [[USENIX]] | date = 2001 | url = http://www.usenix.org/events/sec01/full_papers/frantzen/frantzen_html/node5.html | accessdate = 27 August 2010 }} * {{Cite web | last = Magnusson | first = Peter | authorlink = | title = Understanding stacks and registers in the Sparc architecture(s) | date = April 1997 | url = http://www.sics.se/~psm/sparcstack.html | format = | doi = | accessdate = 27 August 2010 }} {{DEFAULTSORT:Register Window}} [[Category:Digital registers]] [[Category:Instruction processing]] [[de:Registerfenster]] [[fr:Fenêtre de registres]] [[it:Register window]] [[ja:レジスタ・ウィンドウ]]</text> </page> <page> <id>31670</id> <title>Regression analysis</title> <text>In [[statistics]], '''regression analysis''' includes any techniques for modelling and analyzing several variables, when the focus is on the relationship between a [[dependent variable]] and one or more [[independent variable]]s. More specifically, regression analysis helps us understand how the typical value of the dependent variable changes when any one of the independent variables is varied, while the other independent variables are held fixed. Most commonly, regression analysis estimates the [[conditional expectation]] of the dependent variable given the independent variables &mdash; that is, the [[average value]] of the dependent variable when the independent variables are held fixed. Less commonly, the focus is on a [[quantile]], or other [[location parameter]] of the conditional distribution of the dependent variable given the independent variables. In all cases, the estimation target is a [[function (mathematics)|function]] of the independent variables called the '''regression function'''. In regression analysis, it is also of interest to characterize the variation of the dependent variable around the regression function, which can be described by a [[probability distribution]]. Regression analysis is widely used for [[prediction]] and [[forecast]]ing, where its use has substantial overlap with the field of [[machine learning]]. Regression analysis is also used to understand which among the independent variables are related to the dependent variable, and to explore the forms of these relationships. In restricted circumstances, regression analysis can be used to infer [[causality|causal relationships]] between the independent and dependent variables. A large body of techniques for carrying out regression analysis has been developed. Familiar methods such as [[linear regression]] and [[ordinary least squares]] regression are [[parametric statistics|parametric]], in that the regression function is defined in terms of a finite number of unknown [[parameter]]s that are estimated from the [[data]]. [[Nonparametric regression]] refers to techniques that allow the regression function to lie in a specified set of [[function (mathematics)|functions]], which may be [[dimension|infinite-dimensional]]. The performance of regression analysis methods in practice depends on the form of the data-generating process, and how it relates to the regression approach being used. Since the true form of the data-generating process is not known, regression analysis depends to some extent on making assumptions about this process. These assumptions are sometimes (but not always) testable if a large amount of data is available. Regression models for prediction are often useful even when the assumptions are moderately violated, although they may not perform optimally. However, in many applications, especially with small [[effect size|effects]] or questions of [[causality]] based on [[observational study|observational data]], regression methods give misleading results.<ref>David A. Freedman, ''Statistical Models: Theory and Practice'', Cambridge University Press (2005)</ref><ref>R. Dennis Cook; Sanford Weisberg [http://links.jstor.org/sici?sici=0081-1750%281982%2913%3C313%3ACAIAIR%3E2.0.CO%3B2-3 Criticism and Influence Analysis in Regression], ''Sociological Methodology'', Vol. 13. (1982), pp. 313-361</ref> ==History== The earliest form of regression was the [[method of least squares]] (French: ''méthode des moindres carrés''), which was published by [[Adrien Marie Legendre|Legendre]] in 1805,<ref name="Legendre">[[Adrien-Marie Legendre|A.M. Legendre]]. ''Nouvelles méthodes pour la détermination des orbites des comètes'' (1805). “Sur la Méthode des moindres quarrés” appears as an appendix.</ref> and by [[Carl Friedrich Gauss|Gauss]] in 1809.<ref name="Gauss">[[Carl Friedrich Gauss|C.F. Gauss]]. ''Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientum''. (1809)</ref> Legendre and Gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the Sun. Gauss published a further development of the theory of least squares in 1821,<ref name="Gauss2">C.F. Gauss. [http://books.google.co.za/books?id=ZQ8OAAAAQAAJ&printsec=frontcover&dq=Theoria+combinationis+observationum+erroribus+minimis+obnoxiae&as_brr=3#v=onepage&q=&f=false ''Theoria combinationis observationum erroribus minimis obnoxiae'']. (1821/1823)</ref> including a version of the [[Gauss–Markov theorem]]. The term "regression" was coined by [[Francis Galton]] in the nineteenth century to describe a biological phenomenon. The phenomenon was that the heights of descendants of tall ancestors tend to regress down towards a normal average (a phenomenon also known as [[regression toward the mean]]).<ref> {{cite book | last = Mogull | first = Robert G. | title = Second-Semester Applied Statistics | publisher = Kendall/Hunt Publishing Company | year = 2004 | pages = 59 | isbn = 0-7575-1181-3 }}</ref><ref>{{cite journal | last=Galton | first=Francis | journal=Statistical Science | year=1989 | title=Kinship and Correlation (reprinted 1989) | volume=4 | number=2 | url=http://www.jstor.org/stable/2245330 | pages=80–86 | issue=2 | publisher=Institute of Mathematical Statistics}}</ref> For Galton, regression had only this biological meaning,<ref>[[Francis Galton]]. "Typical laws of heredity", Nature 15 (1877), 492-495, 512-514, 532-533. ''(Galton uses the term "reversion" in this paper, which discusses the size of peas.)''</ref><ref>Francis Galton. Presidential address, Section H, Anthropology. (1885) ''(Galton uses the term "regression" in this paper, which discusses the height of humans.)''</ref> but his work was later extended by [[Udny Yule]] and [[Karl Pearson]] to a more general statistical context.<ref>{{cite journal | doi=10.2307/2979746 | last=Yule | first=G. Udny | authorlink=G. Udny Yule | title=On the Theory of Correlation | journal=J. Royal Statist. Soc. | year= 1897 | pages=812&ndash;54 | url=http://www.jstor.org/stable/2979746 | volume=60 | issue=4 | publisher=Blackwell Publishing}}</ref><ref>{{cite journal | doi=10.1093/biomet/2.2.211 | authorlink=Karl Pearson | 