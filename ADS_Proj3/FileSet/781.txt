cycles]] above). However the database system can utilize any CO variant with exactly the same conflicts and voting-deadlock situation, and same resolution. Conflicts can be either ''materialized'' or ''non-materialized'', depending on CO variant used. For example, if [[Commitment ordering#Strict CO (SCO)|SCO]] (below) is used by the distributed database system instead of SS2PL, then the two conflicts in the example are ''materialized'', all local sub-transactions are in ''ready'' states, and vote blocking occurs in the two transactions, one on each node, because of the CO voting rule applied independently on both A and B: due to conflicts <math>T_{2A}=W_{2A}(x)</math> is not voted on before <math>T_{1A}=R_{1A}(x)</math> ends, and <math>T_{1B}=W_{1B}(y)</math> is not voted on before <math>T_{2B}=R_{2B}(y)</math> ends (see [[Commitment ordering#Enforcing global CO|Enforcing global CO]] above), which is a voting-deadlock. Now the ''conflict graph'' has the global cycle (all conflicts are materialized), and again it is resolved by the atomic commitment protocol, and distributed serializability is maintained. Unlikely for a distributed database system, but possible in principle (and occurs in a multi-database), A can employ SS2PL while B employs SCO. In this case the global cycle is neither in the wait-for graph nor in the serializability graph, but still in the ''augmented conflict graph'' (the union of the two). The various combinations are summarized in the following table: {| class="wikitable" style="text-align:center;" |+Voting-deadlock situations |- !Case!! Node<br>A !! Node<br>B !!Possible schedule!!Materialized<br>conflicts<br>on cycle!!Non-<br>materialized<br>conflicts!!<math>T_{1A}</math> =<br><math>R_{1A}(x)</math>!!<math>T_{1B}</math> =<br><math>W_{1B}(y)</math>!!<math>T_{2A}</math> =<br><math>W_{2A}(x)</math>!!<math>T_{2B}</math> =<br><math>R_{2B}(y)</math> |- ! 1 |SS2PL||SS2PL||<math>R_{1A}(x)</math> <math>R_{2B}(y)</math>|| 0 || 2 ||Ready<br>Voted||Running<br>(Blocked)||Running<br>(Blocked)||Ready<br>Voted |- ! 2 |SS2PL|| SCO ||<math>R_{1A}(x)</math> <math>R_{2B}(y)</math> <math>W_{1B}(y)</math>|| 1 || 1 ||Ready<br>Voted ||Ready<br>Vote blocked||Running<br>(Blocked)||Ready<br>Voted |- ! 3 |SCO||SS2PL|| <math>R_{1A}(x)</math> <math>R_{2B}(y)</math> <math>W_{2A}(x)</math> || 1 || 1 ||Ready<br>Voted||Running<br>(Blocked)||Ready<br>Vote blocked||Ready<br>Voted |- ! 4 |SCO||SCO||<math>R_{1A}(x)</math> <math>R_{2B}(y)</math> <math>W_{1B}(y)</math> <math>W_{2A}(x)</math>|| 2 || 0 ||Ready<br>Voted||Ready<br>Vote blocked ||Ready<br>Vote blocked||Ready<br>Voted |} :'''Comments:''' # Conflicts and thus cycles in the ''augmented conflict graph'' are determined by the transactions and their initial scheduling only, independently of the concurrency control utilized. With any variant of CO, any ''global cycle'' (i.e., spans two databases or more) causes a ''voting deadlock''. Different CO variants may differ on whether a certain conflict is ''materialized'' or ''non-materialized''. # Some limited operation order changes in the schedules above are possible, constrained by the orders inside the transactions, but such changes do not change the rest of the table. # As noted above, only case 4 describes a cycle in the (regular) conflict graph which affects serializability. Cases 1-3 describe cycles of locking based global deadlocks (at least one lock blocking exists). All cycle types are equally resolved by the atomic commitment protocol. Case 1 is the common Distributed SS2PL, utilized since the 1980s. However, no research article, except the CO articles, is known to notice this automatic locking global deadlock resolution as of 2009. Such global deadlocks typically have been dealt with by dedicated mechanisms. # Case 4 above is also an example for a typical voting-deadlock when [[Commitment ordering#Distributed optimistic CO (DOCO)|Distributed optimistic CO (DOCO)]] is used (i.e., Case 4 is unchanged when Optimistic CO (OCO; see below) replaces SCO on both A and B): No data-access blocking occurs, and only materialized conflicts exist. ====Hypothetical Multi Single-Threaded Core (MuSiC) environment==== '''Comment:''' While the examples above describe real, recommended utilization of CO, this example is hypothetical, for demonstration only. Certain experimental distributed memory-resident databases advocate multi single-threaded [[Multi-core processor|core]] (MuSiC) transactional environments. "Single-threaded" refers to transaction [[Thread (computer science)|threads]] only, and to ''serial'' execution of transactions. The purpose is possible orders of magnitude gain in performance (e.g., [[Michael Stonebraker#H-Store and VoltDB|H-Store]]<ref name=Stone08>Robert Kallman, Hideaki Kimura, Jonathan Natkins, Andrew Pavlo, Alex Rasin, [[Stanley Zdonik]], Evan Jones, Yang Zhang, Samuel Madden, [[Michael Stonebraker]], John Hugg, Daniel Abadi (2008): [http://portal.acm.org/citation.cfm?id=1454211 "H-Store: A High-Performance, Distributed Main Memory Transaction Processing System"], ''Proceedings of the 2008 VLDB'', pages 1496 - 1499, Auckland, New-Zealand, August 2008.</ref> and [[VoltDB]]) relatively to conventional transaction execution in multiple threads on a same core. In what described below MuSiC is independent of the way the cores are distributed. They may reside in one [[integrated circuit]] (chip), or in many chips, possibly distributed geographically in many computers. In such an environment, if recoverable (transactional) data are partitioned among threads (cores), and it is implemented in the conventional way for distributed CO, as described in previous sections, then DOCO and Strictness exist automatically. However, downsides exist with this straightforward implementation of such environment, and its practicality as a general-purpose solution is questionable. On the other hand tremendous performance gain can be achieved in applications that can bypass these downsides in most situations. '''Comment:''' The MuSiC straightforward implementation described here (which uses, for example, as usual in distributed CO, voting (and transaction thread) blocking in atomic commitment protocol when needed) is for demonstration only, and has '''no connection''' to the implementation in H-Store or any other project. In a MuSiC environment local schedules are ''serial''. Thus both local Optimistic CO (OCO; see below) and the ''Global CO enforcement voting strategy'' condition for the atomic commitment protocol (see [[Commitment ordering#Enforcing global CO|''The Voting Strategy for Global CO Enforcing Theorem'']] above) are met automatically. This results in both distributed CO compliance (and thus distributed serializability) and automatic global (voting) deadlock resolution. Furthermore, also local ''Strictness'' follows automatically in a serial schedule. By Theorem 5.2 in ([[#Raz1992|Raz 1992]]; page 307), when the CO voting strategy is applied, also Global Strictness is guaranteed. Note that ''serial'' locally is the only mode that allows strictness and "optimistic" (no data access blocking) together. The following is concluded: * '''The MuSiC Theorem''' :In MuSiC environments, if recoverable (transactional) data are partitioned among cores (threads), then both :#''OCO'' (and implied ''Serializability''; i.e., DOCO and Distributed serializability) :#''Strictness'' (allowing effective recovery; 1 and 2 implying Strict CO - see SCO below) and :#(voting) ''deadlock resolution'' :automatically exist globally with unbounded scalability in number of cores used. :'''Comment:''' However, two major downsides, which need special handling, may exist: #Local sub-transactions of a global transaction are blocked until commit, which makes the respective cores idle. This reduces core utilization substantially, even if scheduling of the local sub-transactions attempts to execute all of them 