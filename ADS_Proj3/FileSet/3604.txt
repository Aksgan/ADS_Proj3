end-number is "1" then the machine halts (this is easy to construct). State machines are "destructive" of information in the sense that we can't trace backward what they did, or exactly what path they took to arrive at where they are (unless we record all their moves). You can see that in the sequences above. If the state machine is working on "20", how did it get to "20"? We need a Turing machine to tell us. But another argument for a proof says "how could it be otherwise?" The only possibility would be that, somehow, the number just begins to increase to infinity. Is this plausible? And yet we know that for slightly-modified forms of the problem such as { 5*N+3, N/2 } certain numbers fall into never-ending loops. Might there be some number really big N in the { 3*N+1, N/2 } problem that also results in a loop (actually 1 --> 4 --> 2 --> 1 is how the 3*N+1 problem actually ends up if the algorithm isn't halted at 1). ==Notes== {{Reflist}} ==References== * [[G. H. Hardy]] and [[E. M. Wright]], ''An Introduction to the Theory of Numbers'', Fifth Edition, Clarendon Press, Oxford England, 1979, reprinted 2000 with General Index (first edition: 1938). The proofs that e and pi are transcendental are not trivial, but a mathematically-adept reader will be able to wade through them. * [[Alfred North Whitehead]] and [[Bertrand Russell]], ''Principia Mathematica'' to *56, Cambridge at the University Press, 1962, reprint of 2nd edition 1927, first edition 1913. Chap. 2.I. "The Vicious-Circle Principle" p. 37ff, and Chap. 2.VIII. "The Contradictions" p. 60ff. * {{ Citation | last= Turing | first= A.M. | publication-date = 1937 | date = 1936 | title = On Computable Numbers, with an Application to the Entscheidungsproblem | periodical = Proceedings of the London Mathematical Society | series = 2 | volume = 42 | pages = 230–65 | doi= 10.1112/plms/s2-42.1.230 }} (and {{Citation | last = Turing | first = A.M. | publication-date = 1937 | title = On Computable Numbers, with an Application to the Entscheidungsproblem: A correction | periodical = Proceedings of the London Mathematical Society | series = 2 | volume = 43 | pages = 544–6 | doi = 10.1112/plms/s2-43.6.544 | year = 1938 }}). [http://www.turingarchive.org/browse.php/B/12 online version] This is the epochal paper where Turing defines [[Turing machine]]s and shows that it (as well as the [[Entscheidungsproblem]]) is unsolvable. * [[Martin Davis]], ''The Undecidable, Basic Papers on Undecidable Propositions, Unsolvable Problems And Computable Functions'', Raven Press, New York, 1965. Turing's paper is #3 in this volume. Papers include those by Godel, Church, Rosser, Kleene, and Post. * Martin Davis's chapter "What is a Computation" in Lynn Arthur Steen's ''Mathematics Today'', 1978, Vintage Books Edition, New York, 1980. His chapter describes Turing machines in the terms of the simpler [[Post-Turing Machine]], then proceeds onward with descriptions of Turing's first proof and Chaitin's contributions. * [[Andrew Hodges]], ''Alan Turing: The Engima'', Simon and Schuster, New York. Cf Chapter "The Spirit of Truth" for a history leading to, and a discussion of, his proof. * [[Hans Reichenbach]], ''Elements of Symbolic Lo''gic, Dover Publications Inc., New York, 1947. A reference often cited by other authors. * [[Ernest Nagel]] and [[James R. Newman|James Newman]], ''Gödel's Proof'', New York University Press, 1958. * [[Edward Beltrami]], ''What is Random? Chance and Order in Mathematics and Life'', Springer-Verlag New York, Inc., 1999. * [[Torkel Franzén]], ''Godel's Theorem, An Incomplete Guide to Its Use and Abuse'', A.K. Peters, Wellesley Mass, 2005. A recent take on Gödel's Theorems and the abuses thereof. Not so simple a read as the author believes it is. Franzén's (blurry) discussion of Turing's 3rd proof is useful because of his attempts to clarify terminology. Offers discussions of Freeman Dyson's, Stephen Hawking's, Roger Penrose's and Gregory Chaitin's arguments (among others) that use Gödel's theorems, and useful criticism of some philosophic and metaphysical Gödel-inspired dreck that he's found on the web. {{DEFAULTSORT:Proof Of Impossibility}} [[Category:Mathematical logic]] [[Category:Proofs]]</text> </page> <page> <id>30308</id> <title>Proofs involving ordinary least squares</title> <text>{{Unreferenced|date=February 2010}} The purpose of this page is to provide supplementary materials for the [[Ordinary least squares]] article, reducing the load of the main article with mathematics and improving its accessibility, while at the same time retaining the completeness of exposition. == Least squares estimator for ''β'' == Using matrix notation, the sum of squared residuals is given by : <math>S(b) = (y-Xb)'(y-Xb) \,</math> Since this is a quadratic expression and ''S''(''b'') ≥ 0, the global minimum will be found by [[Matrix_calculus#Examples|differentiating]] it with respect to ''b'': : <math> 0 = \frac{dS}{db'}(\hat\beta) = \frac{d}{db'}\bigg(y'y - b'X'y - y'Xb + b'X'Xb\bigg)\bigg|_{b=\hat\beta} = -2X'y + 2X'X\hat\beta</math> By assumption matrix ''X'' has full column rank, and therefore ''X'X'' is invertible and the least squares estimator for ''β'' is given by : <math> \hat\beta = (X'X)^{-1}X'y \, </math> == Unbiasedness of β&#770; == Plug ''y'' = ''Xβ'' + ''ε'' into the formula for <math>\hat\beta</math> and then use the [[Law of iterated expectation]]: : <math>\operatorname{E}[\,\hat\beta] = \operatorname{E}\Big[(X'X)^{-1}X'(X\beta+\varepsilon)\Big] = \beta + \operatorname{E}\Big[(X'X)^{-1}X'\varepsilon\Big] = \beta + \operatorname{E}\Big[\operatorname{E}\Big[(X'X)^{-1}X'\varepsilon|X \Big]\Big] = \beta + \operatorname{E}\Big[(X'X)^{-1}X'\operatorname{E}[\varepsilon|X]\Big] = \beta,</math> where E[''ε''|''X''] = 0 by assumptions of the model. == Expected value of σ&#770;<sup>2</sup> == First we will plug in the expression for ''y'' into the estimator, and use the fact that ''X'M'' = ''MX'' = 0 (matrix ''M'' projects onto the space orthogonal to ''X''): : <math> \hat\sigma^2 = \tfrac{1}{n}y'My = \tfrac{1}{n} (X\beta+\varepsilon)'M(X\beta+\varepsilon) = \tfrac{1}{n} \varepsilon'M\varepsilon </math> Now we can recognize ''ε'Mε'' as a 1×1 matrix, such matrix is equal to its own [[trace]]. This is useful because by properties of trace operator, '''tr'''(''AB'')='''tr'''(''BA''), and we can use this to separate disturbance ''ε'' from matrix ''M'' which is a function of regressors ''X'': : <math> \operatorname{E}\,\hat\sigma^2 = \tfrac{1}{n}\operatorname{E}\big[\operatorname{tr}(\varepsilon'M\varepsilon)\big] = \tfrac{1}{n}\operatorname{tr}\big(\operatorname{E}[M\varepsilon\varepsilon']\big)</math> Using the [[Law of iterated expectation]] this can be written as : <math>\operatorname{E}\,\hat\sigma^2 = \tfrac{1}{n}\operatorname{tr}\Big(\operatorname{E}\big[M\,\operatorname{E}[\varepsilon\varepsilon'|X]\big]\Big) = \tfrac{1}{n}\operatorname{tr}\big(\operatorname{E}[\sigma^2MI]\big) = \tfrac{1}{n}\sigma^2\operatorname{E}\big[ \operatorname{tr}\,M \big] </math> Recall that ''M'' = ''I'' &minus; ''P'' where ''P'' is the projection onto linear space spanned by 