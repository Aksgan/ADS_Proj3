XML document. Xinq then allows that content to be delivered online. Although the original layout and behavior of the website cannot be preserved exactly, Xinq does allow the basic querying and retrieval functionality to be replicated. ===Transactional archiving=== Transactional archiving is an event-driven approach, which collects the actual transactions which take place between a [[web server]] and a [[web browser]]. It is primarily used as a means of preserving evidence of the content which was actually viewed on a particular [[website]], on a given date. This may be particularly important for organizations which need to comply with legal or regulatory requirements for disclosing and retaining information. A transactional archiving system typically operates by intercepting every [[HTTP]] request to, and response from, the web server, filtering each response to eliminate duplicate content, and permanently storing the responses as bitstreams. A transactional archiving system requires the installation of software on the web server, and cannot therefore be used to collect content from a remote website. Examples of commercial transactional archiving software include: * [http://www.projectcomputing.com/products/pageVault/ PageVault] * [http://www.vignette.com/portal/site/us/menuitem.dcbb524431151aaa32189210180141a0/?vgnextoid=1e0395338521b010VgnVCM1000005610140aRCRD&vgnext-selected-menuitem=4b09bdd80b8ff1e8fb3d8010180141a0 Vignette WebCapture] * [http://www.vestigetechnologies.com/ webEcho] ==Difficulties and limitations== ===Crawlers=== Web archives which rely on web crawling as their primary means of collecting the Web are influenced by the difficulties of web crawling: * The [[robots exclusion protocol]] may request crawlers not access portions of a website. Some web archivists may ignore the request and crawl those portions anyway. * Large portions of a web site may be hidden in the [[deep Web]]. For example, the results page behind a web form lies in the deep Web because a crawler cannot follow a link to the results page. * [[Crawler trap]]s (e.g., calendars) may cause a crawler to download an infinite number of pages, so crawlers are usually configured to limit the number of dynamic pages they crawl. However, it is important to note that a native format web archive, i.e. a fully browsable web archive, with working links, media, etc., is only really possible using crawler technology. The Web is so large that crawling a significant portion of it takes a large amount of technical resources. The Web is changing so fast that portions of a website may change before a crawler has even finished crawling it. ===General limitations=== * Some web servers are configured to return different pages to web archiver requests than they would in response to regular browser requests. This is typically done to fool search engines into directing more user traffic to a website, and is often done to avoid accountability, or to provide enhanced content only to those browsers that can display it. Not only must web archivists deal with the technical challenges of web archiving, they must also contend with intellectual property laws. Peter Lyman<ref>Lyman (2002)</ref> states that "although the Web is popularly regarded as a [[public domain]] resource, it is [[copyright]]ed; thus, archivists have no legal right to copy the Web". However [[national library|national libraries]] in many countries do have a legal right to copy portions of the web under an extension of a [[legal deposit]]. Some private non-profit web archives that are made publicly accessible like [[WebCite]] or the [[Internet Archive]] allow content owners to hide or remove archived content that they do not want the public to have access to. Other web archives are only accessible from certain locations or have regulated usage. WebCite cites a recent lawsuit against Google's caching, which [[Google]] won.<ref>[http://www.webcitation.org/faq FAQ] Webcitation.org</ref> ==Aspects of web curation== Web curation, like any digital curation, entails: *Collecting verifiable Web assets *Providing Web asset search and retrieval *Certification of the trustworthiness and integrity of the collection content *Semantic and ontological continuity and comparability of the collection content Thus, besides the discussion on methods of collecting the Web, those of providing access, certification, and organizing must be included. There are a set of popular tools that addresses these curation steps: A suite of tools for Web Curation by [[International Internet Preservation Consortium]]: * [http://crawler.archive.org/ Heritrix - official website] - collecting Web asset * [http://archive-access.sourceforge.net/projects/nutch/ NutchWAX] - search Web archive collections * [http://archive-access.sourceforge.net/projects/wayback/ Wayback (Open source Wayback Machine)] - search and navigate Web archive collections using NutchWax * [http://webcurator.sourceforge.net/manuals.shtml Web Curator Tool] - Selection and Management of Web Collection Other open source tools for manipulating web archives: * [http://code.google.com/p/warc-tools/ WARC Tools] - for creating, reading, parsing and manipulating, web archives programmatically * [http://code.google.com/p/search-tools/ Search Tools] - for indexing and searching full-text and metadata within web archives ==See also== {{Refbegin|2}} * [[Archive]] * [[Archive site]] * [[Digital preservation]] * [[Heritrix]] * [[International Internet Preservation Consortium]] * [[Internet Archive]] [[Wayback Machine]] * [[Library of Congress Digital Library project]] * [[National Digital Information Infrastructure and Preservation Program]] * [[Pandora Archive]] * [http://archive.pt Portuguese Web Archive] * [[Project MINERVA]] * [[UK Web Archiving Consortium]] * [[Web crawling]] * [[WebCite]] * [[Virtual artifact]] {{Refend}} ==Notes== {{Reflist|2}} ==References== {{refbegin}} * {{cite book | last = Brown | first = A. | title = Archiving Websites: a practical guide for information management professionals | publisher = Facet Publishing | place = London | year = 2006 | isbn = 1-85604-553-6}} * {{cite book | last = Brügger | first = N. | title = Archiving Websites. General Considerations and Strategies | publisher = [[The Centre for Internet Research]] | place = Aarhus | year = 2005 | isbn = 87-990507-0-6 | url = http://www.cfi.au.dk/en/publications/cfi}} * {{cite journal | author = Day, M. | title = Preserving the Fabric of Our Lives: A Survey of Web Preservation Initiatives | journal = Research and Advanced Technology for Digital Libraries: Proceedings of the 7th European Conference (ECDL) | year = 2003 | pages = 461–472 | url = http://www.ukoln.ac.uk/metadata/presentations/ecdl2003-day/day-paper.pdf}} * {{cite journal| author = Eysenbach, G. and Trudel, M. | year = 2005 | title = Going, going, still there: using the WebCite service to permanently archive cited web pages | url = http://www.jmir.org/2005/5/e60/ | journal = Journal of Medical Internet Research | volume = 7 | issue = 5 | doi = 10.2196/jmir.7.5.e60| pages = e60| pmid 