in support of the domain area. (http://www.featuredrivendevelopment.com/files/fddprocessesA4.pdf) |- | | <span id="SMALL GROUP MODEL">SMALL GROUP MODEL</span> | | A model for the domain area made by a group of no more than three. (http://www.featuredrivendevelopment.com/files/fddprocessesA4.pdf) |- | | <span id="SUBJECT AREA">SUBJECT AREA</span> | | An area of major interest or importance to the enterprise. It is centered on a major resource, product or activity. The subject areas provide reference information when conducting detailed requirements gathering. (www.georgetown.edu/uis/ia/dw/GLOSSARY0816.html) |- | | <span id="TEAM MODEL">TEAM MODEL</span> | | A proposed model selected by the modeling team or composed by merging ideas from the proposed models. (http://www.featuredrivendevelopment.com/files/fddprocessesA4.pdf) |- | | <span id="TODO TASK LIST">TODO TASK LIST</span> | | Wiki: [[Todo list]] |} {{Software Engineering}} [[Category:Project management]] [[Category:Agile software development]] [[de:Feature Driven Development]] [[it:Feature Driven Development]] [[nl:Feature Driven Development]] [[ja:ユーザー機能駆動開発]] [[pl:Feature Driven Development]] [[pt:Feature Driven Development]]</text> </page> <page> <id>13375</id> <title>Feature extraction</title> <text>In [[pattern recognition]] and in [[image processing]], '''feature extraction''' is a special form of [[dimensionality reduction]]. When the input data to an algorithm is too large to be processed and it is suspected to be notoriously redundant (much data, but not much information) then the input data will be transformed into a reduced representation set of features (also named features vector). Transforming the input data into the set of features is called ''feature extraction''. If the features extracted are carefully chosen it is expected that the features set will extract the relevant information from the input data in order to perform the desired task using this reduced representation instead of the full size input. ==General== Feature extraction involves simplifying the amount of resources required to describe a large set of data accurately. When performing analysis of complex data one of the major problems stems from the number of variables involved. Analysis with a large number of variables generally requires a large amount of memory and computation power or a [[statistical classification|classification]] algorithm which [[overfitting|overfits]] the training sample and generalizes poorly to new samples. Feature extraction is a general term for methods of constructing combinations of the variables to get around these problems while still describing the data with sufficient accuracy. Best results are achieved when an expert constructs a set of application-dependent features. Nevertheless, if no such expert knowledge is available general dimensionality reduction techniques may help. These include: * [[Principal components analysis]] * [[Semidefinite embedding]] * [[Multifactor dimensionality reduction]] * [[Nonlinear dimensionality reduction]] * [[Isomap]] * [[Kernel PCA]] * [[Latent semantic analysis]] * [[Partial least squares]] * [[Independent component analysis]] ==Image processing== It can be used in the area of [[image processing]] which involves using [[algorithm]]s to detect and isolate various desired portions or shapes (features) of a [[digitized image]] or [[video stream]]. It is particularly important in the area of [[optical character recognition]] <!--The following is just a list of subject headings to be filled in later *Template operations & Fourier Transforms--> ===Low-level=== * [[Edge detection]] * [[Corner detection]] * [[Blob detection]] * [[Ridge detection]] * [[Scale-invariant feature transform]] ====Curvature==== *Edge direction, changing intensity, [[autocorrelation]]. ====Image motion==== *[[Motion detection]]. Area based, differential approach. [[Optical flow]]. === Shape Based === * [[Thresholding (image processing)|Thresholding]] * [[Blob extraction]] * [[Template matching]] * [[Hough transform]] ** Lines ** Circles/Ellipse ** Arbitrary shapes (Generalized Hough Transform) ===Flexible methods=== *Deformable, parameterized shapes *Active contours (snakes) ==Feature extraction in software== Many [[:Category:Data_analysis_software|data analysis software systems]] provide for feature extraction and dimension reduction. Common numerical programming environments such as [[MATLAB]], [[SciLab]], [[NumPy]] and [[R (programming language)|the R language]] provide some of the simpler feature extraction techniques (e.g. [[Principal Components Analysis]]) via built-in commands. More specific algorithms are often available as publicly-available scripts or third-party add-ons. ==References== Kohonen self-organising map (KSOM) extracted features for enhancing MLP-ANN prediction models of BOD~5 Author Rustum, R. Adeloye, A. Simala, A. Journal title IAHS PUBLICATION Bibliographic details 2007, NUMB 314, pages 181-187. http://direct.bl.uk/bld/PlaceOrder.do?UIN=214120420&ETOC=RN&from=searchengine ==See also== *[[Cluster analysis]] *[[Dimensionality reduction]] *[[Feature detection]] *[[Feature selection]] *[[Data mining]] *[[Connected component labeling]] *[[Segmentation (image processing)]] * [http://rapid-i.com/content/view/26/201/lang,en/ RapidMiner] and [http://spl.utko.feec.vutbr.cz/en/component/content/article/46-image-processing-extension-for-rapidminer-5 Image Processing Extension for Rapidminer] (open-source tools) {{unreferenced|date=November 2010}} ==External links== * [http://jmlr.csail.mit.edu/papers/special/feature03.html JMLR Special Issue on Variable and Feature Selection] [[Category:Computer vision]] [[Category:Machine learning]] [[Category:Dimension reduction]] {{compu-graphics-stub}} [[ar:استخلاص المميزات]] [[fa:استخراج ویژگی]] [[pt:Extração de características]]</text> </page> <page> <id>13380</id> <title>Feature selection</title> <text>In [[machine learning]] and [[statistics]], '''feature selection''', also known as '''variable selection''', '''feature reduction''', '''attribute selection''' or '''variable subset selection''', is the technique of selecting a subset of relevant features for building robust learning models. When applied in [[biology]] domain, the technique is also called [[discriminative gene selection]], which detects influential [[genes]] based on [[DNA microarray]] experiments. By removing most irrelevant and redundant features from the data, feature selection helps improve the performance of learning models by: :* Alleviating the effect of the [[curse of dimensionality]]. :* Enhancing generalization capability. :* Speeding up learning process. :* Improving model interpretability. Feature selection also helps people to acquire better understanding about their data by telling them which are the important features and how they are related with each other. ==Introduction== Simple feature selection algorithms are ad hoc, but there are also more methodical approaches. From a theoretical perspective, it can be shown that optimal feature selection for [[supervised learning]] problems requires an exhaustive search of all possible subsets of features of the chosen cardinality. If large numbers of features are available, this is impractical. For practical supervised learning algorithms, the search is for a satisfactory set of features instead of an optimal set. Feature selection algorithms typically fall into two categories: feature ranking and subset selection. Feature ranking ranks the features by a metric and eliminates all features that do not achieve an adequate score. Subset selection searches the set of possible features for the optimal subset. In statistics, the most popular form of feature selection is [[stepwise regression]]. It is a greedy algorithm that adds the best feature (or deletes the worst feature) at each round. The main control issue is deciding when to stop the algorithm. In machine learning, this is typically done by [[Cross-validation (statistics)|cross-validation]]. In statistics, some 