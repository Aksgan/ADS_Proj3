coin" or [[Lebesgue measure]] - on the space of infinite binary sequences) is random. Also, since it can be shown that the Kolmogorov complexity relative to two different universal machines differs by at most a constant, the collection of random infinite sequences does not depend on the choice of universal machine (in contrast to finite strings). This definition of randomness is usually called ''Martin-Löf'' randomness, after [[Per Martin-Löf]], to distinguish it from other similar notions of randomness. It is also sometimes called ''1-randomness'' to distinguish it from other stronger notions of randomness (2-randomness, 3-randomness, etc.). (Related definitions can be made for alphabets other than the set <math>\{0,1\}</math>.) ==Specific Sequence== Algorithmic information theory (AIT) is the information theory of individual objects, using computer science, and concerns itself with the relationship between computation, information, and randomness. The information content or complexity of an object can be measured by the length of its shortest description. For instance the string <code>"0101010101010101010101010101010101010101010101010101010101010101"</code> has the short description "32 repetitions of '01'", while <code>"1100100001100001110111101110110011111010010000100101011110010110"</code> presumably has no simple description other than writing down the string itself. More formally, the Algorithmic Complexity (AC) of a string x is defined as the length of the shortest program computes or outputs x, where the program is run on some fixed reference universal computer. A closely related notion is the probability that a universal computer outputs some string x when fed with a program chosen at random. This Algorithmic "Solomon-off" Probability (AP) is key in addressing the old philosophical problem of induction in a formal way. The major drawback of AC and AP are their incomputability. Time-bounded "Levin" complexity penalizes a slow program by adding the logarithm of its running time to its length. This leads to computable variants of AC and AP, and Universal "Levin" Search (US) solves all inversion problems in optimal (apart from some unrealistically large multiplicative constant) time. AC and AP also allow a formal and rigorous definition of randomness of individual strings do not depend on physical or philosophical intuitions about non-determinism or likelihood. Roughly, a string is Algorithmic "Martin-Loef" Random (AR) if it is incompressible in the sense that its algorithmic complexity is equal to its length. AC, AP, and AR are the core sub-disciplines of AIT, but AIT spawns into many other areas. It serves as the foundation of the Minimum Description Length (MDL) principle, can simplify proofs in computational complexity theory, has been used to define a universal similarity metric between objects, solves the Maxwell daemon problem, and many others. == See also == * [[Kolmogorov complexity]] * [[Algorithmically random sequence]] * [[Algorithmic probability]] * [[Chaitin's constant]] * [[Chaitin–Kolmogorov randomness]] * [[Computationally indistinguishable]] * [[Distribution ensemble]] * [[Epistemology]] * [[Invariance theorem]] * [[Limits to knowledge]] * [[Minimum description length]] * [[Minimum message length]] * [[Pseudorandom ensemble]] * [[Pseudorandom generator]] * [[Simplicity theory]] * [[Uniform ensemble]] {{More footnotes|date=June 2010}} == References == <references/> == External links == * [http://www.scholarpedia.org/article/Algorithmic_information_theory Algorithmic Information Theory (Scholarpedia)] * [http://www.cs.auckland.ac.nz/CDMTCS/chaitin/unknowable/ch6.html Chaitin's account of the history of AIT]. == Further reading == * Blum, M. (1967) On the Size of Machines, Information and Control, v. 11, pp. 257–265 * Blum M. (1967a) A Machine-independent Theory of Complexity of Recursive Functions, Journal of the ACM, v. 14, No.2, pp. 322–336 * Burgin, M. (1982) Generalized Kolmogorov complexity and duality in theory of computations, Soviet Math. Dokl., v.25, No. 3, pp. 19–23 * Burgin, M. (1990) Generalized Kolmogorov Complexity and other Dual Complexity Measures, Cybernetics, No. 4, pp. 21–29 * Burgin, M. ''Super-recursive algorithms'', Monographs in computer science, Springer, 2005 * Calude, C.S. (1996) Algorithmic information theory: Open problems, J. UCS, v. 2, pp. 439–441 * Calude, C.S. ''Information and Randomness: An Algorithmic Perspective'', (Texts in Theoretical Computer Science. An EATCS Series), Springer-Verlag, Berlin, 2002 * Chaitin, G.J. (1966) On the Length of Programs for Computing Finite Binary Sequences, J. Association for Computing Machinery, v. 13, No. 4, pp. 547–569 * Chaitin, G.J. (1969) On the Simplicity and Speed of Programs for Computing Definite Sets of Natural Numbers, J. Association for Computing Machinery, v. 16, pp. 407–412 * Chaitin, G.J. (1975) A Theory of Program Size Formally Identical to Information Theory, J. Association for Computing Machinery, v. 22, No. 3, pp. 329–340 * Chaitin, G.J. (1977) Algorithmic information theory, IBM Journal of Research and Development, v.21, No. 4, 350-359 * Chaitin, G.J. ''Algorithmic Information Theory'', Cambridge University Press, Cambridge, 1987 * Kolmogorov, A.N. (1965) Three approaches to the definition of the quantity of information, Problems of Information Transmission, No. 1, pp. 3–11 * Kolmogorov, A.N. (1968) Logical basis for information theory and probability theory, IEEE Trans. Inform. Theory, vol. IT-14, pp. 662–664 * Levin, L. A. (1974) Laws of information (nongrowth) and aspects of the foundation of probability theory, Problems of Information Transmission, v. 10, No. 3, pp. 206–210 * Levin, L.A. (1976) Various Measures of Complexity for Finite Objects (Axiomatic Description), Soviet Math. Dokl., v. 17, pp. 522–526 * Li, M., and Vitanyi, P. ''An Introduction to Kolmogorov Complexity and its Applications'', Springer-Verlag, New York, 1997 * Solomonoff, R.J. (1960) ''A Preliminary Report on a General Theory of Inductive Inference'', Technical Report ZTB-138, Zator Company, Cambridge, Mass. * Solomonoff, R.J. (1964) A Formal Theory of Inductive Inference, Information and Control, v. 7, No. 1, pp. 1–22; No.2, pp. 224–254 * Solomonoff, R.J. (2009) Algorithmic Probability: Theory and Applications, Information Theory and Statistical Learning, Springer NY, Emmert-Streib, F. and Dehmer, M. (Eds), ISBN 978-0-387-84815-0. * Van Lambagen, (1989) Algorithmic Information Theory, Journal for Symbolic Logic, v. 54, pp. 1389–1400 * Zurek, W.H. (1991) Algorithmic Information Content, Church-Turing Thesis, physical entropy, and Maxwell’s demon, in Complexity, Entropy and the Physics of Information, (Zurek, W.H., Ed.) Addison-Wesley, pp. 73–89 * Zvonkin, A.K. and Levin, L. A. (1970) The Complexity of Finite Objects and the Development of the Concepts of Information and Randomness by Means of the Theory of Algorithms, Russian Mathematics Surveys, v. 256, pp. 83–124 {{Statistics}} {{DEFAULTSORT:Algorithmic Information Theory}} [[Category:Algorithmic information theory| ]] [[Category:Information theory]] [[Category:Randomness]] [[ca:Teoria algorísmica de la informació]] [[de:Algorithmische Informationstheorie]] 