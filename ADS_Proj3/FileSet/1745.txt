in the size of the network because, while negotiating the injunction, [[LimeWire]] had staff add remote shutdown of gnutella access and file transfers as some users installed affected versions. When the injunction came into force versions later than 5.5.10 left the network or were disabled as users started them. On November 9, 2010, LimeWire was resurrected by a secret team of developers and dubbed [[Limewire Pirate Edition]]. It was based on LimeWire 5.6 BETA. This version had its server dependencies removed and all the PRO features enabled for free. == Design == To envision how gnutella originally worked, imagine a large circle of users ''(called nodes),'' each of whom have gnutella client software. On initial startup, the client software must [[Bootstrapping (computing)|bootstrap]] and find at least one other node. Various methods have been used for this, including a pre-existing address list of possibly working nodes shipped with the software, using updated web caches of known nodes (called [[Gnutella Web Cache]]s), UDP host caches and, rarely, even [[Internet Relay Chat|IRC]]. Once connected, the client requests a list of working addresses. The client tries to connect to the nodes it was shipped, as well as nodes it receives from other clients, until it reaches a certain quota. It connects to only that many nodes, locally caches the addresses it has not yet tried, and discards the addresses it tried that were invalid. When the user wants to do a search, the client sends the request to each actively connected node. In version 0.4 of the protocol, the number of actively connected nodes for a client was quite small (around 5), so each node then forwarded the request to all its actively connected nodes, and they in turn forwarded the request, and so on, until the packet reached a predetermined number of "hops" from the sender (maximum 7). Since version 0.6, gnutella is a composite network made of leaf nodes and ultra nodes (also called ultrapeers). The leaf nodes are connected to a small number of ultrapeers (typically 3) while each ultrapeer is connected to more than 32 other ultrapeers. With this higher [[outdegree]], the maximum number of "hops" a query can travel was lowered to 4. Leaves and ultrapeers use the Query Routing Protocol to exchange a Query Routing Table (QRT), a table of 64 [[Binary prefix#IEC standard prefixes|Ki]]-slots and up to 2 [[Binary prefix#IEC standard prefixes|Mi]]-slots consisting of hashed keywords. A leaf node sends its QRT to each of the ultrapeers it is connected to, and ultrapeers merge the QRT of all their leaves (downsized to 128 [[Binary prefix#IEC standard prefixes|Ki]]-slots) plus their own QRT (if they share files) and exchange that with their own neighbours. Query routing is then done by hashing the words of the query and seeing whether all of them match in the QRT. Ultrapeers do that check before forwarding a query to a leaf node, and also before forwarding the query to a peer ultra node provided this is the last hop the query can travel. If a search request turns up a result, the node that has the result contacts the searcher. In the classic gnutella protocol, response messages were sent back along the route the query came through, as the query itself did not contain identifying information of the node. This scheme was later revised, so that search results now are delivered over [[User Datagram Protocol]] (UDP) directly to the node that initiated the search, usually an ultrapeer of the node. Thus, in the current protocol, the queries carry the [[IP address]] and port number of either node. This lowers the amount of traffic routed through the gnutella network, making it significantly more scalable. If the user decides to download the file, they negotiate the [[file transfer]]. If the node which has the requested file is not [[Firewall (networking)|firewalled]], the querying node can connect to it directly. However, if the node is firewalled, stopping the source node from receiving incoming connections, the client wanting to download a file sends it a so called "push request" to the server for the remote client to initiate the connection instead (to "push" the file). At first, these push requests were routed along the original chain it used to send the query. This was rather unreliable because routes would often break and routed packets are always subject to flow control. Therefore so called "push proxies" were introduced. These are usually the ultrapeers of a leaf node and they are announced in search results. The client connects to one of these "push proxies" using a HTTP request and the proxy sends a "push request" to leaf on behalf of the client. Normally, it is also possible to send a push request over UDP to the push proxy which is more efficient than using TCP. Push proxies have two advantages: First, ultrapeer-leaf connections are more stable than routes which makes push requests much more reliable. Second, it reduces the amount of traffic routed through the gnutella network. Finally, when a user disconnects, the client software saves the list of nodes that it was actively connected to and those collected from pong packets for use the next time it attempts to connect so that it becomes independent from any kind of bootstrap services. In practice, this method of searching on the gnutella network was often unreliable. Each node is a regular computer user; as such, they are constantly connecting and disconnecting, so the network is never completely stable. Also, the bandwidth cost of searching on gnutella grew exponentially to the number of connected users,<ref>[http://www.darkridge.com/~jpr5/doc/gnutella.html Why Gnutella Can't Scale. No, Really.] February 2001.</ref> often saturating connections and rendering slower nodes useless. Therefore, search requests would often be dropped, and most queries reached only a very small part of the network. This observation identified the gnutella network as an [[Scalability|unscalable]] distributed system, and inspired the development of [[distributed hash table]]s, which are much more scalable but support only exact-match, rather than keyword, search. To address the problems of [[bottleneck (engineering)|bottleneck]]s, gnutella developers implemented 