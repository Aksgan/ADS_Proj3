parity bits for a known [[permutation]] of the payload data, again computed using an RSC convolutional code. Thus, two redundant but different sub-blocks of parity bits are sent with the payload. The complete block has ''m+n'' bits of data with a code rate of ''m/(m+n)''. The [[permutation]] of the payload data is carried out by a device called an [[interleaver]]. Hardware-wise, this turbo-code encoder consists of two identical RSC coders, ะก<sub>1</sub> and C<sub>2</sub>, as depicted in the figure, which are connected to each other using a concatenation scheme, called ''parallel concatenation'': [[Image:turbo encoder.svg]] In the figure, ''M'' is a memory register. The delay line and interleaver force input bits d<sub>k</sub> to appear in different sequences. At first iteration, the input sequence ''d''<sub>k</sub> appears at both outputs of the encoder, ''x''<sub>k</sub> and'' y''<sub>1k</sub> or ''y''<sub>2k</sub> due to the encoder's systematic nature. If the encoders ''C''<sub>1</sub> and ''C''<sub>2</sub> are used respectively in ''n''<sub>1</sub> and ''n''<sub>2</sub> iterations, their rates are respectively equal to :<math>~R_1=\frac{n_1+n_2}{2n_1+n_2}</math>, :<math>~R_2=\frac{n_1+n_2}{2n_2+n_1}</math>. ==The decoder== The decoder is built in the similar way as the above encoder - two elementary decoders are interconnected to each other, but in serial way, not in parallel. The <math>DEC_1</math> decoder operates on lower speed (i.e. <math>R_1</math>), thus, it is intended for the <math>C_1</math> encoder, and <math>DEC_2</math> is for <math>C_2</math> correspondingly. <math>DEC_1</math> yields a [[Turbo code#Soft decision approach|soft decision]] which causes <math>L_1</math> delay. The same delay is caused by the delay line in the encoder. The <math>DEC_2</math>'s operation causes <math>L_2</math> delay. [[Image:turbo decoder.svg]] An interleaver installed between the two decoders is used here to scatter error bursts coming from <math>DEC_1</math> output. ''DI'' block is a demultiplexing and insertion module. It works as a switch, redirecting input bits to <math>DEC_1</math> at one moment and to <math>DEC_2</math> at another. In OFF state, it feeds both <math>y_{1k}</math> and <math>y_{2k}</math> inputs with padding bits (zeros). Consider a memoryless [[Additive white Gaussian noise|AWGN]] channel, and assume that at ''k''-th iteration, the decoder receives a pair of random variables: :<math>~x_k=(2d_k-1)+a_k</math>, :<math>~y_k=2(Y_k-1)+b_k</math> where <math>a_k</math> and <math>b_k</math> are independent noise components having the same variance <math>\sigma^2</math>. <math>Y_k</math> is a ''k''-th bit from <math>y_k</math> encoder output. Redundant information is demultiplexed and sent through ''DI'' to <math>DEC_1</math> (when <math>y_k=y_{1k}</math>) and to <math>DEC_2</math> (when <math>y_k=y_{2k}</math>). <math>DEC_1</math> yields a soft decision, i.e.: :<math>\Lambda(d_k)=\log\frac{p(d_k=1)}{p(d_k=0)}</math> and delivers it to <math>DEC_2</math>. <math>\Lambda(d_k)</math> is called the ''logarithm of the likelihood ratio'' (LLR). <math>p(d_k=i), i \in \{0,1\}</math> is the ''a posteriori probability'' (APP) of the <math>d_k</math> data bit which shows the probability of interpreting a received <math>d_k</math> bit as <math>i</math>. Taking the ''LLR'' into account, <math>DEC_2</math> yields a hard decision, i.e. a decoded bit. It is known that the [[Viterbi algorithm]] is unable to calculate APP, thus it cannot be used in <math>DEC_1</math>. Instead of that, a modified [[BCJR algorithm]] is used. For <math>DEC_2</math>, the [[Viterbi algorithm]] is an appropriate one. However, the depicted structure is not an optimal one, because <math>DEC_1</math> uses only a proper fraction of the available redundant information. In order to improve the structure, a feedback loop is used (see the dotted line on the figure). ==Soft decision approach== The decoder front-end produces an integer for each bit in the data stream. This integer is a measure of how likely it is that the bit is a 0 or 1 and is also called ''soft bit''. The integer could be drawn from the range [-127, 127], where: * -127 means "certainly 0" * -100 means "very likely 0" * 0 means "it could be either 0 or 1" * 100 means "very likely 1" * 127 means "certainly 1" * etc This introduces a probabilistic aspect to the data-stream from the front end, but it conveys more information about each bit than just 0 or 1. For example, for each bit, the front end of a traditional wireless-receiver has to decide if an internal analog voltage is above or below a given threshold voltage level. For a turbo-code decoder, the front end would provide an integer measure of how far the internal voltage is from the given threshold. To decode the ''m+n''-bit block of data, the decoder front-end creates a block of likelihood measures, with one likelihood measure for each bit in the data stream. There are two parallel decoders, one for each of the ''n/2''-bit parity sub-blocks. Both decoders use the sub-block of ''m'' likelihoods for the payload data. The decoder working on the second parity sub-block knows the permutation that the coder used for this sub-block. ==Solving hypotheses to find bits== The key innovation of turbo codes is how they use the likelihood data to reconcile differences between the two decoders. Each of the two convolutional decoders generates a hypothesis (with derived likelihoods) for the pattern of ''m'' bits in the payload sub-block. The hypothesis bit-patterns are compared, and if they differ, the decoders exchange the derived likelihoods they have for each bit in the hypotheses. Each decoder incorporates the derived likelihood estimates from the other decoder to generate a new hypothesis for the bits in the payload. Then they compare these new hypotheses. This iterative process continues until the two decoders come up with the same hypothesis for the ''m''-bit pattern of the payload, typically in 15 to 18 cycles. An analogy can be drawn between this process and that of solving cross-reference puzzles like [[crossword]] or [[sudoku]]. Consider a partially-completed, possibly garbled crossword puzzle. Two puzzle solvers (decoders) are trying to solve it: one possessing only the "down" clues (parity bits), and the other possessing only the "across" clues. To start, both solvers guess the answers (hypotheses) to their own clues, noting down how confident they are in each letter (payload bit). Then, they compare notes, by exchanging answers and confidence ratings with each other, noticing where and how they differ. Based on this new knowledge, they both come up with updated answers and confidence ratings, repeating the whole process until they converge to the same solution. ==Performance== Turbo codes perform well due to the attractive combination of the code's random appearance on the channel 