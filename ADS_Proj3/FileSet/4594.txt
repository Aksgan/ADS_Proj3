as it provides a theoretical means for compressing data, allowing us to represent any sequence ''X''<sup>''n''</sup> using ''nH''(''X'') bits on average, and, hence, justifying the use of entropy as a measure of information from a source. The AEP can also be proven for a large class of [[stationary ergodic process]]es, allowing typical set to be defined in more general cases. ==(Weakly) typical sequences (weak typicality, entropy typicality)== If a sequence ''x''<sub>1</sub>, ..., ''x''<sub>''n''</sub> is drawn from an [[Independent identically-distributed random variables|i.i.d. distribution]] ''X'' defined over a finite alphabet <math>\mathcal{X}</math>, then the typical set, ''A''<sub>''ε''</sup><sup>(''n'')</sup><math>\in\mathcal{X}</math> is defined as those sequences which satisfy: :<math> 2^{-n(H(X)+\varepsilon)} \leqslant p(x_1, x_2, \dots , x_n) \leqslant 2^{-n(H(X)-\varepsilon)} </math> Where : <math> H(X) = - \sum_{y \isin \mathcal{X}}p(y)\log_2 p(y) </math> is the information entropy of ''X''. The probability above need only be within a factor of 2<sup>''n''ε''</sup>. It has the following properties if ''n'' is sufficiently large, ε can be chosen arbitrarily small so that: #The probability of a sequence from ''X'' being drawn from ''A''<sub>''ε''</sup><sup>(''n'')</sup> is greater than 1 &minus; ''ε'' #<math>\left| {A_\varepsilon}^{(n)} \right| \leqslant 2^{n(H(X)+\varepsilon)}</math> #<math>\left| {A_\varepsilon}^{(n)} \right| \geqslant (1-\varepsilon)2^{n(H(X)-\varepsilon)}</math> For a general stochastic process {''X''(''t'')} with AEP, the (weakly) typical set can be defined similarly with ''p''(''x''<sub>1</sub>, ''x''<sub>2</sub>, ..., ''x''<sub>''n''</sub>) replaced by ''p''(''x''<sub>0</sub><sup>''τ''</sup>) (i.e. the probability of the sample limited to the time interval [0, ''τ'']), ''n'' being the [[degrees of freedom (physics and chemistry)|degree of freedom]] of the process in the time interval and ''H''(''X'') being the [[entropy rate]]. If the process is continuous-valued, [[differential entropy]] is used instead. ==Strongly typical sequences (strong typicality, letter typicality)== If a sequence ''x''<sub>1</sub>, ..., ''x''<sub>''n''</sub> is drawn from some specified joint distribution defined over a finite or an infinite alphabet <math>\mathcal{X}</math>, then the strongly typical set, ''A''<sub>ε,strong</sub><sup>(''n'')</sup><math>\in\mathcal{X}</math> is defined as the set of sequences which satisfy :<math> \left|\frac{N(x^n)}{n}-p(x^n)\right| < \frac{\varepsilon}{\|\mathcal{X}\|}. </math> where <math>{N(x^n)}</math> is the number of occurrences of a specific symbol in the sequence. It can be shown that strongly typical sequences are also weakly typical (with a different constant ε), and hence the name. The two forms, however, are not equivalent. Strong typicality is often easier to work with in proving theorems for memoryless channels. However, as is apparent from the definition, this form of typicality is only defined for random variables having finite support. ==Jointly typical sequences== Two sequences <math>x^n</math> and <math>y^n</math> are jointly ε-typical if the pair <math>(x^n,y^n)</math> is ε-typical with respect to the joint distribution <math>p(x^n,y^n)=\prod_{i=1}^n p(x_i,y_i)</math> and both <math>x^n</math> and <math>y^n</math> are ε-typical with respect to their marginal distributions <math>p(x^n)</math> and <math>p(y^n)</math>. The set of all such pairs of sequences <math>(x^n,y^n)</math> is denoted by <math>A_{\varepsilon}^n(X,Y)</math>. Jointly ε-typical ''n''-tuple sequences are defined similarly. Let <math>\tilde{X}^n</math> and <math>\tilde{Y}^n</math> be two independent sequences of random variables with the same marginal distributions <math>p(x^n)</math> and <math>p(y^n)</math>. Then the set of jointly typical sequences has the following properties: #<math> P\left[ (X^n,Y^n) \in A_{\varepsilon}^n(X,Y) \right] \geqslant 1 - \epsilon </math> #<math> \left| A_{\varepsilon}^n(X,Y) \right| \leqslant 2^{n (H(X,Y) + \epsilon)} </math> #<math> \left| A_{\varepsilon}^n(X,Y) \right| \geqslant (1 - \epsilon) 2^{n (H(X,Y) - \epsilon)} </math> #<math> P\left[ (\tilde{X}^n,\tilde{Y}^n) \in A_{\varepsilon}^n(X,Y) \right] \leqslant 2^{-n (I(X;Y) - 3 \epsilon)} </math> #<math> P\left[ (\tilde{X}^n,\tilde{Y}^n) \in A_{\varepsilon}^n(X,Y) \right] \geqslant (1 - \epsilon) 2^{-n (I(X;Y) + 3 \epsilon)}</math> {{Expand section|date=December 2009}} ==Applications of typicality== {{Expand section|date=December 2009}} ===Typical set encoding=== In [[communication]], typical set encoding encodes only the typical set of a stochastic source with fixed length block codes. Asymptotically, it is, by the AEP, lossless and achieves the minimum rate equal to the entropy rate of the source. {{Expand section|date=December 2009}} ===Typical set decoding=== In [[communication]], typical set decoding is used in conjunction with [[random coding]] to estimate the transmitted message as the one with a codeword that is jointly ε-typical with the observation. i.e. :<math>\hat{w}=w \iff (\exists!w)( (x_1^n(w),y_1^n)\in A_{\varepsilon}^n(X,Y)) </math> where <math>\hat{w},x_1^n(w),y_1^n</math> are the message estimate, codeword of message <math>w</math> and the observation respectively. <math>A_{\varepsilon}^n(X,Y)</math> is defined with respect to the joint distribution <math>p(x_1^n)p(y_1^n|x_1^n)</math> where <math>p(y_1^n|x_1^n)</math> is the transition probability that characterizes the channel statistics, and <math>p(x_1^n)</math> is some input distribution used to generate the codewords in the random codebook. {{Expand section|date=December 2009}} ===Universal null-hypothesis testing=== {{Empty section|date=December 2009}} ===Universal channel code=== {{Expand section|date=December 2009}} {{See also|algorithmic complexity theory}} ==See also== * [[Asymptotic equipartition property]] * [[Source coding theorem]] * [[Noisy-channel coding theorem]] ==References== * [[C. E. Shannon]], "[http://plan9.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf A Mathematical Theory of Communication]", ''[[Bell System Technical Journal]]'', vol. 27, pp. 379–423, 623-656, July, October, 1948 * {{Cite book | last = Cover | first = Thomas M. | title = Elements of Information Theory | chapter = Chapter 3: Asymptotic Equipartition Property, Chapter 5: Data Compression, Chapter 8: Channel Capacity | year = 2006 | publisher = John Wiley & Sons | isbn = 0-471-24195-4 }} * [[David J. C. MacKay]]. ''[http://www.inference.phy.cam.ac.uk/mackay/itila/book.html Information Theory, Inference, and Learning Algorithms]'' Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1 {{DEFAULTSORT:Typical Set}} [[Category:Information theory]] [[Category:Probability theory]]</text> </page> <page> <id>38533</id> <title>Tyranny of numbers</title> <text>{{intromissing|date=November 2009}} {{nofootnotes|date=November 2009}} The '''tyranny of numbers''' was a problem faced in the 1960s by [[computer engineering|computer engineers]]. Engineers were unable to increase the performance of their designs due to the huge number of components involved. In theory, every component needed to be wired to every other one, and were typically strung and [[soldering|soldered]] by hand. In order to improve performance, more components would be needed, and it seemed that future designs would consist almost entirely of wiring. The term was first used by the Vice President of [[Bell Labs]] in 1957 in a paper celebrating the 10th anniversary of the invention of the [[transistor]]. Referring to the problems many designers were having, he stated: {{Quotation|For some time now, electronic man has known how 'in principle' to extend greatly his visual, tactile, and mental abilities through the digital transmission and processing of all kinds of information. However, all these functions suffer from what has been called 'the tyranny of numbers.' Such systems, because of their complex digital nature, require hundreds, thousands, and sometimes tens of thousands of electron devices.|Jack Morton|[http://everything2.com/e2node/The%2520Tyranny%2520of%2520Numbers The Tyranny of 