*[[Geographic information system]] *[[Stereo camera]] *[[Stereo vision]] *[[Structure from motion]] ==References== <references> <ref name="FOGEL2008">{{cite web | first = David| last = Fogel | title = Image Rectification with Radial Basis Functions | url = http://www.ncgia.ucsb.edu/conf/SANTA_FE_CD-ROM/sf_papers/fogel_david/santafe.html | accessdate = 2008-06-09 }}</ref> <ref name="FUSIELLO2000">{{cite web | first = Andrea | last = Fusiello | title = Epipolar Rectification | date = 2000-03-17 | url = http://profs.sci.univr.it/~fusiello/rectif_cvol/rectif_cvol.html | accessdate = 2008-06-09}}</ref> <ref name="ORAM2001">{{ cite web | first = Daniel | last = Oram | title = Rectification for Any Epipolar Geometry | year = 2001 | url = http://www.bmva.org/bmvc/2001/papers/82/accepted_82.pdf | accessdate = 2010-06-08}}</ref> <ref name="FUSIELLO2000_2">{{cite journal | doi = 10.1007/s001380050120 | first1 = Andrea | last1 = Fusiello | first2 = Emanuele | last2 = Trucco | first3 = Alessandro | last3 = Verri | title = A compact algorithm for rectification of stereo pairs | date = 2000-03-02 | journal = Machine Vision and Applications | volume = 12 | pages = 16–22 | publisher = Springer-Verlag | url = http://profs.sci.univr.it/~fusiello/papers/00120016.pdf | accessdate = 2010-06-08}}</ref> <ref name="LIM2004">{{ cite journal | first1 = Ser-Nam | last1 = Lim | first2 = Anurag | last2 = Mittal | first3 = Larry | last3 = Davis | first4 = Nikos | last4 = Paragios | title = Uncalibrated stereo rectification for automatic 3D surveillance | journal = International Conference on Image Processing | volume = 2 | page = 1357 | url = http://www.umiacs.umd.edu/users/sernam/papers/rect.pdf | accessdate = 2010-06-08 }}</ref> <ref name="POLLEFEYS1999">{{ cite journal | first1 = Marc | last1 = Pollefeys | first2 = Reinhard | last2 = Koch | first3 = Luc | last3 = Van Gool | title = A simple and efficient rectification method for general motion | journal = Proc. International Conference on Computer Vision | page = 496-501 | year = 1999 | url = http://www.inf.ethz.ch/personal/pomarc/pubs/PollefeysICCV99.pdf | accessdate = 2011-01-019}}</ref> </references> <ol start="7"> <li>{{cite journal | author=R. I. Hartley | title=Theory and Practice of Projective Rectification | journal=Int. Journal of Computer Vision | volume=35 | pages=115–127 | year=1999 | doi=10.1023/A:1008115206617}}</li> <li>{{cite web | first = Marc | last = Pollefeys | title = Polar rectification | url = http://www.cs.unc.edu/~marc/tutorial/node99.html | accessdate = 2007-06-09}}</li> <li>{{cite book | author = Linda G. Shapiro and George C. Stockman | title = Computer Vision | publisher = Prentice Hall | year = 2001 | isbn = 0-13-030796-3 | pages = 580 }}</li> </ol> {{DEFAULTSORT:Image Rectification}} [[Category:Geographic information systems]] [[Category:Geometry in computer vision]] [[Category:Image processing]] [[de:Rektifizierung]]</text> </page> <page> <id>17950</id> <title>Image registration</title> <text>[[Image:Registrator Demo2.png|thumb|right|Registering and summing multiple exposures of the same scene improves signal to noise ratio, allowing to see things previously impossible to see.]] {{Refimprove|date=August 2008}} '''Image registration''' is the process of transforming different sets of data into one coordinate system. Data may be multiple photographs, data from different sensors, from different times, or from different viewpoints.<ref>[http://portal.acm.org/citation.cfm?id=146374 Lisa Gottesfeld Brown, A survey of image registration techniques (''abstract''), ACM Computing Surveys (CSUR) archive, Volume 24 , Issue 4, December 1992), Pages: 325 - 376]</ref> It is used in [[computer vision]], [[medical imaging]], military [[automatic target recognition]], and compiling and analyzing images and data from satellites. Registration is necessary in order to be able to compare or integrate the data obtained from these different measurements. == Algorithm classification == === Intensity-based vs feature-based === Image registration or image alignment algorithms can be classified into intensity-based and feature-based.<ref name="AG">A. Ardeshir Goshtasby: [http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471649546.html 2-D and 3-D Image Registration for Medical, Remote Sensing, and Industrial Applications], Wiley Press, 2005.</ref> One of the images is referred to as the ''reference'' or ''source'' and the second image is referred to as the ''target'' or ''sensed.'' Image registration involves spatially transforming the target image to align with the reference image.<ref name="AG"/> Intensity-based methods compare intensity patterns in images via correlation metrics, while feature-based methods find correspondence between image features such as points, lines, and contours.<ref name="AG"/> Intensity-based methods register entire images or subimages. If subimages are registered, centers of corresponding subimages are treated as corresponding feature points. Feature-based method established correspondence between a number of points in images. Knowing the correspondence between a number of points in images, a transformation is then determined to map the target image to the reference images, thereby establishing point-by-point correspondence between the reference and target images.<ref name="AG"/> === Transformation models === Image registration algorithms can also be classified according to the transformation models they use to relate the target image space to the reference image space. The first broad category of transformation models includes [[linear transformation]]s, which include translation, rotation, scaling, and other affine transforms. [[Linear transformation]]s are global in nature, thus, they cannot model local geometric differences between images.<ref name="AG"/> The second category of transformations allow 'elastic' or 'nonrigid' transformations. These transformations are capable of locally warping the target image to align with the reference image. Nonrigid transformations include radial basis functions (thin-plate or surface splines, multiquadrics, and compactly-supported transformations<ref name="AG"/>), physical continuum models (viscous fluids), and large deformation models ([[diffeomorphism]]s). === Spatial vs. frequency domain methods === Spatial methods operate in the image domain, matching intensity patterns or features in images. Some of the feature matching algorithms are outgrowths of traditional techniques for performing manual image registration, in which an operator chooses corresponding [[Feature (computer vision)|control points]] (CPs) in images. When the number of control points exceeds the minimum required to define the appropriate transformation model, iterative algorithms like [[RANSAC]] can be used to robustly estimate the parameters of a particular transformation type (e.g. affine) for registration of the images. Frequency-domain methods find the transformation parameters for registration of the images while working in the transform domain. Such methods work for simple transformations, such as translation, rotation, and scaling. Applying the [[Phase correlation]] method to a pair of images produces a third image which contains a single peak. The location of this peak corresponds to the relative translation between the images. Unlike many spatial-domain algorithms, the phase correlation method is resilient to noise, occlusions, and other defects typical of medical or satellite images. Additionally, the 