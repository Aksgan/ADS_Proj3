Therefore, they have no idea how they actually performed (i.e. which questions they answered correctly.) ==Standards of quality== In general, high-quality assessments are considered those with a high level of [[reliability (statistics)|reliability]] and [[validity (statistics)|validity]]. Approaches to reliability and validity vary, however. ===Reliability=== [[Reliability (statistics)|Reliability]] relates to the consistency of an assessment. A reliable assessment is one which consistently achieves the same results with the same (or similar) cohort of students. Various factors affect reliability—including ambiguous questions, too many options within a question paper, vague marking instructions and poorly trained markers. Traditionally, the reliability of an assessment is based on the following: # Temporal stability: Performance on a test is comparable on two or more separate occasions. # Form equivalence: Performance among examinees is equivalent on different forms of a test based on the same content. # Internal consistency: Responses on a test are consistent across questions. For example: In a survey that asks respondents to rate attitudes toward technology, consistency would be expected in responses to the following questions: #* "I feel very negative about computers in general." #* "I enjoy using computers."<ref name="Yu, Chong Ho">Yu, Chong Ho (2005). "Reliability and Validity." Educational Assessment. Available at [http://www.creative-wisdom.com/teaching/assessment/reliability.html Creative-wisdom.com]. Retrieved January 29, 2009.</ref> Reliability can also be expressed in mathematical terms as: '''Rx = VT/Vx''' where Rx is the reliability in the observed (test) score, X; Vt and Vx are the variability in ‘true’ (i.e., candidate’s innate performance) and measured test scores respectively. The Rx can range from 0 (completely unreliable), to 1 (completely reliable). An Rx of 1 is rarely achieved, and an Rx of 0.8 is generally considered reliable. <ref>{{cite journal |author=Vergis A, Hardy K |title=Principles of Assessment: A Primer for Medical Educators in the Clinical Years |journal=The Internet Journal of Medical Education | volume=1 |issue=1 | year=2010 |url=http://www.ispub.com/journal/the_internet_journal_of_medical_education/volume_1_number_1_74/article_printable/principles-of-assessment-a-primer-for-medical-educators-in-the-clinical-years-4.html}}</ref> ===Validity=== A [[validity (statistics)|valid]] assessment is one which measures what it is intended to measure. For example, it would not be valid to assess driving skills through a written test alone. A more valid way of assessing driving skills would be through a combination of tests that help determine what a driver knows, such as through a written test of driving knowledge, and what a driver is able to do, such as through a performance assessment of actual driving. Teachers frequently complain that some examinations do not properly assess the [[syllabus]] upon which the examination is based; they are, effectively, questioning the validity of the exam. Validity of an assessment is generally gauged through examination of evidence in the following categories: # Content – Does the content of the test measure stated objectives? # Criterion – Do scores correlate to an outside reference? (ex: Do high scores on a 4th grade reading test accurately predict reading skill in future grades?) # Construct – Does the assessment correspond to other significant variables? (ex: Do [[English as a second language|ESL]]students consistently perform differently on a writing exam than native English speakers?)<ref name="Moskal, Barbara M., & Leydens, Jon A">Moskal, Barbara M., & Leydens, Jon A (2000). "Scoring Rubric Development: Validity and Reliability." Practical Assessment, Research & Evaluation, 7(10). Retrieved January 30, 2009 from [http://PAREonline.net/getvn.asp?v=7&n=10PAREonline.net]</ref> # Face – Does the item or theory make sense, and is it seemingly correct to the expert reader?<ref>{{cite journal |author=Vergis A, Hardy K |title=Principles of Assessment: A Primer for Medical Educators in the Clinical Years |journal=The Internet Journal of Medical Education | volume=1 |issue=1 | year=2010 | url=http://www.ispub.com/journal/the_internet_journal_of_medical_education/volume_1_number_1_74/article_printable/principles-of-assessment-a-primer-for-medical-educators-in-the-clinical-years-4.html}}</ref> A good assessment has both validity and reliability, plus the other quality attributes noted above for a specific context and purpose. In practice, an assessment is rarely totally valid or totally reliable. A ruler which is marked wrong will always give the same (wrong) measurements. It is very reliable, but not very valid. Asking random individuals to tell the time without looking at a clock or watch is sometimes used as an example of an assessment which is valid, but not reliable. The answers will vary between individuals, but the average answer is probably close to the actual time. In many fields, such as medical research, educational testing, and psychology, there will often be a trade-off between reliability and validity. A history test written for high validity will have many essay and fill-in-the-blank questions. It will be a good measure of mastery of the subject, but difficult to score completely accurately. A history test written for high reliability will be entirely multiple choice. It isn't as good at measuring knowledge of history, but can easily be scored with great precision. We may generalize from this. The more reliable our estimate is of what we purport to measure, the less certain we are that we are actually measuring that aspect of attainment. It is also important to note that there are at least thirteen sources of invalidity, which can be estimated for individual students in test situations. They never are. Perhaps this is because their social purpose demands the absence of any error, and validity errors are usually so high that they would destabilize the whole assessment industry. It is well to distinguish between "subject-matter" validity and "predictive" validity. The former, used widely in education, predicts the score a student would get on a similar test but with different questions. The latter, used widely in the workplace, predicts performance. Thus, a subject-matter-valid test of knowledge of driving rules is appropriate while a predictively-valid test would assess whether the potential driver could follow those rules. ===Testing standards=== In the field of [[psychometrics]], the [[Standards for Educational and Psychological Testing]]<ref>[http://www.apa.org/science/standards.html#overview ''The Standards for Educational and Psychological Testing'']</ref> place standards about validity and reliability, along with [[Measurement#Difficulties|errors of measurement]]and related considerations under the general topic of test construction, evaluation and documentation. The second major topic covers standards related to fairness in testing, including [[justice|fairness]] in testing and test use, the [[right]]s and [[social responsibility|responsibilities]] of test takers, testing individuals of diverse [[language|linguistic backgrounds]], and testing individuals with [[disability|disabilities]]. The third and final major topic covers standards related to testing applications, including the responsibilities 