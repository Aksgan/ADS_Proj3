and legal documents more readily produces usable output than conversation or less standardised text. Improved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has [[word sense disambiguation|unambiguously identified]] which words in the text are names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports). The progress and potential of machine translation has been debated much through its history. Since the 1950s, a number of scholars has questioned the possibility of achieving fully automatic machine translation of high quality.<ref>First and most notably Bar-Hillel, Yeheshua: "A demonstration of the nonfeasibility of fully automatic high quality machine translation", in ''Language and Information: Selected essays on their theory and application'' (Jerusalem Academic Press, 1964), pp. 174–179.</ref> Some critics claim that there are in-principle obstacles to automatizing the translation process.<ref>[https://docs.google.com/fileview?id=0B7-4xydn3MXJZjFkZTllZjItN2Q5Ny00YmUxLWEzODItNTYyMjhlNTY5NWIz Madsen, Mathias: The Limits of Machine Translation (2010)]</ref> ==History== {{Main|History of machine translation}} The idea of machine translation may be traced back to the 17th century. In 1629, [[René Descartes]] proposed a universal language, with equivalent ideas in different tongues sharing one symbol. In the 1950s, The [[Georgetown-IBM experiment|Georgetown experiment]] (1954) involved fully-automatic translation of over sixty [[Russian language|Russian]] sentences into [[English language|English]]. The experiment was a great success and ushered in an era of substantial funding for machine-translation research. The authors claimed that within three to five years, machine translation would be a solved problem. Real progress was much slower, however, and after the [[ALPAC|ALPAC report]] (1966), which found that the ten-year-long research had failed to fulfill expectations, funding was greatly reduced. Beginning in the late 1980s, as [[computation]]al power increased and became less expensive, more interest was shown in [[statistical machine translation|statistical models for machine translation]]. The idea of using digital computers for translation of natural languages was proposed as early as 1946 by [[Andrew Donald Booth|A. D. Booth]] and possibly others. The Georgetown experiment was by no means the first such application, and a demonstration was made in 1954 on the [[APEXC]] machine at [[Birkbeck, University of London|Birkbeck College]] ([[University of London]]) of a rudimentary translation of English into French. Several papers on the topic were published at the time, and even articles in popular journals (see for example ''[[Wireless World]]'', Sept. 1955, Cleave and Zacharov). A similar application, also pioneered at Birkbeck College at the time, was reading and composing [[Braille]] texts by computer. ==Translation process== {{Main|Translation process}} The [[translation process]] may be stated as: # [[Decoding]] the [[meaning (linguistic)|meaning]] of the [[source text]]; and # Re-[[encoding]] this [[meaning (linguistic)|meaning]] in the [[target language]]. Behind this ostensibly simple procedure lies a complex [[cognitive]] operation. To decode the meaning of the [[source text]] in its entirety, the translator must interpret and analyse all the features of the text, a process that requires in-depth knowledge of the [[grammar]], [[semantics]], [[syntax]], [[idiom]]s, etc., of the [[source language]], as well as the [[culture]] of its speakers. The translator needs the same in-depth knowledge to re-encode the meaning in the [[target language]]. Therein lies the challenge in machine translation: how to program a computer that will "understand" a text as a person does, and that will "create" a new text in the [[target language]] that "sounds" as if it has been written by a person. This problem may be approached in a number of ways. ==Approaches== [[File:Direct translation and transfer translation pyramind.svg|thumb|right|300px|Pyramid showing comparative depths of intermediary representation, [[interlingual machine translation]] at the peak, followed by transfer-based, then direct translation.]] Machine translation can use a method based on [[Expert System|linguistic rules]], which means that words will be translated in a linguistic way &mdash; the most suitable (orally speaking) words of the target language will replace the ones in the source language. It is often argued that the success of machine translation requires the problem of [[natural language processing|natural language understanding]] to be solved first. Generally, rule-based methods parse a text, usually creating an intermediary, symbolic representation, from which the text in the target language is generated. According to the nature of the intermediary representation, an approach is described as [[interlingual machine translation]] or [[transfer-based machine translation]]. These methods require extensive [[lexicon]]s with [[morphology (linguistics)|morphological]], [[syntax|syntactic]], and [[semantics|semantic]] information, and large sets of rules. Given enough data, machine translation programs often work well enough for a [[native speaker]] of one language to get the approximate meaning of what is written by the other native speaker. The difficulty is getting enough data of the right kind to support the particular method. For example, the large multilingual [[Text corpus|corpus]] of data needed for statistical methods to work is not necessary for the grammar-based methods. But then, the grammar methods need a skilled linguist to carefully design the grammar that they use. To translate between closely related languages, a technique referred to as [[shallow-transfer machine translation]] may be used. ===Rule-based=== The rule-based machine translation paradigm includes transfer-based machine translation, interlingual machine translation and dictionary-based machine translation paradigms. {{Main|Rule-based machine translation}} '''''Transfer-based machine translation''''' {{Main|Transfer-based machine translation}} '''''Interlingual''''' {{Main|Interlingual machine translation}} Interlingual machine translation is one instance of rule-based machine-translation approaches. In this approach, the source language, i.e. the text to be translated, is transformed into an interlingual, i.e. source-/target-language-independent representation. The target language is then generated out of the [[interlinguistics|interlingua]]. '''''Dictionary-based''''' {{Main|Dictionary-based machine translation}} Machine translation can use a method based on [[dictionary]] entries, which means that the words will be translated as they are by a dictionary. ===Statistical=== {{Main|Statistical machine translation}} Statistical machine translation tries to generate translations using [[statistical methods]] based on bilingual text corpora, such as the [[Hansard#Machine translation|Canadian Hansard]] corpus, the English-French record of the Canadian parliament and [[EUROPARL]], the record of the [[European Parliament]]. Where such corpora are available, impressive results can be achieved translating texts of a similar kind, but such corpora are still very rare. The first statistical machine translation 