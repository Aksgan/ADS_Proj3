head and the guard such that the head is equal to the goal and the guard is entailed. In practice, [[entailment]] may be checked with an incomplete method. An extension to the syntax and semantics of concurrent logic programming is the ''atomic tell''. When the interpreter uses a clause, its guard is added to the constraint store. However, also added are the constraints of the body. Due to commitment to this clause, the interpreter does not backtrack if the constraints of the body are inconsistent with the store. This condition can be avoided by the use of atomic tell, which is a variant in which the clause contain a sort of "second guard" that is only checked for consistency. Such a clause is written <code>H :- G:D|B</code>. This clause is used to rewrite a literal only if <code>G</code> is entailed by the constraint store and <code>D</code> is consistent with it. In this case, both <code>G</code> and <code>D</code> are added to the constraint store. ==History== The study of concurrent constraint logic programming started at the end of the 1980s, when some of the principles of concurrent logic programming were integrated into constraint logic programming by [[Michael J. Maher]]. The theoretical properties of concurrent constraint logic programming were later studied by various authors, such as [[Vijay A. Saraswat]]. ==See also== * [[Curry (programming language)|Curry]], a logic functional programming language, which allows programming concurrent systems[http://www.informatik.uni-kiel.de/~curry,/examples/#residuation]. ==References== *{{cite book | first=Kim | last=Marriot | coauthors=Peter J. Stuckey | title=Programming with constraints: An introduction | year=1998 | publisher=MIT Press }} ISBN 0-262-13341-5 *{{cite book | first=Thom | last=Fr&uuml;hwirth | coauthors=Slim Abdennadher | title=Essentials of constraint programming | year=2003 | publisher=Springer }} ISBN 3-540-67623-6 *{{cite journal | first=Joxan | last=Jaffar | coauthors=Michael J. Maher | title=Constraint logic programming: a survey | journal=Journal of logic programming | volume=19/20 | pages=503–581 | year=1994 | doi=10.1016/0743-1066(94)90033-7 }} [[Category:Constraint satisfaction]] [[Category:Programming paradigms]] [[Category:Concurrent computing]] [[Category:Constraint programming]] [[Category:Logic programming]] [[ja:並行制約プログラミング]]</text> </page> <page> <id>7693</id> <title>Conditional convergence</title> <text>In [[mathematics]], a [[series (mathematics)|series]] or [[integral]] is said to be '''conditionally convergent''' if it converges, but it does not [[Absolute convergence|converge absolutely]]. ==Definition== More precisely, a series <math>\scriptstyle\sum\limits_{n=0}^\infty a_n</math> is said to '''converge conditionally''' if <math>\scriptstyle\lim\limits_{m\rightarrow\infty}\,\sum\limits_{n=0}^m\,a_n</math> exists and is a finite number (not &infin; or &minus;&infin;), but <math>\scriptstyle\sum\limits_{n=0}^\infty \left|a_n\right| = \infty.</math> A classical example is given by :<math>1 - {1 \over 2} + {1 \over 3} - {1 \over 4} + {1 \over 5} - \cdots =\sum\limits_{n=1}^\infty {(-1)^{n+1} \over n}</math> which converges to <math>\ln (2)\,\!</math>, but is not absolutely convergent (see [[Harmonic series (mathematics)|Harmonic series]]). The simplest examples of conditionally convergent series (including the one above) are the [[alternating series]]. [[Bernhard Riemann]] proved that a conditionally convergent series may be rearranged to converge to any sum at all, including &infin; or &minus;&infin;; see ''[[Riemann series theorem]]''. ==See also== *[[Absolute convergence]] *[[Unconditional convergence]] ==References== * Walter Rudin, ''Principles of Mathematical Analysis'' (McGraw-Hill: New York, 1964). [[Category:Mathematical series]] [[Category:Integral calculus]] [[Category:Mathematical analysis]] [[Category:Summability theory]] [[bs:Uslovna konvergencija]] [[ru:Условная сходимость]] [[sv:Betingad konvergens]] [[uk:Умовна збіжність]]</text> </page> <page> <id>7694</id> <title>Conditional entropy</title> <text>[[Image:Conditional_entropy.png|thumb|256px|right|Individual (H(X),H(Y)), joint (H(X,Y)), and conditional entropies for a pair of correlated subsystems X,Y with mutual information I(X; Y).]] In [[information theory]], the '''conditional entropy''' (or '''equivocation''') quantifies the remaining [[information entropy|entropy]] (i.e. uncertainty) of a [[random variable]] <math>Y</math> given that the value of another random variable <math>X</math> is known. It is referred to as ''the entropy of <math>Y</math> conditional on <math>X</math>'', and is written <math>H(Y|X)</math>. Like other entropies, the conditional entropy is measured in [[bit]]s, [[nat (information)|nat]]s, or [[ban (information)|ban]]s. == Definition == More precisely, if <math>H(Y|X=x)</math> is the entropy of the variable <math>Y</math> conditional on the variable <math>X</math> taking a certain value <math>x</math>, then <math>H(Y|X)</math> is the result of averaging <math>H(Y|X=x)</math> over all possible values <math>x</math> that <math>X</math> may take. Given discrete random variable <math>X</math> with support <math>\mathcal X</math> and <math>Y</math> with support <math>\mathcal Y</math>, the conditional entropy of <math>Y</math> given <math>X</math> is defined as: ::<math>\begin{align} H(Y|X)\ &\stackrel{\mathrm{def}}{=}\sum_{x\in\mathcal X}\,p(x)\,H(Y|X=x)\\ &{=}-\sum_{x\in\mathcal X}p(x)\sum_{y\in\mathcal Y}\,p(y|x)\,\log\,p(y|x)\\ &=-\sum_{x\in\mathcal X}\sum_{y\in\mathcal Y}\,p(x,y)\,\log\,p(y|x)\\ &=-\sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log\,p(y|x)\\ &=-\sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log \frac {p(x,y)} {p(x)} \\ &= \sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log \frac {p(x)} {p(x,y)}. \end{align}</math> ==Chain rule== From this definition and the definition of conditional probability, the chain rule for conditional entropy is <math>H(Y|X)\,=\,H(Y,X)-H(X) \, .</math> This is true because <math>\begin{align} H(Y|X)=&\sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log \frac {p(x)} {p(x,y)}\\ =&-\sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log\,p(x,y) + \sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log\,p(x) \\ =& H(X,Y) + \sum_{x \in \mathcal X} p(x)\log\,p(x) \\ =& H(X,Y) - H(X). \end{align}</math> ==Intuition== Intuitively, the combined system contains <math>H(X,Y)</math> bits of information: we need <math>H(X,Y)</math> bits of information to reconstruct its exact state. If we learn the value of <math>X</math>, we have gained <math>H(X)</math> bits of information, and the system has <math>H(Y|X)</math> bits of uncertainty remaining. <math>H(Y|X)=0</math> if and only if the value of <math>Y</math> is completely determined by the value of <math>X</math>. Conversely, <math>H(Y|X) = H(Y)</math> if and only if <math>Y</math> and <math>X</math> are [[independent random variables]]. ==Generalization to quantum theory== In [[quantum information theory]], the conditional entropy is generalized to the [[conditional quantum entropy]]. ==Other properties== For any <math>X</math> and <math>Y</math>: <math>H(X|Y) \le H(X)</math> <math>H(X,Y) = H(X|Y) + H(Y|X) + I(X;Y)</math>, where <math>I(X;Y)</math> is the mutual information between <math>X</math> and <math>Y</math>. <math>I(X;Y) \le H(X)</math>, where <math>I(X;Y)</math> is the mutual information between <math>X</math> and <math>Y</math>. For independent <math>X</math> and <math>Y</math>: <math>H(Y|X) = H(X)</math> and <math>H(X|Y) = H(X)</math> ==References== # {{cite book |author=Theresa M. Korn; Korn, Granino Arthur |title=Mathematical Handbook for Scientists and Engineers: Definitions, Theorems, and Formulas for Reference and Review |publisher=Dover Publications |location=New York |year= |pages=613–614 |isbn=0-486-41147-8 |oclc= |doi=}} # {{cite book |author=C. Arndt |title=Information Measures: Information and its description in Science and Engineering) |publisher=Springer |location=Berlin |year=2001 |pages=370–373 |isbn=3-540-41633-1 |oclc= |doi=}} == See also == * [[Entropy (information theory)]] * [[Mutual information]] * [[Conditional quantum entropy]] * [[Variation of information]] * [[Likelihood function]] [[Category:Entropy and information]] [[Category:Information theory]] [[bar:Bedingte Entropie]] [[de:Bedingte Entropie]] [[fr:Entropie conditionnelle]] [[pl:Entropia warunkowa]] [[zh:条件熵]]</text> </page> <page> <id>7711</id> <title>Conference Room Pilot</title> <text>{{Orphan|date=March 2007}} '''Conference Room Pilot (CRP)''' is a term used in [[software]] procurement and software acceptance testing. A CRP may be used 