lack of information in the data and the model used in the analyst's macroscopic description of the system, and also what those data say about the nature of the underlying reality. The fitness of the probabilities depends on whether the constraints of the specified macroscopic model are a sufficiently accurate and/or complete description of the system to capture all of the experimentally reproducible behaviour. This cannot be guaranteed, ''a priori''. For this reason MaxEnt proponents also call the method '''predictive statistical mechanics'''. The predictions can fail. But if they do, this is informative, because it signals the presence of new constraints needed to capture reproducible behaviour in the system, which had not been taken into account. === Is entropy "real" ? === The thermodynamic entropy (at equilibrium) is a function of the state variables of the model description. It is therefore as "real" as the other variables in the model description. If the model constraints in the probability assignment are a "good" description, containing all the information needed to predict reproducible experimental results, then that includes all of the results one could predict using the formulae involving entropy from classical thermodynamics. To that extent, the MaxEnt ''S<sub>Th</sub>'' is as "real" as the entropy in classical thermodynamics. Of course, in reality there is only one real state of the system. The entropy is not a direct function of that state. It is a function of the real state only through the (subjectively chosen) macroscopic model description. === Is ergodic theory relevant ? === The Gibbsian [[statistical ensemble|ensemble]] idealises the notion of repeating an experiment again and again on ''different'' systems, not again and again on the ''same'' system. So long-term time averages and the [[ergodic hypothesis]], despite the intense interest in them in the first part of the twentieth century, strictly speaking are not relevant to the probability assignment for the state one might find the system in. However, this changes if there is additional knowledge that the system is being prepared in a particular way some time before the measurement. One must then consider whether this gives further information which is still relevant at the time of measurement. The question of how 'rapidly mixing' different properties of the system are then becomes very much of interest. Information about some degrees of freedom of the combined system may become unusable very quickly; information about other properties of the system may go on being relevant for a considerable time. If nothing else, the medium and long-run time correlation properties of the system are interesting subjects for experimentation in themselves. Failure to accurately predict them is a good indicator that relevant macroscopically determinable physics may be missing from the model. === The Second Law === According to [[Liouville's theorem (Hamiltonian)|Liouville's theorem]] for [[Hamiltonian dynamics]], the hyper-volume of a cloud of points in [[phase space]] remains constant as the system evolves. Therefore, the information entropy must also remain constant, if we condition on the original information, and then follow each of those microstates forward in time: :<math>\Delta S_I = 0 \,</math> However, as time evolves, that initial information we had becomes less directly accessible. Instead of being easily summarisable in the macroscopic description of the system, it increasingly relates to very subtle correlations between the positions and momenta of individual molecules. (Compare to Boltzmann's [[H-theorem]].) Equivalently, it means that the probability distribution for the whole system, in 6N-dimensional phase space, becomes increasingly irregular, spreading out into long thin fingers rather than the initial tightly defined volume of possibilities. Classical thermodynamics is built on the assumption that entropy is a [[state function]] of the [[macroscopic variable]]s -- i.e., that none of the history of the system matters, so that it can all be ignored. The extended, wispy, evolved probability distribution, which still has the initial Shannon entropy ''S<sub>Th</sub><sup>(1)</sup>'', should reproduce the expectation values of the observed macroscopic variables at time ''t<sub>2</sub>''. However it will no longer necessarily be a maximum entropy distribution for that new macroscopic description. On the other hand, the new thermodynamic entropy ''S<sub>Th</sub><sup>(2)</sup>'' assuredly ''will'' measure the maximum entropy distribution, by construction. Therefore, we expect: :<math>{S_{Th}}^{(2)} \geq {S_{Th}}^{(1)} </math> At an abstract level, this result simply means that some of the information we originally had about the system has become "no longer useful" at a macroscopic level. At the level of the 6''N''-dimensional probability distribution, this result represents [[coarse graining]] -- i.e., information loss by smoothing out very fine-scale detail. === Caveats with the argument === Some caveats should be considered with the above. 1. Like all statistical mechanical results according to the MaxEnt school, this increase in thermodynamic entropy is only a ''prediction''. It assumes in particular that the initial macroscopic description contains all of the information relevant to predicting the later macroscopic state. This may not be the case, for example if the initial description fails to reflect some aspect of the preparation of the system which later becomes relevant. In that case the "failure" of a MaxEnt prediction tells us that there is something more which is relevant that we may have overlooked in the physics of the system. It is also sometimes suggested that [[quantum measurement]], especially in the [[decoherence]] interpretation, may give an apparently unexpected reduction in entropy per this argument, as it appears to involve macroscopic information becoming available which was previously inaccessible. (However, the entropy accounting of quantum measurement is tricky, because to get full decoherence one may be assuming an infinite environment, with an infinite entropy). 2. The argument so far has glossed over the question of ''fluctuations''. It has also implicitly assumed that the uncertainty predicted at time ''t<sub>1</sub>'' for the variables at time ''t<sub>2</sub>'' will be much smaller than the measurement error. But if the measurements do meaningfully update our knowledge of the system, our uncertainty as to its state is reduced, giving a new ''S<sub>'''I'''</sub><sup>(2)</sup>'' which is ''less'' than ''S<sub>'''I'''</sub><sup>(1)</sup>''. (Note that if we allow ourselves the abilities of [[Laplace's demon]], the consequences of this new information can also be mapped 