of the same characteristics as clusters, but MPPs have specialized interconnect networks (whereas clusters use commodity hardware for networking). MPPs also tend to be larger than clusters, typically having "far more" than 100 processors.<ref>Hennessy and Patterson, p. 537.</ref> In an MPP, "each CPU contains its own memory and copy of the operating system and application. Each subsystem communicates with the others via a high-speed interconnect."<ref>[http://www.pcmag.com/encyclopedia_term/0,,t=mpp&i=47310,00.asp MPP Definition.] ''PC Magazine''. Retrieved on November 7, 2007.</ref> [[Image:BlueGeneL cabinet.jpg|right|thumbnail|upright|A cabinet from [[Blue Gene]]/L, ranked as the fourth fastest supercomputer in the world according to the 11/2008 [[TOP500]] rankings. Blue Gene/L is a massively parallel processor.]] [[Blue Gene/L]], the fifth fastest supercomputer in the world according to the June 2009 TOP500 ranking, is an MPP. =====Grid computing===== {{main|Grid computing}} Grid computing is the most distributed form of parallel computing. It makes use of computers communicating over the [[Internet]] to work on a given problem. Because of the low bandwidth and extremely high latency available on the Internet, grid computing typically deals only with [[embarrassingly parallel]] problems. [[List of distributed computing projects|Many grid computing applications]] have been created, of which [[SETI@home]] and [[Folding@Home]] are the best-known examples.<ref>Kirkpatrick, Scott (January 31, 2003). "Computer Science: Rough Times Ahead". ''Science'', Vol. 299. No. 5607, pp. 668 - 669. DOI: 10.1126/science.1081623</ref> Most grid computing applications use [[middleware]], software that sits between the operating system and the application to manage network resources and standardize the software interface. The most common grid computing middleware is the [[Berkeley Open Infrastructure for Network Computing]] (BOINC). Often, grid computing software makes use of "spare cycles", performing computations at times when a computer is idling. ====Specialized parallel computers==== Within parallel computing, there are specialized parallel devices that remain niche areas of interest. While not [[Domain-specific programming language|domain-specific]], they tend to be applicable to only a few classes of parallel problems. =====Reconfigurable computing with field-programmable gate arrays===== [[Reconfigurable computing]] is the use of a [[field-programmable gate array]] (FPGA) as a co-processor to a general-purpose computer. An FPGA is, in essence, a computer chip that can rewire itself for a given task. FPGAs can be programmed with [[hardware description language]]s such as [[VHDL]] or [[Verilog]]. However, programming in these languages can be tedious. Several vendors have created [[C to HDL]] languages that attempt to emulate the syntax and/or semantics of the [[C programming language]], with which most programmers are familiar. The best known C to HDL languages are [[Mitrion-C]], [[Impulse C]], [[DIME-C]], and [[Handel-C]]. Specific subsets of [[SystemC]] based on C++ can also be used for this purpose. AMD's decision to open its [[HyperTransport]] technology to third-party vendors has become the enabling technology for high-performance reconfigurable computing.<ref name="DAmour">D'Amour, Michael R., Chief Operating Officer, [[DRC Computer Corporation]]. "Standard Reconfigurable Computing". Invited speaker at the University of Delaware, February 28, 2007.</ref> According to Michael R. D'Amour, Chief Operating Officer of [[DRC Computer Corporation]], "when we first walked into AMD, they called us 'the [[CPU socket|socket]] stealers.' Now they call us their partners."<ref name="DAmour"/> =====General-purpose computing on graphics processing units (GPGPU)===== {{main|GPGPU}} {{Further|[[Molecular modeling on GPU]]}} [[Image:NvidiaTesla.jpg|right|thumbnail|Nvidia's [[Nvidia Tesla|Tesla GPGPU card]]]] General-purpose computing on [[graphics processing unit]]s (GPGPU) is a fairly recent trend in computer engineering research. GPUs are co-processors that have been heavily optimized for [[computer graphics]] processing.<ref>Boggan, Sha'Kia and Daniel M. Pressel (August 2007). [http://www.arl.army.mil/arlreports/2007/ARL-SR-154.pdf GPUs: An Emerging Platform for General-Purpose Computation] (PDF). ARL-SR-154, U.S. Army Research Lab. Retrieved on November 7, 2007.</ref> Computer graphics processing is a field dominated by data parallel operations—particularly [[linear algebra]] [[Matrix (mathematics)|matrix]] operations. In the early days, GPGPU programs used the normal graphics APIs for executing programs. However, recently several new programming languages and platforms have been built to do general purpose computation on GPUs with both [[Nvidia]] and [[AMD]] releasing programming environments with [[CUDA]] and [[Close to Metal|CTM]] respectively. Other GPU programming languages are [[BrookGPU]], [[PeakStream]], and [[RapidMind]]. Nvidia has also released specific products for computation in their [[Nvidia Tesla|Tesla series]]. =====Application-specific integrated circuits===== {{main|Application-specific integrated circuit}} Several [[application-specific integrated circuit]] (ASIC) approaches have been devised for dealing with parallel applications.<ref>Maslennikov, Oleg (2002). [http://www.springerlink.com/content/jjrdrb0lelyeu3e9/ "Systematic Generation of Executing Programs for Processor Elements in Parallel ASIC or FPGA-Based Systems and Their Transformation into VHDL-Descriptions of Processor Element Control Units".] ''Lecture Notes in Computer Science'', '''2328/2002:''' p. 272.</ref><ref>Shimokawa, Y.; Y. Fuwa and N. Aramaki (18–21 November 1991). [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=170708 A parallel ASIC VLSI neurocomputer for a large number of neurons and billion connections per second speed.] IEEE International Joint Conference on Neural Networks. '''3:''' pp. 2162–67.</ref><ref>Acken, K.P.; M.J. Irwin, R.M. Owens (July 1998). [http://www.ingentaconnect.com/content/klu/vlsi/1998/00000019/00000002/00167697?crawler=true "A Parallel ASIC Architecture for Efficient Fractal Image Coding".] ''The Journal of VLSI Signal Processing'', '''19'''(2):97–113(17)</ref> Because an ASIC is (by definition) specific to a given application, it can be fully optimized for that application. As a result, for a given application, an ASIC tends to outperform a general-purpose computer. However, ASICs are created by [[X-ray lithography]]. This process requires a mask, which can be extremely expensive. A single mask can cost over a million US dollars.<ref>Kahng, Andrew B. (June 21, 2004) "[http://www.future-fab.com/documents.asp?grID=353&d_ID=2596 Scoping the Problem of DFM in the Semiconductor Industry]." University of California, San Diego. "Future design for manufacturing (DFM) technology must reduce design [non-recoverable expenditure] cost and directly address manufacturing [non-recoverable expenditures] – the cost of a mask set and probe card – which is well over $1 million at the 90 nm technology node and creates a significant damper on semiconductor-based innovation."</ref> (The smaller the transistors required for the chip, the more expensive the mask will be.) Meanwhile, performance increases in general-purpose computing over time (as described by Moore's Law) tend to wipe out these gains in only one or two chip generations.<ref name="DAmour"/> High initial cost, and the tendency to be overtaken by Moore's-law-driven general-purpose computing, has rendered ASICs unfeasible for most parallel computing applications. However, some have been built. One example is the peta-flop [[RIKEN MDGRAPE-3]] machine which uses custom ASICs for [[molecular dynamics]] simulation. =====Vector processors===== {{main|Vector processor}} [[Image:Cray-1-p1010221.jpg|right|thumbnail|The [[Cray-1]] is the most famous vector processor.]] A vector processor is a CPU or 