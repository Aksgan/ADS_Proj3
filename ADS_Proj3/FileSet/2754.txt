software was [[CANDIDE]] from [[IBM]]. Google used [[SYSTRAN]] for several years, but switched to a statistical translation method in October 2007. Recently, they improved their translation capabilities by inputting approximately 200 billion words from [[United Nations]] materials to train their system. Accuracy of the translation has improved.<ref>[http://blog.outer-court.com/archive/2005-05-22-n83.html Google Translator: The Universal Language<!-- Bot generated title -->]</ref> ===Example-based=== {{Main|Example-based machine translation}} Example-based machine translation (EBMT) approach was proposed by [[Makoto Nagao]] in 1984.<ref>Nagao, M. 1981. A Framework of a Mechanical Translation between Japanese and English by Analogy Principle, in Artificial and Human Intelligence, A. Elithorn and R. Banerji (eds.) North- Holland, pp. 173-180, 1984.</ref><ref>{{Cite web | url = http://www.aclweb.org/index.php?option=com_content&task=view&id=36&Itemid=30 | title = the Association for Computational Linguistics - 2003 ACL Lifetime Achievement Award | publisher = Association for Computational Linguistics | accessdate = 2010-03-10}}</ref> It is often characterised by its use of a bilingual [[Text corpus|corpus]] as its main knowledge base, at run-time. It is essentially a translation by [[analogy]] and can be viewed as an implementation of [[case-based reasoning]] approach of [[machine learning]]. ===Hybrid MT=== Hybrid machine translation (HMT) leverages the strengths of statistical and rule-based translation methodologies.<ref name="speechtechmag.com">[http://www.speechtechmag.com/Articles/News/News-Feature/AppTek-Launches-Hybrid-Machine-Translation-Software-52871.aspx Boretz, Adam, "AppTek Launches Hybrid Machine Translation Software" SpeechTechMag.com (posted 2 MAR 2009) ]</ref> Several MT companies ([[Asia Online]], LinguaSys, [[Systran]], [[Polytechnic_University_of_Valencia|UPV]]) are claiming to have a hybrid approach using both rules and statistics. The approaches differ in a number of ways: * '''Rules post-processed by statistics''': Translations are performed using a rules based engine. Statistics are then used in an attempt to adjust/correct the output from the rules engine. * '''Statistics guided by rules''': Rules are used to pre-process data in an attempt to better guide the statistical engine. Rules are also used to post-process the statistical output to perform functions such as normalization. This approach has a lot more power, flexibility and control when translating. ==Major issues== ===Disambiguation=== {{Main|Word sense disambiguation}} Word-sense disambiguation concerns finding a suitable translation when a word can have more than one meaning. The problem was first raised in the 1950s by [[Yehoshua Bar-Hillel]].<ref>[http://ourworld.compuserve.com/homepages/WJHutchins/Miles-6.htm Milestones in machine translation - No.6: Bar-Hillel and the nonfeasibility of FAHQT] by John Hutchins</ref> He pointed out that without a "universal encyclopedia", a machine would never be able to distinguish between the two meanings of a word.<ref>Bar-Hillel (1960), "Automatic Translation of Languages". Available online at http://www.mt-archive.info/Bar-Hillel-1960.pdf</ref> Today there are numerous approaches designed to overcome this problem. They can be approximately divided into "shallow" approaches and "deep" approaches. Shallow approaches assume no knowledge of the text. They simply apply statistical methods to the words surrounding the ambiguous word. Deep approaches presume a comprehensive knowledge of the word. So far, shallow approaches have been more successful. {{Citation needed|date=April 2007}} The late [[Claude Piron]], a long-time translator for the [[United Nations]] and the [[World Health Organization]], wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve [[ambiguity|ambiguities]] in the [[source text]], which the [[grammatical]] and [[lexical]] exigencies of the [[target language]] require to be resolved: : Why does a translator need a whole workday to translate five pages, and not an hour or two? ..... About 90% of an average text corresponds to these simple conditions. But unfortunately, there's the other 10%. It's that part that requires six [more] hours of work. There are ambiguities one has to resolve. For instance, the author of the source text, an Australian physician, cited the example of an epidemic which was declared during World War II in a "Japanese prisoner of war camp". Was he talking about an American camp with Japanese prisoners or a Japanese camp with American prisoners? The English has two senses. It's necessary therefore to do research, maybe to the extent of a phone call to Australia.<ref name="piron">[[Claude Piron]], ''Le d√©fi des langues'' (The Language Challenge), Paris, L'Harmattan, 1994. <!-- GFDL translation by Jim Henry --></ref> The ideal deep approach would require the translation software to do all the research necessary for this kind of disambiguation on its own; but this would require a higher degree of [[AI]] than has yet been attained. A shallow approach which simply guessed at the sense of the ambiguous English phrase that Piron mentions (based, perhaps, on which kind of prisoner-of-war camp is more often mentioned in a given corpus) would have a reasonable chance of guessing wrong fairly often. A shallow approach that involves "ask the user about each ambiguity" would, by Piron's estimate, only automate about 25% of a professional translator's job, leaving the harder 75% still to be done by a human. ===Named entities=== {{Expand section|date=January 2010}} Related to [[named entity recognition]] in [[information extraction]]. ==Applications== There are now many [[software]] programs for translating natural language, several of them [[online]], such as: *[[Ta with you]] [http://www.tauyou.com] is specialized in customized machine translation solutions in any language. Their web-based user interface makes it easy for any Language Service Provider to generate any combination of domain and language pair to achieve the best quality. Their solution works with almost human quality for combinations from/to Spanish. *[[LinguaSys]] [http://www.linguasys.net] provides highly customized hybrid machine translation that can go from any language to any language. *[[Asia Online]][http://www.asiaonline.net] provides a custom machine translation engine building capability that they claim gives near-human quality compared to the "gist" based quality of free online engines. [[Asia Online]] also provides tools to edit and create custom machine translation engines with their [http://www.languagestudio.com Language Studio] suite of products. *[[Hindi to Punjabi Machine Translation System]][http://h2p.learnpunjabi.org], provides machine translation using a direct approach. It translates Hindi into Punjabi. It also features writing e-mail in the Hindi language and sending the same in Punjabi to the recipient. * [https://sites.google.com/site/khaledshaalan/publications/journal-papers/Generating_Arabic_MT_journal.zip?attredirects=0 Arabic machine translation] in multilingual framework. *[[Worldlingo]] provides machine translation using both statistical based TE's and rule based TE's. Most recognizable as the MT partner in Microsoft Windows and [[Microsoft Office 2008 for Mac|Microsoft Mac Office]]. *[[Power Translator]] *[[SDL ETS]] and [[Language Weaver|SDL Language Weaver]] which power [[FreeTranslation.com (website)]] *[[SYSTRAN]], 