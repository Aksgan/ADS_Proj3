is reused to minimize external memory bandwidth. Since the kernel and stream abstractions expose data dependencies, compiler tools can fully automate and optimize on-chip management tasks. Stream processing hardware can use [[scoreboarding]], for example, to launch [[direct memory access|DMA]]s at runtime, when dependencies become known. The elimination of manual DMA management reduces software complexity, and the elimination of hardware caches reduces the amount of die area not dedicated to computational units such as [[Arithmetic logic unit|ALU]]s. During the 1980s stream processing was explored within [[dataflow programming]]. An example is the language [[SISAL]] (Streams and Iteration in a Single Assignment Language). == Applications == Stream processing is essentially a compromise, driven by a data-centric model that works very well for traditional DSP or GPU-type applications (such as image, video and digital signal processing) but less so for general purpose processing with more randomized data access (such as databases). By sacrificing some flexibility in the model, the implications allow easier, faster and more efficient execution. Depending on the context, [[central processing unit|processor]] design may be tuned for maximum efficiency or a trade-off for flexibility. Stream processing is especially suitable for applications that exhibit three application characteristics{{Citation needed|date=June 2008}}: <ul> <li>'''Compute Intensity''', the number of arithmetic operations per I/O or global memory reference. In many signal processing applications today it is well over 50:1 and increasing with algorithmic complexity.</li> <li>'''Data Parallelism''' exists in a kernel if the same function is applied to all records of an input stream and a number of records can be processed simultaneously without waiting for results from previous records. </li> <li>'''Data Locality''' is a specific type of temporal locality common in signal and media processing applications where data is produced once, read once or twice later in the application, and never read again. Intermediate streams passed between kernels as well as intermediate data within kernel functions can capture this locality directly using the stream processing programming model.</li> </ul> <!-- Could we please leave finding examples to the readers? Examples of records within streams include: * In graphics, each record might be the vertex, normal, and color information for a triangle; * In image processing, each record might be a single pixel from an image; * In a video encoder, each record may be 256 pixels forming a macroblock of data; or * In wireless signal processing, each record could be a sequence of samples received from an antenna. For each record we can only read from the input, perform operations on it, and write to the output. It is permissible to have multiple inputs and multiple outputs, but never a piece of memory that is both readable and writable. <!-- We could discuss weeks on whatever this can be considered generally true by pointing out the implications of the exact wording. --> == Comparison to prior parallel paradigms == Basic computers started from a sequential execution paradigm. Traditional [[Central processing unit|CPU]]s are [[SISD]] based, which means they conceptually perform only one operation at a time. As the computing needs of the world evolved, the amount of data to be managed increased very quickly. It was obvious that the sequential programming model could not cope with the increased need for processing power. Various efforts have been spent on finding alternative ways to perform massive amounts of computations but the only solution was to exploit some level of parallel execution. The result of those efforts was [[SIMD]], a programming paradigm which allowed applying one instruction to multiple instances of (different) data. Most of the time, SIMD was being used in a [[SWAR]] environment. By using more complicated structures, one could also have [[MIMD]] parallelism. Although those two paradigms were efficient, real-world implementations were plagued with limitations from memory alignment problems to synchronization issues and limited parallelism. Only few SIMD processors survived as stand-alone components; most were embedded in standard CPUs. Consider a simple program adding up two arrays containing 100 4-component [[Vector (geometric)|vector]]s (i.e. 400 numbers in total). === Conventional, sequential paradigm === <source lang="c"> for(int i = 0; i < 100 * 4; i++) result[i] = source0[i] + source1[i]; </source> This is the sequential paradigm that is most familiar. Variations do exist (such as inner loops, structures and such) but they ultimately boil down to that. ===Parallel SIMD paradigm, packed registers (SWAR)=== <source lang="c"> for(int el = 0; el < 100; el++) // for each vector vector_sum(result[el], source0[el], source1[el]); </source> This is actually oversimplified. It assumes the instruction <code>vector_sum</code> works. Although this is what happens with [[Intrinsic function|instruction intrinsics]], much information is actually not taken into account here such as the number of vector components and their data format. This is done for clarity. You can see however, this method reduces the number of decoded instructions from ''numElements * componentsPerElement'' to ''numElements''. The number of jump instructions is also decreased, as the loop is run fewer times. These gains result from the parallel execution of the four mathematical operations. What happened however is that the packed SIMD register holds a certain amount of data so it's not possible to get more parallelism. The speed up is somewhat limited by the assumption we made of performing four parallel operations (please note this is common for both [[AltiVec]] and [[Streaming SIMD Extensions|SSE]]). ===Parallel Stream paradigm (SIMD/MIMD)=== <source lang="c"> // This is a fictional language for demonstration purposes. elements = array streamElement([number,number])[100] kernel = instance streamKernel("@arg0[@iter]") result = kernel.invoke(elements) </source> As you can see, the idea is to define the whole set of data instead of each single block. Describing the set of data is assumed to be in the first two rows. After that, the result is inferred from the sources and kernel. For simplicity, there's a 1:1 mapping between input and output data but this does not need to be. Applied kernels can also be much more complex. An implementation of this paradigm can "unroll" a loop internally. This allows throughput to scale with chip complexity, easily utilizing hundreds of ALUs.{{ref|scale}} The elimination of complex data patterns makes much of 