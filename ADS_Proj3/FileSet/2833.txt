[[calculus]] and [[Lagrange multipliers]]. === Caveats === Note that not all classes of distributions contain a maximum entropy distribution. It is possible that a class contain distributions of arbitrarily large entropy (e.g. the class of all continuous distributions on '''R''' with mean 0), or that the entropies are bounded above but there is no distribution which attains the maximal entropy (e.g. the class of all continuous distributions ''X'' on '''R''' with E(''X'') = 0 and E(''X''<sup>2</sup>) = E(''X''<sup>3</sup>) = 1 {{Citation needed|date=April 2010}}). It is also possible that the expected value restrictions for the class ''C'' force the probability distribution to be zero in certain subsets of ''S''. In that case our theorem doesn't apply, but one can work around this by shrinking the set ''S''. == See also== * [[Exponential family]] * [[Gibbs measure]] * [[Partition function (mathematics)]] == Notes == {{Reflist}} {{No footnotes|date=August 2009}} == References == * T. M. Cover and J. A. Thomas, ''Elements of Information Theory'', 1991. Chapter 11. * I. J. Taneja, ''[http://www.mtm.ufsc.br/~taneja/book/book.html Generalized Information Measures and Their Applications]'' 2001. [http://www.mtm.ufsc.br/~taneja/book/node14.html Chapter 1] {{ProbDistributions|families}} {{DEFAULTSORT:Maximum Entropy Probability Distribution}} [[Category:Entropy and information]] [[Category:Continuous distributions]] [[Category:Discrete distributions]] [[Category:Particle statistics]] [[Category:Types of probability distributions]]</text> </page> <page> <id>23819</id> <title>Maximum entropy thermodynamics</title> <text>In [[physics]], '''maximum entropy thermodynamics''' (colloquially, ''MaxEnt'' [[thermodynamics]]) views [[equilibrium thermodynamics]] and [[statistical mechanics]] as [[Inference#Inference and uncertainty|inference]] processes. More specifically, MaxEnt applies inference techniques rooted in [[Shannon information theory]], [[Bayesian probability]], and the [[principle of maximum entropy]]. These techniques are relevant to any situation requiring prediction from incomplete or insufficient data (e.g., [[image processing|image reconstruction]], [[signal processing]], [[spectral analysis]], and [[inverse problem]]s). MaxEnt thermodynamics began with two papers [[Edwin T. Jaynes]] published in the 1957 ''Physical Review''. == Maximum Shannon entropy == Central to the MaxEnt thesis is the [[principle of maximum entropy]], which states that given certain "testable information" about a [[probability distribution]], for example particular [[expectation]] values, but which is not in itself sufficient to uniquely determine the distribution, one should prefer the distribution which maximizes the [[Shannon entropy|Shannon information entropy]]. :<math>S_I = - \sum p_i \ln p_i</math> This is known as the [[Gibbs algorithm]], having been introduced by [[J. Willard Gibbs]] in 1878, to set up [[statistical ensemble]]s to predict the properties of thermodynamic systems at equilibrium. It is the cornerstone of the statistical mechanical analysis of the thermodynamic properties of equilibrium systems (see [[partition function]]). A direct connection is thus made between the equilibrium [[thermodynamic entropy]] ''S<sub>Th</sub>'', a [[state function]] of pressure, volume, temperature, etc., and the [[information entropy]] for the predicted distribution with maximum uncertainty conditioned only on the expectation values of those variables: :<math>S_{Th}(P,V,T,...)_{(eqm)} = k_B \, S_I(P,V,T,...)</math> ''k<sub>B</sub>'', [[Boltzmann's constant]], has no fundamental physical significance here, but is necessary to retain consistency with the previous historical definition of entropy by [[Clausius]] (1865) (see [[Boltzmann's constant]]). However, the MaxEnt school argue that the MaxEnt approach is a general technique of statistical inference, with applications far beyond this. It can therefore also be used to predict a distribution for "trajectories" Î“ "over a period of time" by maximising: :<math>S_I = - \sum p_{\Gamma} \ln p_{\Gamma}</math> This "information entropy" does ''not'' necessarily have a simple correspondence with thermodynamic entropy. But it can be used to predict features of [[non-equilibrium thermodynamics|nonequilibrium thermodynamic]] systems as they evolve over time. In the field of [[near-equilibrium thermodynamics]], the [[Onsager reciprocal relations]] and the [[Green-Kubo relations]] fall out very directly. The approach also creates a solid theoretical framework for the study of [[far-from-equilibrium thermodynamics]], making the derivation of the [[fluctuation theorem|entropy production fluctuation theorem]] particularly straightforward. Practical calculations for most far-from-equilibrium systems remain very challenging, however. ''Technical note'': For the reasons discussed in the article [[differential entropy]], the simple definition of Shannon entropy ceases to be directly applicable for [[random variable]]s with continuous [[probability distribution function]]s. Instead the appropriate quantity to maximise is the "relative information entropy," :<math>H_c=-\int p(x)\log\frac{p(x)}{m(x)}\,dx.</math> ''H<sub>c</sub>'' is the negative of the [[Kullback-Leibler divergence]], or discrimination information, of ''m''(''x'') from ''p''(''x''), where ''m''(''x'') is a prior [[invariant measure]] for the variable(s). The relative entropy ''H<sub>c</sub>'' is always less than zero, and can be thought of as (the negative of) the number of [[bit]]s of uncertainty lost by fixing on ''p''(''x'') rather than ''m''(''x''). Unlike the Shannon entropy, the relative entropy ''H<sub>c</sub>'' has the advantage of remaining finite and well-defined for continuous ''x'', and invariant under 1-to-1 coordinate transformations. The two expressions coincide for [[discrete probability distribution]]s, if one can make the assumption that ''m''(''x''<sub>i</sub>) is uniform - i.e. the [[principle of equal a-priori probability]], which underlies statistical thermodynamics. == Philosophical Implications == Adherents to the MaxEnt viewpoint take a clear position on some of the [[philosophy of thermal and statistical physics|conceptual/philosophical questions]] in thermodynamics. This position is sketched below. === The nature of the probabilities in statistical mechanics === Jaynes (1985<ref name="Jaynes 1985">Jaynes, E.T. (1985). Some random observations, ''Synthese'' '''63''': 115-138.</ref>, 2003<ref name="Jaynes 2003">Jaynes, E.T. (2003). ''Probability Theory: The Logic of Science'', posth. publ., ed. G.L. Bretthorst, Cambridge University Press, Cambridge, ISBN 0521592712.</ref>, ''et passim'') discussed the concept of probability. According to the MaxEnt viewpoint, the probabilities in statistical mechanics are determined jointly by two factors: by respectively specified particular models for the underlying state space (e.g. Liouvillian [[phase space]]); and by respectively specified particular partial descriptions of the system (the macroscopic description of the system used to constrain the MaxEnt probability assignment). The probabilities are [[Objectivity (science)|objective]] in the sense that, given these inputs, a uniquely defined probability distribution will result, independent of the subjectivity or arbitrary opinion of particular persons. The probabilities are epistemic in the sense that they are defined in terms of specified data and derived from those data by definite and objective rules of inference. Here the word epistemic, which refers to objective and impersonal scientific knowledge, is used in the sense that contrasts it with opiniative, which refers to the subjective or arbitrary beliefs of particular persons; this contrast was used by [[Plato]] and [[Aristotle]] and stands reliable today. The probabilities represent both the degree of knowledge and 