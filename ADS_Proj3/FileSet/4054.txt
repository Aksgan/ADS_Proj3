space]] of a matrix ''M''. The right singular vectors corresponding to vanishing singular values of ''M'' span the null space of ''M''. E.g., the null space is spanned by the last two columns of <math>V</math> in the above example. The left singular vectors corresponding to the non-zero singular values of ''M'' span the range of ''M''. As a consequence, the [[rank of a matrix|rank]] of ''M'' equals the number of non-zero singular values which is the same as the number of non-zero diagonal elements in <math>\Sigma</math>. In numerical linear algebra the singular values can be used to determine the ''effective rank'' of a matrix, as [[rounding error]] may lead to small but non-zero singular values in a rank deficient matrix. ===Low-rank matrix approximation=== Some practical applications need to solve the problem of approximating a matrix <math>M</math> with another matrix <math>\tilde{M}</math> which has a specific rank <math>r</math>. In the case that the approximation is based on minimizing the [[Frobenius norm]] of the difference between <math>M</math> and <math>\tilde{M}</math> under the constraint that <math>\mbox{rank}(\tilde{M}) = r</math> it turns out that the solution is given by the SVD of <math>M</math>, namely :<math> \tilde{M} = U \tilde{\Sigma} V^* </math> where <math>\tilde{\Sigma}</math> is the same matrix as <math>\Sigma</math> except that it contains only the <math>r</math> largest singular values (the other singular values are replaced by zero). This is known as the '''Eckart&ndash;Young theorem''', as it was proved by those two authors in 1936 (although it was later found to have been known to earlier authors; see G. W. Stewart, "On the early history of the singular value decomposition," ''SIAM Review'' '''35''', p. 551-556, 1993). '''Quick proof:''' We hope to minimize <math>\|M - \tilde M\|_F</math> subject to <math>\mbox{rank}(\tilde{M}) = r</math>. Suppose the SVD of <math> M = U \Sigma V^* </math>. Since the [[Frobenius norm]] is unitarily invariant, we have an equivalent statement: :<math>\min_{\tilde M} \|\Sigma - U^* \tilde M V\|_F.</math> Note that since <math>\Sigma</math> is diagonal, <math>U^* \tilde M V</math> should be diagonal in order to minimize the [[Frobenius norm]]. Remember that the [[Frobenius norm]] is the square-root of the summation of the squared modulus of all entries. This implies that <math>U</math> and <math>V</math> are also singular matrices of <math>\tilde M</math>. Thus we can assume that <math>\tilde M</math> to minimize the above statement has the form: :<math>{\tilde M} = U S V^*,</math> where <math>S</math> is diagonal. The diagonal entries <math>s_i</math> of <math>S</math> are not necessarily ordered as in SVD. :<math> \min_{\tilde M} \|\Sigma - S\|_F \equiv \min_{s_i} \sqrt {\sum_{i=1}^{n} (\sigma_i - s_i)^2 }. </math> From the rank constraint, i.e. <math>S</math> has <math>r</math> non-zero diagonal entries, the minimum of the above statement is obtained as follows: :<math> \min_{s_i} \sqrt {\sum_{i=1}^{r} (\sigma_i - s_i)^2 + \sum_{i=r+1}^{n} \sigma_i^2 } = \sqrt {\sum_{i=r+1}^{n} \sigma_i^2 }. </math> Therefore, <math>\tilde M</math> of rank <math>r</math> is the best approximation of <math>M</math> in the [[Frobenius norm]] sense when <math>\sigma_i = s_i \quad (i=1,\dots,r)</math> and the corresponding singular vectors are same as those of <math>M</math>. Another example of matrix approximation by SVD is the solution to the [[orthogonal Procrustes problem]] of approximating any given matrix by an orthogonal matrix. ===Separable models=== The SVD can be thought of as decomposing a matrix into a weighted, ordered sum of separable matrices. By separable, we mean that a matrix <math>\mathbf{A}</math> can be written as an [[outer product]] of two vectors <math>\mathbf{A}= \mathbf{u} \otimes \mathbf{v}</math>, or, in coordinates, <math>\mathbf{A(i,j)}= \mathbf{u}(i) \mathbf{v}(j)</math>. Specifically, the matrix M can be decomposed as: : <math>\mathbf{M} = \sum_i \mathbf{A}_i = \sum_i \sigma_i U_i \otimes V_i ^ \dagger.</math> Here <math>U_i</math> and <math>V_i</math> are the ''i''<sup>th</sup> columns of the corresponding SVD matrices, <math>\sigma_i</math> are the ordered singular values, and each <math>\mathbf{A}_i</math> is separable. The SVD can be used to find the decomposition of an image processing filter into separable horizontal and vertical filters.<ref>[http://blogs.mathworks.com/steve/2006/11/28/separable-convolution-part-2/ Separable convolution: Part 2 | Steve on Image Processing<!-- Bot generated title -->]</ref> Note that the number of non-zero <math>\sigma_i</math> is exactly the rank of the matrix. Separable models often arise in biological systems, and the SVD decomposition is useful to analyze such systems. For example, some visual area V1 simple cells receptive fields can be well described <ref>DeAngelis et al., Receptive-field dynamics in the central visual pathways, Trends Neurosci. 1995, {{Entrez Pubmed|8545912}}</ref> by a [[Gabor filter]] in the space domain multiplied by a modulation function in the time domain. Thus, given a linear filter evaluated through, for example, [[Spike-triggered average|reverse correlation]], one can rearrange the two spatial dimensions into one dimension, thus yielding a two dimensional filter (space, time) which can be decomposed through SVD. The first column of U in the SVD decomposition is then a Gabor while the first column of V represents the time modulation (or vice-versa). One may then define an index of separability, <math>\alpha = \sigma_1^2 / \sum_i \sigma_i^2</math>, which is the fraction of the power in the matrix M which is accounted for by the first separable matrix in the decomposition.<ref>Depireux et al., Spectro-temporal response field characterization with dynamic ripples in ferret primary auditory cortex, J. Neurophysiol. 2001, {{Entrez Pubmed|11247991}}</ref> ===Nearest orthonormal matrix=== In the [[orthogonal Procrustes problem]] we want to determine the [[orthonormal matrix]] closest to <math>A</math> using the [[Frobenius norm]]. The solution is the product <math>U V^*</math>.<ref>[http://www.wou.edu/~beavers/Talks/Willamette1106.pdf The Singular Value Decomposition in Symmetric (Lowdin) Orthogonalization and Data Compression]</ref> This intuitively makes sense because an orthonormal matrix would have the decomposition <math>U I V^*</math> where <math>I</math> is the identity matrix, so that if <math>A = U \Sigma V^*</math> then the product <math>A = U V^*</math> amounts to replacing the singular values with ones. ===The Kabsch Algorithm=== The [[Kabsch algorithm]] uses SVD to compute the optimal rotation (with respect to least-squares minimization) that will align a set of points with a corresponding set of points. It is used, among other applications, to compare the structures of molecules. ===Other examples=== The SVD is also applied extensively to the study of linear [[inverse problem]]s, and is useful in the analysis of regularization methods such as that of [[Tikhonov regularization|Tikhonov]]. It is widely used in [[statistics]] where it is 