of the variables themselves (i.e. the [[mean]]s); sometimes expectations of squared variables (which can be related to the [[variance]] of the variables), or expectations of higher powers (i.e. higher [[moment (mathematics)|moment]]s) also appear. In most cases, the other variables' distributions will be from known families, and the formulas for the relevant expectations can be looked up. However, those formulas depend on those distributions' parameters, which depend in turn on the expectations about other variables. The result is that the formulas for the parameters of each variable's distributions can be expressed as a series of equations with mutual, [[nonlinear]] dependencies among the variables. Usually, it is not possible to solve this system of equations directly. However, as described above, the dependencies suggest a simple iterative algorithm, which in most cases is guaranteed to converge. An example will make this process clearer. ==A Simple Example== Imagine a simple Bayesian model consisting of a single node with a [[Gaussian distribution]], with unknown [[mean]] and [[precision (statistics)|precision]] (or equivalently, an unknown [[variance]], since the precision is the reciprocal of the variance).<ref>Based on Chapter 10 of ''Pattern Recognition and Machine Learning'' by [[Christopher M. Bishop]]</ref> We place [[conjugate prior]] distributions on the unknown mean and variance, i.e. the mean also follows a Gaussian distribution while the precision follows a [[gamma distribution]]. In other words: :<math> \begin{array}{lcl} \mu & \sim & \mathcal{N}(\mu_0, (\lambda_0 \tau)^{-1}) \\ \tau & \sim & \text{Gam}(a_0, b_0) \\ \{x_1, \dots, x_N\} & \sim & \mathcal{N}(\mu, \tau^{-1}) \\ N &=& \text{number of data points} \end{array} </math> We are given <math>N</math> data points <math>\mathbf{X} = \{x_1, \dots, x_N\}</math> and our goal is to infer the [[posterior distribution]] <math>q(\mu,\tau)</math> of the parameters <math>\mu</math> and <math>\tau</math>. The [[hyperparameter]]s <math>\mu_0</math>, <math>\lambda_0</math>, <math>a_0</math> and <math>b_0</math> are fixed, given values. They can be set to small positive numbers to give broad prior distributions indicating ignorance about the prior distributions of <math>\mu</math> and <math>\tau</math>. The joint probability of all variables can be rewritten as :<math>p(\mathbf{X},\mu,\tau) = p(\mathbf{X}|\mu,\tau) p(\mu|\tau) p(\tau)</math> where the individual factors are :<math> \begin{array}{lcl} p(\mathbf{X}|\mu,\tau) & = & \prod_{n=1}^N \mathcal{N}(x_n|\mu,\tau^{-1}) \\ p(\mu|\tau) & = & \mathcal{N}(\mu|\mu_0, (\lambda_0 \tau)^{-1}) \\ p(\tau) & = & \text{Gam}(\tau|a_0, b_0) \end{array} </math> where :<math> \begin{array}{lcl} \mathcal{N}(x|\mu,\sigma^2) & = & \frac{1}{\sqrt{2\pi\sigma^2}} e^\frac{(x-\mu)^2}{2\sigma^2} \\ \text{Gam}(\tau|a,b) & = & \frac{1}{\Gamma(a)} b^a \tau^{a-1} e^{-b \tau} \end{array} </math> Assume that <math>q(\mu,\tau) = q(\mu)q(\tau)</math>, i.e. that the posterior distribution factorizes into independent factors for <math>\mu</math> and <math>\tau</math>. This type of assumption underlies the variational Bayesian method. The true posterior distribution does not in fact factor this way (in fact, in this simple case, it is known to be a [[Gaussian-gamma distribution]]), and hence the result we obtain will be an approximation. Then :<math> \begin{array}{lcl} \ln q_\mu^*(\mu) &=& \operatorname{E}_{\tau}[\ln p(\mathbf{X}|\mu,\tau) + \ln p(\mu|\tau)] + \text{const.} \\ &=& - \frac{\operatorname{E}[\tau]}{2} \{ \lambda_0(\mu-\mu_0)^2 + \sum_{n=1}^N (x_n-\mu)^2 \} + \text{const.} \end{array} </math> Note that the term <math>\operatorname{E}_{\tau}[\ln p(\tau)]</math> will be a function solely of <math>a_0</math> and <math>b_0</math> and hence is constant with respect to <math>\mu</math>; thus it has been absorbed into the constant term at the end. By expanding the squares inside of the braces, separating out and grouping the terms involving <math>\mu</math> and <math>\mu^2</math> and [[completing the square]] over <math>\mu</math>, we see that <math>q_\mu^*(\mu)</math> is a [[Gaussian distribution]] <math>\mathcal{N}(\mu|\mu_N,\lambda_N^{-1})</math> where we have defined: :<math> \begin{array}{lcl} \mu_N &=& \frac{\lambda_0 \mu_0 + N \bar{x}}{\lambda_0 + N} \\ \lambda_N &=& (\lambda_0 + N) \operatorname{E}[\tau] \end{array} </math> Similarly, :<math> \begin{array}{lcl} \ln q_\tau^*(\tau) &=& \operatorname{E}_{\mu}[\ln p(\mathbf{X}|\mu,\tau) + \ln p(\mu|\tau)] + \ln p(\tau) + \text{const.} \\ &=& (a_0 - 1) \ln \tau - b_0 \tau + \frac{N}{2} \ln \tau - \frac{\tau}{2} \operatorname{E}_\mu [\sum_{n=1}^N (x_n-\mu)^2 + \lambda_0(\mu - \mu_0)^2] + \text{const.} \end{array} </math> Exponentiating both sides, we can see that <math>q_\tau^*(\tau)</math> is a [[gamma distribution]] <math>\text{Gam}(\tau|a_N, b_N)</math> where we have defined :<math> \begin{array}{lcl} a_N &=& a_0 + \frac{N+1}{2} \\ b_N &=& b_0 + \frac{1}{2} \operatorname{E}_\mu [\sum_{n=1}^N (x_n-\mu)^2 + \lambda_0(\mu - \mu_0)^2] \end{array} </math> In each case, the parameters for the distribution over one of the variables depend on expectations taken with respect to the other variable. The formulas for the expectations of moments of the Gaussian and gamma distributions are well-known, but depend on the parameters. Hence the formulas for each distribution's parameters depend on the other distribution's parameters. This naturally suggests an [[expectation-maximization algorithm|EM]]-like algorithm where we first initialize the parameters to some values (perhaps the values of the hyperparameters of the corresponding prior distributions) and iterate, computing new values for each set of parameters using the current values of the other parameters. This is guaranteed to converge to a local maximum, and since both posterior distributions are in the [[exponential family]], this local maximum will be a [[global maximum]]. Note also that the posterior distributions have the same form as the corresponding prior distributions. We did ''not'' assume this; the only assumption we made was that the distributions factorize, and the form of the distributions followed naturally. It turns out (see below) that the fact that the posterior distributions have the same form as the prior distributions is not a coincidence, but a general result whenever the prior distributions are members of the [[exponential family]], which is the case for most of the standard distributions. ==A More Complex Example== Imagine a Bayesian [[Gaussian mixture model]] described as follows: :<math> \begin{array}{lcl} \mathbf{\pi} & \sim & \text{SymDir}(K, \alpha_0) \\ \mathbf{\Lambda}_{i=1 \dots K} & \sim & \mathcal{W}(\mathbf{W}_0, \nu_0) \\ \mathbf{\mu}_{i=1 \dots K} & \sim & \mathcal{N}(\mathbf{\mu}_0, (\beta_0 \mathbf{\Lambda}_i)^{-1}) \\ \mathbf{z}[i = 1 \dots N] & \sim & \text{Mult}(1, \mathbf{\pi}) \\ \mathbf{x}_{i=1 \dots N} & \sim & \mathcal{N}(\mathbf{\mu}_{z_i}, {\mathbf{\Lambda}_{z_i}}^{-1}) \\ K &=& \text{number of mixing components} \\ N &=& \text{number of data points} \end{array} </math> Note: *SymDir() is the symmetric [[Dirichlet distribution]] of dimension <math>K</math>, with the hyperparameter for each component set to <math>\alpha_0</math>. The Dirichlet distribution is the [[conjugate prior]] of the [[categorical distribution]] or [[multinomial distribution]]. *<math>\mathcal{W}()</math> is the [[Wishart distribution]], which is the conjugate prior of the [[precision matrix]] (inverse [[covariance matrix]]) for a [[multivariate Gaussian distribution]]. *Mult() is a [[multinomial distribution]] over a single observation. The state 