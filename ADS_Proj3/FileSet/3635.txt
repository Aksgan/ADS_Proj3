Philadelphia, PA, USA, 2007, ISBN 0-89871-641-1, 9780898716412</ref> suggested changing its name to ''Francis algorithm''. [[Gene_H._Golub|Golub]] and [[Charles_F._Van_Loan|Van Loan]] use the term ''Francis QR step''. == Interpretation and convergence == The QR algorithm can be seen as a more sophisticated variation of the basic "power" [[eigenvalue algorithm]]. Recall that the power algorithm repeatedly multiplies ''A'' times a single vector, normalizing after each iteration. The vector converges to an eigenvector of the largest eigenvalue. Instead, the QR algorithm works with a complete basis of vectors, using QR decomposition to renormalize (and orthogonalize). For a symmetric matrix ''A'', upon convergence, ''AQ'' = ''Q&Lambda;'', where ''&Lambda;'' is the diagonal matrix of eigenvalues to which ''A'' converged, and where ''Q'' is a composite of all the orthogonal similarity transforms required to get there. Thus the columns of ''Q'' are the eigenvectors. == Other variants== One variant of the ''QR algorithm'', ''the Golub-Kahan-Reinsch'' algorithm starts with reducing a general matrix into a bidiagonal one.<ref>Bochkanov Sergey Anatolyevich. ALGLIB User Guide - General Matrix operations - Singular value decomposition . ALGLIB Project. 2010-12-11. URL:http://www.alglib.net/matrixops/general/svd.php. Accessed: 2010-12-11. (Archived by WebCite® at http://www.webcitation.org/5utO4iSnR)</ref>. This variant of the ''QR algorithm'' for the computation of eigenvalues, which was first described by {{harvtxt|Golub|Kahan|1965}}. The [[LAPACK]] subroutine [http://www.netlib.org/lapack/double/dbdsqr.f DBDSQR] implements this iterative method, with some modifications to cover the case where the singular values are very small {{harv|Demmel|Kahan|1990}}. Together with a first step using Householder reflections and, if appropriate, [[QR decomposition]], this forms the [http://www.netlib.org/lapack/double/dgesvd.f DGESVD] routine for the computation of the [[singular value decomposition]]. == References == <references/> == External links == * {{planetmath reference|id=1474|title=Eigenvalue problem}} * [http://www.math.umn.edu/~olver/aims_/qr.pdf Prof. Peter Olver's notes on orthogonal bases and the workings of the QR algorithm] * [http://math.fullerton.edu/mathews/n2003/QRMethodMod.html Module for the QR Method] {{Numerical linear algebra}} [[Category:Numerical linear algebra]] [[de:QR-Algorithmus]] [[ja:QR法]] [[fi:QR-algoritmi]]</text> </page> <page> <id>30662</id> <title>QR decomposition</title> <text>In [[linear algebra]], a '''QR decomposition''' (also called a '''QR factorization''') of a [[matrix (mathematics)|matrix]] is a [[matrix decomposition|decomposition]] of the matrix into an [[orthogonal matrix|orthogonal]] and an [[upper triangular matrix]]. QR decomposition is often used to solve the [[linear least squares (mathematics)|linear least squares]] problem, and is the basis for a particular [[eigenvalue algorithm]], the [[QR algorithm]]. ==Definition== ===Square matrix=== Any real square matrix ''A'' may be decomposed as : <math> A = QR, \, </math> where ''Q'' is an [[orthogonal matrix]] (its columns are [[orthogonal]] [[unit vector]]s meaning ''Q''<sup>T</sup>''Q'' = ''I'') and ''R'' is an upper [[triangular matrix]] (also called right triangular matrix). This generalizes to a complex square matrix ''A'' and a [[unitary matrix]] ''Q''. If ''A'' is [[invertible matrix|nonsingular]], then the factorization is unique if we require that the diagonal elements of ''R'' are positive. ===Rectangular matrix=== More generally, we can factor a complex ''m''&times;''n'' matrix ''A'', with ''m'' ≥ ''n'', as the product of an ''m''&times;''m'' [[unitary matrix]] ''Q'' and an ''m''&times;''n'' upper triangular matrix ''R''. As the bottom (''m''&minus;''n'') rows of an ''m''&times;''n'' upper triangular matrix consist entirely of zeroes, it is often useful to partition ''R'', or both ''R'' and ''Q''.: :<math> A = QR = Q \begin{bmatrix} R_1 \\ 0 \end{bmatrix} = \begin{bmatrix} Q_1, Q_2 \end{bmatrix} \begin{bmatrix} R_1 \\ 0 \end{bmatrix} = Q_1 R_1, </math> where ''R''<sub>1</sub> is an ''n''×''n'' upper triangular matrix, ''Q''<sub>1</sub> is ''m''×''n'', ''Q''<sub>2</sub> is ''m''×(''m''&minus;''n''), and ''Q''<sub>1</sub> and ''Q''<sub>2</sub> both have orthogonal columns. {{harvtxt|Golub|Van Loan|1996|loc=§5.2}} call ''Q''<sub>1</sub>''R''<sub>1</sub> the ''thin QR factorization'' of ''A''. If ''A'' is of full [[matrix rank|rank]] ''n'' and we require that the diagonal elements of ''R''<sub>1</sub> are positive then ''R''<sub>1</sub> and ''Q''<sub>1</sub> are unique, but in general ''Q''<sub>2</sub> is not. ''R''<sub>1</sub> is then equal to the upper triangular factor of the [[Cholesky decomposition]] of ''A''* ''A'' (= ''A''<sup>T</sup>''A'' if ''A'' is real). ===QL, RQ and LQ decompositions=== Analogously, we can define QL, RQ, and LQ decompositions, with ''L'' being a ''left'' triangular matrix. ==Computing the QR decomposition== There are several methods for actually computing the QR decomposition, such as by means of the [[Gram–Schmidt process]], [[Householder transformation]]s, or [[Givens rotation]]s. Each has a number of advantages and disadvantages. === Using the Gram-Schmidt process === {{details|Gram-Schmidt#Numerical stability}} Consider the [[Gram–Schmidt process]] applied to the columns of the full column rank matrix <math>A=[\mathbf{a}_1, \cdots, \mathbf{a}_n]</math>, with [[inner product]] <math>\langle\mathbf{v},\mathbf{w}\rangle = \mathbf{v}^\top \mathbf{w}</math> (or <math>\langle\mathbf{v},\mathbf{w}\rangle = \mathbf{v}^* \mathbf{w}</math> for the complex case). Define the projection: :<math>\mathrm{proj}_{\mathbf{e}}\mathbf{a} = \frac{\left\langle\mathbf{e},\mathbf{a}\right\rangle}{\left\langle\mathbf{e},\mathbf{e}\right\rangle}\mathbf{e} </math> then: :<math> \begin{align} \mathbf{u}_1 &= \mathbf{a}_1, & \mathbf{e}_1 &= {\mathbf{u}_1 \over \|\mathbf{u}_1\|} \\ \mathbf{u}_2 &= \mathbf{a}_2-\mathrm{proj}_{\mathbf{e}_1}\,\mathbf{a}_2, & \mathbf{e}_2 &= {\mathbf{u}_2 \over \|\mathbf{u}_2\|} \\ \mathbf{u}_3 &= \mathbf{a}_3-\mathrm{proj}_{\mathbf{e}_1}\,\mathbf{a}_3-\mathrm{proj}_{\mathbf{e}_2}\,\mathbf{a}_3, & \mathbf{e}_3 &= {\mathbf{u}_3 \over \|\mathbf{u}_3\|} \\ & \vdots &&\vdots \\ \mathbf{u}_k &= \mathbf{a}_k-\sum_{j=1}^{k-1}\mathrm{proj}_{\mathbf{e}_j}\,\mathbf{a}_k, &\mathbf{e}_k &= {\mathbf{u}_k\over\|\mathbf{u}_k\|} \end{align} </math> We then rearrange the equations above so that the <math>\mathbf{a}_i</math>s are on the left, using the fact that the <math>\mathbf{e}_i</math> are unit vectors: :<math> \begin{align} \mathbf{a}_1 &= \langle\mathbf{e}_1,\mathbf{a}_1 \rangle \mathbf{e}_1 \\ \mathbf{a}_2 &= \langle\mathbf{e}_1,\mathbf{a}_2 \rangle \mathbf{e}_1 + \langle\mathbf{e}_2,\mathbf{a}_2 \rangle \mathbf{e}_2 \\ \mathbf{a}_3 &= \langle\mathbf{e}_1,\mathbf{a}_3 \rangle \mathbf{e}_1 + \langle\mathbf{e}_2,\mathbf{a}_3 \rangle \mathbf{e}_2 + \langle\mathbf{e}_3,\mathbf{a}_3 \rangle \mathbf{e}_3 \\ &\vdots \\ \mathbf{a}_k &= \sum_{j=1}^{k} \langle \mathbf{e}_j, \mathbf{a}_k \rangle \mathbf{e}_j \end{align} </math> where <math>\langle\mathbf{e}_i,\mathbf{a}_i \rangle = \|\mathbf{u}_i\|</math>. This can be written in matrix form: :<math> A = Q R </math> where: :<math>Q = \left[ \mathbf{e}_1, \cdots, \mathbf{e}_n\right] \qquad \text{and} \qquad R = \begin{pmatrix} \langle\mathbf{e}_1,\mathbf{a}_1\rangle & \langle\mathbf{e}_1,\mathbf{a}_2\rangle & \langle\mathbf{e}_1,\mathbf{a}_3\rangle & \ldots \\ 0 & \langle\mathbf{e}_2,\mathbf{a}_2\rangle & \langle\mathbf{e}_2,\mathbf{a}_3\rangle & \ldots \\ 0 & 0 & \langle\mathbf{e}_3,\mathbf{a}_3\rangle & \ldots \\ \vdots & \vdots & \vdots & \ddots \end{pmatrix}.</math> ==== Example ==== Consider the decomposition of : <math>A = \begin{pmatrix} 12 & -51 & 4 \\ 6 & 167 & -68 \\ -4 & 24 & -41 \end{pmatrix} .</math> Recall that an orthogonal matrix <math>Q</math> has the property : <math> \begin{matrix} Q^{T}\,Q = I. \end{matrix} </math> Then, we can calculate <math>Q</math> by means of Gram-Schmidt as follows: : <math> U = \begin{pmatrix} \mathbf u_1 & \mathbf u_2 & \mathbf u_3 \end{pmatrix} = \begin{pmatrix} 12 & -69 & -58/5 \\ 6 & 158 & 6/5 \\ -4 & 30 & -33 \end{pmatrix}; </math> : <math> Q = \begin{pmatrix} \frac{\mathbf u_1}{\|\mathbf u_1\|} & \frac{\mathbf u_2}{\|\mathbf u_2\|} & \frac{\mathbf u_3}{\|\mathbf u_3\|} \end{pmatrix} = \begin{pmatrix} 6/7 & -69/175 