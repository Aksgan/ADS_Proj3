any predictor as a linear combination of the others. See [[Multicollinearity]]. * The errors are [[uncorrelated]], that is, the [[variance-covariance matrix]] of the errors is [[Diagonal matrix|diagonal]] and each non-zero element is the variance of the error. *The variance of the error is constant across observations ([[homoscedasticity]]). (Note: If not, [[weighted least squares]] or other methods might instead be used). These are sufficient conditions for the least-squares estimator to possess desirable properties, in particular, these assumptions imply that the parameter estimates will be [[bias of an estimator|unbiased]], [[consistent estimator|consistent]], and [[efficient (statistics)|efficient]] in the class of linear unbiased estimators. It is important to note that actual data rarely satisfies the assumptions. That is, the method is used even though the assumptions are not true. Variation from the assumptions can sometimes be used as a measure of how far the model is from being useful. Many of these assumptions may be relaxed in more advanced treatments. Reports of statistical analyses usually include analyses of tests on the sample data and methodology for the fit and usefulness of the model. Assumptions include the geometrical support of the variables{{Clarify|date=February 2010}} (Cressie, 1996). Independent and dependent variables often refer to values measured at point locations. There may be spatial trends and spatial autocorrelation in the variables that violates statistical assumptions of regression. Geographic weighted regression is one technique to deal with such data (Fotheringham et al., 2002). Also, variables may include values aggregated by areas. With aggregated data the [[Modifiable Areal Unit Problem]] can cause extreme variation in regression parameters (Fotheringham and Wong, 1991). When analyzing data aggregated by political boundaries, postal codes or census areas results may be very different with a different choice of units. ==Linear regression== {{Main|Linear regression}} In linear regression, the model specification is that the dependent variable, <math> y_i </math> is a [[linear combination]] of the ''parameters'' (but need not be linear in the ''independent variables''). For example, in [[simple linear regression]] for modeling <math> n </math> data points there is one independent variable: <math> x_i </math>, and two parameters, <math>\beta_0</math> and <math>\beta_1</math>: :straight line: <math>y_i=\beta_0 +\beta_1 x_i +\varepsilon_i,\quad i=1,\dots,n.\!</math> In multiple linear regression, there are several independent variables or functions of independent variables. For example, adding a term in ''x<sub>i</sub>''<sup>2</sup> to the preceding regression gives: :parabola: <math>y_i=\beta_0 +\beta_1 x_i +\beta_2 x_i^2+\varepsilon_i,\ i=1,\dots,n.\!</math> This is still linear regression; although the expression on the right hand side is quadratic in the independent variable <math>x_i</math>, it is linear in the parameters <math>\beta_0</math>, <math>\beta_1</math> and <math>\beta_2.</math> In both cases, <math>\varepsilon_i</math> is an error term and the subscript <math>i</math> indexes a particular observation. Given a random sample from the population, we estimate the population parameters and obtain the sample linear regression model: : <math> \widehat{y_i} = \widehat{\beta}_0 + \widehat{\beta}_1 x_i. </math> The [[errors and residuals in statistics|residual]], <math> e_i = y_i - \widehat{y}_i </math>, is the difference between the value of the dependent variable predicted by the model, <math> \widehat{y_i}</math> and the true value of the dependent variable <math>y_i</math>. One method of estimation is [[ordinary least squares]]. This method obtains parameter estimates that minimize the sum of squared [[errors and residuals in statistics|residuals]], SSE,<ref>M. H. Kutner, C. J. Nachtsheim, and J. Neter (2004), "Applied Linear Regression Models", 4th ed., McGraw-Hill/Irwin, Boston (p. 25)</ref><ref>N. Ravishankar and D. K. Dey (2002), "A First Course in Linear Model Theory", Chapman and Hall/CRC, Boca Raton (p. 101)</ref> also sometimes denoted [[Residual sum of squares|RSS]]: :<math>SSE=\sum_{i=1}^N e_i^2. \, </math> Minimization of this function results in a set of [[linear least squares|normal equations]], a set of simultaneous linear equations in the parameters, which are solved to yield the parameter estimators, <math>\widehat{\beta}_0, \widehat{\beta}_1</math>. [[Image:Linear regression.png|thumb|right|300px|Illustration of linear regression on a data set.]] In the case of simple regression, the formulas for the least squares estimates are :<math>\widehat{\beta_1}=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}\text{ and }\hat{\beta_0}=\bar{y}-\widehat{\beta_1}\bar{x}</math> where <math>\bar{x}</math> is the [[Arithmetic mean|mean]] (average) of the <math>x</math> values and <math>\bar{y}</math> is the mean of the <math>y</math> values. See [[simple linear regression]] for a derivation of these formulas and a numerical example. Under the assumption that the population error term has a constant variance, the estimate of that variance is given by: : <math> \hat{\sigma}^2_\varepsilon = \frac{SSE}{N-2}.\,</math> This is called the [[mean square error]] (MSE) of the regression. The [[standard error (statistics)|standard error]]s of the parameter estimates are given by :<math>\hat\sigma_{\beta_0}=\hat\sigma_{\varepsilon} \sqrt{\frac{1}{N} + \frac{\bar{x}^2}{\sum(x_i-\bar x)^2}}</math> :<math>\hat\sigma_{\beta_1}=\hat\sigma_{\varepsilon} \sqrt{\frac{1}{\sum(x_i-\bar x)^2}}.</math> Under the further assumption that the population error term is normally distributed, the researcher can use these estimated standard errors to create [[confidence interval]]s and conduct [[hypothesis test]]s about the [[population parameter]]s. ===General linear model=== In the more general multiple regression model, there are ''p'' independent variables: : <math> y_i = \beta_0 + \beta_1 x_{1i} + \cdots + \beta_p x_{pi} + \varepsilon_i, \, </math> The least square parameter estimates are obtained by ''p'' normal equations. The residual can be written as :<math>e_i=y_i - \hat\beta_0 - \hat\beta_1 x_1 - \cdots - \hat\beta_p x_p.</math> The '''normal equations''' are :<math>\sum_{i=1}^n \sum_{k=1}^p X_{ij}X_{ik}\hat \beta_k=\sum_{i=1}^n X_{ij}y_i,\ j=1,\dots,p.\,</math> Note that for the normal equations depicted above, <math> y_i = \beta_1 x_{1i} + \cdots + \beta_p x_{pi} + \varepsilon_i \, </math> That is, there is no <math> \beta_0 </math>. Thus in what follows, <math>\boldsymbol \beta = (\beta_1, \beta_2, \dots, \beta_p).</math> In matrix notation, the normal equations for <math> k </math> responses (usually <math> k=1 </math>) are written as :<math>\mathbf{_p(X_n^\top X )_p\hat \boldsymbol \beta_k= _pX_n^\top Y_k}.\,</math> with [[generalized inverse]] (<math> - </math>) solution, subscripts showing matrix dimensions: :<math>\mathbf{_p\hat \boldsymbol \beta_k= {}_p(X_n^\top X )_p^-X_n^\top Y_k}.\,</math> For more detailed derivation, see [[linear least squares (mathematics)|linear least squares]], and for a numerical example, see [[linear regression#Example|linear regression (example)]]. ===Regression diagnostics=== Once a regression model has been constructed, it may be important to confirm the [[goodness of fit]] of the model and the [[statistical significance]] of the estimated parameters. Commonly used checks of goodness of fit include the [[R-squared]], analyses of the pattern of [[errors and residuals in statistics|residuals]] and hypothesis testing. Statistical significance can be checked by an [[F-test]] of the overall fit, followed by [[t-test]]s of individual parameters. Interpretations of these 