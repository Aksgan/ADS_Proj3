a form of [[radix sort]]. A trie forms the fundamental data structure of [[Burstsort]]; currently (2007) the fastest known, memory/cache-based, string sorting algorithm.<ref name="cachestringsort">{{Cite web|url=http://www.cs.mu.oz.au/~rsinha/papers/SinhaRingZobel-2006.pdf|format=PDF|title=Cache-Efficient String Sorting Using Copying|accessdate=2008-11-15}}</ref> A [[parallel algorithm]] for sorting N keys based on tries is [[Big-O notation|O]](1) if there are N processors and the length of the keys have a constant upper bound. There is the potential that the keys might collide by having common prefixes or by being identical to one another, reducing or eliminating the speed advantage of having multiple processors operating in parallel. === Full text search === A special kind of trie, called a [[suffix tree]], can be used to index all suffixes in a text in order to carry out fast full text searches. === Bitwise tries === Bitwise tries are much the same as a normal character based trie except that individual bits are used to traverse what effectively becomes a form of binary tree. Generally, implementations use a special CPU instruction to very quickly find the first set bit in a fixed length key (e.g. GCC's __builtin_clz() intrinsic). This value is then used to index a 32 or 64 entry table which points to the first item in the bitwise trie with that number of leading zero bits. The search then proceeds by testing each subsequent bit in the key and choosing child[0] or child[1] appropriately until the item is found. Although this process might sound slow, it is very cache-local and highly parallelizable due to the lack of register dependencies and therefore in fact performs excellently on modern out-of-order execution CPUs. A [[red-black tree]] for example performs much better on paper, but is highly cache-unfriendly and causes multiple pipeline and TLB stalls on modern CPUs which makes that algorithm bound by memory latency rather than CPU speed. In comparison, a bitwise trie rarely accesses memory and when it does it does so only to read, thus avoiding SMP cache coherency overhead, and hence is becoming increasingly the algorithm of choice for code which does a lot of insertions and deletions such as memory allocators (e.g. recent versions of the famous [[Malloc#dlmalloc and its derivatives|Doug Lea's allocator (dlmalloc) and its descendents]]). A reference implementation of bitwise tries in C and C++ useful for further study can be found at http://www.nedprod.com/programs/portable/nedtries/. === Compressing tries === When the trie is mostly static, i.e. all insertions or deletions of keys from a prefilled trie are disabled and only lookups are needed, and when the trie nodes are not keyed by node specific data (or if the node's data is common) it is possible to compress the trie representation by merging the common branches.<ref>{{Cite web |url = http://www.pg.gda.pl/~jandac/daciuk98.ps.gz |title = Incremental Construction of Minimal Acyclic Finite-State Automata |year = 2000 |author = Jan Daciuk, Stoyan Mihov, Bruce W. Watson, Richard E. Watson |work = Computational Linguistics |publisher = Association for Computational Linguistics |pages = Vol. 26, No. 1, Pages 3â€“16 |doi = 10.1162/089120100561601 |archiveurl = http://www.mitpressjournals.org/doi/abs/10.1162/089120100561601 |archivedate= 2006-03-13 |quote = This paper presents a method for direct building of minimal acyclic finite states automaton which recognizes a given finite list of words in lexicographical order. Our approach is to construct a minimal automaton in a single phase by adding new strings one by one and minimizing the resulting automaton on-the-fly |accessdate = 2009-05-28 }}</ref> This application is typically used for compressing lookup tables when the total set of stored keys is very sparse within their representation space. For example it may be used to represent sparse [[bitset]]s (i.e. subsets of a much fixed enumerable larger set) using a trie keyed by the bit element position within the full set, with the key created from the string of bits needed to encode the integral position of each element. The trie will then have a very degenerate form with many missing branches, and compression becomes possible by storing the leaf nodes (set segments with fixed length) and combining them after detecting the repetition of common patterns or by filling the unused gaps. Such compression is also typically used, in the implementation of the various fast lookup tables needed to retrieve [[Unicode]] character properties (for example to represent case mapping tables, or lookup tables containing the combination of base and combining characters needed to support Unicode normalization). For such application, the representation is similar to transforming a very large unidimensional sparse table into a multidimensional matrix, and then using the coordinates in the hyper-matrix as the string key of an uncompressed trie. The compression will then consist of detecting and merging the common columns within the hyper-matrix to compress the last dimension in the key; each dimension of the hypermatrix stores the start position within a storage vector of the next dimension for each coordinate value, and the resulting vector is itself compressible when it is also sparse, so each dimension (associated to a layer level in the trie) is compressed separately. Some implementations do support such data compression within dynamic sparse tries and allow insertions and deletions in compressed tries, but generally this has a significant cost when compressed segments need to be split or merged, and some tradeoff has to be made between the smallest size of the compressed trie and the speed of updates, by limiting the range of global lookups for comparing the common branches in the sparse trie. The result of such compression may look similar to trying to transform the trie into a [[directed acyclic graph]] (DAG), because the reverse transform from a DAG to a trie is obvious and always possible, however it is constrained by the form of the key chosen to index the nodes. Another compression approach is to "unravel" the data structure into a single byte array.<ref>{{Cite web |url=http://www.aclweb.org/anthology/W/W09/W09-1505.pdf |format=PDF |title = Tightly packed tries: how to fit large models into memory, and make them load fast, too |year = 2009 |author = Ulrich Germann, Eric Joanis, Samuel Larkin |work = ACL Workshops: Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for 