space is a "one-of-K" representation, i.e. a <math>K</math>-dimensional vector in which one of the elements is 1 (specifying the identity of the observation) and all other elements are 0. *<math>\mathcal{N}()</math> is the [[Gaussian distribution]], in this case specifically the [[multivariate Gaussian distribution]]. The interpretation of the above variables is as follows: *<math>\mathbf{X} = \{\mathbf{x}_1, \dots, \mathbf{x}_N\}</math> is the set of <math>N</math> data points, each of which is a <math>K</math>-dimensional vector distributed according to a [[multivariate Gaussian distribution]]. *<math>\mathbf{Z} = \{\mathbf{z}_1, \dots, \mathbf{z}_N\}</math> is a set of latent variables, one per data point, specifying which mixture component the corresponding data point belongs to, using a "one-of-K" vector representation with components <math>z_{nk}</math> for <math>k = 1 \dots K</math>, as described above. *<math>\mathbf{\pi}</math> is the mixing proportions for the <math>K</math> mixture components. *<math>\mathbf{\mu}_{i=1 \dots K}</math> and <math>\mathbf{\Lambda}_{i=1 \dots K}</math> specify the parameters ([[mean]] and [[precision (statistics)|precision]]) associated with each mixture component. The joint probability of all variables can be rewritten as :<math>p(\mathbf{X},\mathbf{Z},\mathbf{\pi},\mathbf{\mu},\mathbf{\Lambda}) = p(\mathbf{X}|\mathbf{Z},\mathbf{\mu},\mathbf{\Lambda}) p(\mathbf{Z}|\mathbf{\pi}) p(\mathbf{\pi}) p(\mathbf{\mu}|\mathbf{\Lambda}) p(\mathbf{\Lambda})</math> where the individual factors are :<math> \begin{array}{lcl} p(\mathbf{X}|\mathbf{Z},\mathbf{\mu},\mathbf{\Lambda}) & = & \prod_{n=1}^N \prod_{k=1}^K \mathcal{N}(\mathbf{x}_n|\mathbf{\mu}_k,\mathbf{\Lambda}_k^{-1})^{z_{nk}} \\ p(\mathbf{Z}|\mathbf{\pi}) & = & \prod_{n=1}^N \prod_{k=1}^K \pi_k^{z_{nk}} \\ p(\mathbf{\pi}) & = & \frac{\Gamma(K\alpha_0)}{\Gamma(\alpha_0)^K} \prod_{k=1}^K \pi_k^{\alpha_0-1} \\ p(\mathbf{\mu}|\mathbf{\Lambda}) & = & \mathcal{N}(\mathbf{\mu}_k|\mathbf{m}_0,(\beta_0 \mathbf{\Lambda}_k)^{-1}) \\ p(\mathbf{\Lambda}) & = & \mathcal{W}(\mathbf{\Lambda}_k|\mathbf{W}_0, \nu_0) \end{array} </math> where :<math> \begin{array}{lcl} \mathcal{N}(\mathbf{x}|\mathbf{\mu},\mathbf{\Sigma}) & = & \frac{1}{(2\pi)^{D/2}} \frac{1}{|\mathbf{\Sigma}|^{1/2}} \exp \{-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T \mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu}) \} \\ \mathcal{W}(\mathbf{\Lambda}|\mathbf{W},\nu) & = & B(\mathbf{W},\nu) |\mathbf{\Lambda}|^{(\nu-D-1)/2} \exp \left(-\frac{1}{2} \text{Tr}(\mathbf{W}^{-1}\mathbf{\Lambda}) \right) \\ B(\mathbf{W},\nu) & = & |\mathbf{W}|^{-\nu/2} (2^{\nu D/2} \pi^{D(D-1)/4} \prod_{i=1}^{D} \Gamma(\frac{\nu + 1 - i}{2}))^{-1} \\ D & = & \text{dimensionality of each data point} \end{array} </math> Assume that <math>q(\mathbf{Z},\mathbf{\pi},\mathbf{\mu},\mathbf{\Lambda}) = q(\mathbf{Z})q(\mathbf{\pi},\mathbf{\mu},\mathbf{\Lambda})</math>. Then :<math> \begin{array}{lcl} \ln q^*(\mathbf{Z}) &=& \operatorname{E}_{\mathbf{\pi},\mathbf{\mu},\mathbf{\Lambda}}[\ln p(\mathbf{X},\mathbf{Z},\mathbf{\pi},\mathbf{\mu},\mathbf{\Lambda})] + \text{const.} \\ &=& \operatorname{E}_{\mathbf{\pi}}[\ln p(\mathbf{Z}|\mathbf{\pi})] + \operatorname{E}_{\mathbf{\mu},\mathbf{\Lambda}}[\ln p(\mathbf{X}|\mathbf{Z},\mathbf{\mu},\mathbf{\Lambda})] + \text{const.} \\ &=& \sum_{n=1}^N \sum_{k=1}^K z_{nk} \ln \rho_{nk} + \text{const.} \end{array} </math> where we have defined :<math>\ln \rho_{nk} = \operatorname{E}[\ln \pi_k] + \frac{1}{2} \operatorname{E}[\ln |\mathbf{\Lambda}_k|] - \frac{D}{2} \ln(2\pi) - \frac{1}{2} \operatorname{E}_{\mathbf{\mu}_k,\mathbf{\Lambda}_k} [(\mathbf{x}_n - \mathbf{\mu}_k)^T \mathbf{\Lambda}_k (\mathbf{x}_n - \mathbf{\mu}_k)]</math> Exponentiating both sides of the formula for <math>\ln q^*(\mathbf{Z})</math> yields :<math>q^*(\mathbf{Z}) \propto \prod_{n=1}^N \prod_{k=1}^K \rho_{nk}^{z_{nk}}</math> Requiring that this be normalized ends up requiring that the <math>\rho_{nk}</math> sum to 1 over all values of <math>k</math>, yielding :<math>q^*(\mathbf{Z}) = \prod_{n=1}^N \prod_{k=1}^K r_{nk}^{z_{nk}}</math> where :<math>r_{nk} = \frac{\rho_{nk}}{\sum_{j=1}^K \rho_{nj}}</math> In other words, <math>q^*(\mathbf{Z})</math> is a product of single-observation [[multinomial distribution]]s, and factors over each individual <math>\mathbf{z}_n</math>, which is distributed as a single-observation multinomial distribution with parameters <math>r_{nk}</math> for <math>k = 1 \dots K</math>. Furthermore, we note that :<math>\operatorname{E}[z_{nk}] = r_{nk}</math> which is a standard result for categorical distributions. Now, considering the factor <math>q(\mathbf{\pi},\mathbf{\mu},\mathbf{\Lambda})</math>, note that it automatically factors into <math>q(\mathbf{\pi}) \prod_{k=1}^K q(\mathbf{\mu}_k,\mathbf{\Lambda}_k)</math> due to the structure of the graphical model defining our Gaussian mixture model, which is specified above. Then, :<math> \begin{array}{lcl} \ln q^*(\mathbf{\pi}) &=& \ln p(\mathbf{\pi}) + \operatorname{E}_{\mathbf{Z}}[\ln p(\mathbf{Z}|\mathbf{\pi})] + \text{const.} \\ &=& (\alpha_0 - 1) \sum_{k=1}^K \ln \pi_k + \sum_{n=1}^N \sum_{k=1}^K r_{nk} \ln \pi_k + \text{const.} \end{array} </math> Taking the exponential of both sides, we recognize <math>q^*(\mathbf{\pi})</math> as a [[Dirichlet distribution]] :<math>q^*(\mathbf{\pi}) \sim \text{Dir}(\mathbf{\alpha})</math> where :<math>\alpha_k = \alpha_0 + N_k</math> where :<math>N_k = \sum_{n=1}^N r_{nk}</math> Finally :<math>\ln q^*(\mathbf{\mu}_k,\mathbf{\Lambda}_k) = \ln p(\mathbf{\mu}_k,\mathbf{\Lambda}_k) + \sum_{n=1}^N \operatorname{E}[z_{nk}] \ln \mathcal{N}(\mathbf{x}_n|\mathbf{\mu}_k,\mathbf{\Lambda}_k^{-1}) + \text{const.}</math> Grouping and reading off terms involving <math>\mathbf{\mu}_k</math> and <math>\mathbf{\Lambda}_k</math>, the result is a [[Gaussian-Wishart distribution]] given by :<math>q^*(\mathbf{\mu}_k,\mathbf{\Lambda}_k) = \mathcal{N}(\mathbf{\mu}_k|\mathbf{m}_k,(\beta_k \mathbf{\Lambda}_k)^{-1}) \mathcal{W}(\mathbf{\Lambda}_k|\mathbf{W}_k,\nu_k)</math> given the definitions :<math> \begin{array}{lcl} \beta_k &=& \beta_0 + N_k \\ \mathbf{m}_k &=& \frac{1}{\beta_k} (\beta_0 \mathbf{m}_0 + N_k {\bar \mathbf{x}}_k) \\ \mathbf{W}_k^{-1} &=& \mathbf{W}_0^{-1} + N_k \mathbf{S}_k + \frac{\beta_0 N_k}{\beta_0 + N_k} ({\bar \mathbf{x}}_k - \mathbf{m}_0)({\bar \mathbf{x}}_k - \mathbf{m}_0)^T \\ \nu_k &=& \nu_0 + N_k \\ N_k &=& \sum_{n=1}^N r_{nk} \\ {\bar \mathbf{x}}_k &=& \frac{1}{N_k} \sum_{n=1}^N r_{nk} \mathbf{x}_n \\ \mathbf{S}_k &=& \sum_{n=1}^N (\mathbf{x}_n - {\bar \mathbf{x}}_k) (\mathbf{x}_n - {\bar \mathbf{x}}_k)^T \end{array} </math> Finally, notice that these functions require the values of <math>r_{nk}</math>, which make use of <math>\rho_{nk}</math>, which is defined in turn based on <math>\operatorname{E}[\ln \pi_k]</math>, <math>\operatorname{E}[\ln |\mathbf{\Lambda}_k|]</math>, and <math>\operatorname{E}_{\mathbf{\mu}_k,\mathbf{\Lambda}_k} [(\mathbf{x}_n - \mathbf{\mu}_k)^T \mathbf{\Lambda}_k (\mathbf{x}_n - \mathbf{\mu}_k)]</math>. Now that we have determined the distributions over which these expectations are taken, we can derive formulas for them: :<math> \begin{array}{lclcl} \operatorname{E}_{\mathbf{\mu}_k,\mathbf{\Lambda}_k} [(\mathbf{x}_n - \mathbf{\mu}_k)^T \mathbf{\Lambda}_k (\mathbf{x}_n - \mathbf{\mu}_k)] &&&=& D\beta_k^{-1} + \nu_k (\mathbf{x}_n - \mathbf{m}_k)^T \mathbf{W}_k (\mathbf{x}_n - \mathbf{m}_k) \\ \ln {\tilde{\Lambda}}_k &\equiv& \operatorname{E}[\ln |\mathbf{\Lambda}_k|] &=& \sum_{i=1}^D \psi (\frac{\nu_k + 1 - i}{2}) + D \ln 2 + \ln |\mathbf{\Lambda}_k| \\ \ln {\tilde{\pi}}_k &\equiv& \operatorname{E}[\ln |\pi_k|] &=& \psi(\alpha_k) - \psi(\sum{i=1}^K \alpha_i) \end{array} </math> These results lead to :<math>r_{nk} \propto {\tilde{\pi}}_k {\tilde{\Lambda}}_k^{1/2} \exp \{ - \frac{D}{2 \beta_k} - \frac{\nu_k}{2} (\mathbf{x}_n - \mathbf{m}_k)^T \mathbf{W}_k (\mathbf{x}_n - \mathbf{m}_k) \}</math> These can be converted from proportional to absolute values by normalizing over <math>k</math> so that the corresponding values sum to 1. Note that: #The update equations for the parameters <math>\beta_k</math>, <math>\mathbf{m}_k</math>, <math>\mathbf{W}_k</math> and <math>\nu_k</math> of the variables <math>\mathbf{\mu}_k</math> and <math>\mathbf{\Lambda}_k</math> depend on the statistics <math>N_k</math>, <math>{\bar \mathbf{x}}_k</math>, and <math>\mathbf{S}_k</math>, and these statistics in turn depend on <math>r_{nk}</math>. #The update equations for the parameters <math>\alpha_{1 \dots K}</math> of the variable <math>\mathbf{\pi}</math> depend on the statistic <math>N_k</math>, which depends in turn on <math>r_{nk}</math>. #The update equation for <math>r_{nk}</math> has a direct circular dependence on <math>\beta_k</math>, <math>\mathbf{m}_k</math>, <math>\mathbf{W}_k</math> and <math>\nu_k</math> as well as an indirect circular dependence on <math>\mathbf{W}_k</math>, <math>\nu_k</math> and <math>\alpha_{1 \dots K}</math> through <math>{\tilde{\pi}}_k</math> and <math>{\tilde{\Lambda}}_k</math>. This suggests an iterative procedure that alternates between two steps: #An E-step that computes the value of <math>r_{nk}</math> using the current values of all the other parameters. #An M-step that uses the new value of <math>r_{nk}</math> to compute new values of all the other parameters. Note that these steps correspond closely with the standard EM algorithm to derive a [[maximum likelihood]] or [[maximum a posteriori]] (MAP) solution for the parameters of a [[Gaussian mixture model]]. The responsibilities <math>r_{nk}</math> in the E step correspond closely to the [[posterior probability|posterior probabilities]] of the latent variables given the data, i.e. <math>p(\mathbf{Z}|\mathbf{X})</math>; the computation of the statistics <math>N_k</math>, <math>{\bar \mathbf{x}}_k</math>, and <math>\mathbf{S}_k</math> corresponds closely to the computation of corresponding "soft-count" statistics over the data; and the use of those statistics to compute new values of the parameters corresponds closely to the use of soft 