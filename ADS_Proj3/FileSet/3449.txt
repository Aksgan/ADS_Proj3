There are various types of switchers for audio and video, from simple [[selector switch]]es to sophisticated [[production switcher]]s. However, emulating or exceeding the capabilities of audio and/or video patch bays requires specialized devices like [[routing switcher]]s and [[matrix router]]s (aka "[[crosspoint switcher]]s"). Like patch panels, switching equipment for nearly any type of signal is available, including analog and digital video and audio, as well as [[radio frequency|RF]] (cable TV), [[MIDI]], telephone, networking, electrical, and just about anything else. Switching equipment may be electronic, mechanical, or electro-mechanical. Some switcher hardware can be controlled via computer and/or other external devices. Some have automated and/or pre-programmed operational capabilities. There are also software switcher applications used to route signals and control data within a "pure digital" computer environment. [[Distribution frame]]s are cheaper, but less convenient. ==See also== *[[Cable management]] *[[Meet-me-room]] *[[Wiring closet]] ==References== *{{FS1037C MS188}} {{DEFAULTSORT:Patch Panel}} [[Category:Telephony equipment]] [[Category:Audio engineering]] [[Category:Broadcast engineering]] [[Category:Electrical signal connectors]] [[Category:Networking hardware]] [[cs:Patch panel]] [[de:Steckfeld (Verkabelung)]] [[de:Rangierfeld]] [[it:Patch panel]] [[nl:Patchpanel]] [[ja:パッチパネル]] [[pl:Panel krosowniczy]] [[ro:Patch panel]] [[ru:Коммутационная панель]] [[uk:Патч-панель]]</text> </page> <page> <id>28614</id> <title>Path tracing</title> <text>{{about||tracing network paths|traceroute|other uses|tracing (disambiguation)}} [[Image:Pathtrace3.png|right|thumb|350px|A simple scene showing the soft phenomena simulated with path tracing.]] '''Path tracing''' is a [[computer graphics]] [[Rendering (computer graphics)|rendering]] technique that attempts to simulate the physical behaviour of [[light]] as closely as possible. It is a generalisation of conventional [[Ray tracing (graphics)|ray tracing]], tracing [[Ray (optics)|rays]] from the virtual camera through several bounces on or through objects. The image quality provided by path tracing is usually superior to that of images produced using conventional rendering methods at the cost of much greater computation requirements. Path tracing naturally [[simulate]]s many effects that have to be specifically added to other methods ([[Ray tracing (graphics)|ray tracing]] or [[scanline rendering]]), such as soft [[shadows]], [[depth of field]], [[motion blur]], [[Caustic (optics)|caustics]], [[ambient occlusion]], and indirect lighting. Implementation of a renderer including these effects is correspondingly simpler. Due to its accuracy and [[unbiased rendering|unbiased]] nature, path tracing is used to generate reference images when testing the quality of other rendering [[algorithm]]s. In order to get high quality images from path tracing, a large number of rays must be traced to avoid visible artifacts in the form of [[Image noise|noise]]. == History == {{Further|[[Rendering (computer graphics)#Chronology of important published ideas|Rendering, Chronology of important published ideas]]}} The [[rendering equation]] and its use in computer graphics was presented by James Kajiya in [[1986]].{{Ref|kajiya1986rendering}} This presentation contained what was probably the first description of the path tracing algorithm. Later that year, Lafortune suggested many refinements, including bidirectional path tracing.{{Ref|lafortune1986mathematical}} [[Metropolis light transport]], a method of perturbing previously found paths in order to increase performance for difficult scenes, was introduced in 1997 by Eric Veach and [[Leonidas J. Guibas]]. More recently, computers and [[GPU]]s have become powerful enough to render images more quickly, causing more widespread interest in path tracing algorithms. Tim Purcell first presented a [[global illumination]] algorithm running on a GPU in [[2002]].{{Ref|purcell2002ray}} In [[2009]], Vladimir Koylazov demonstrated the first commercial implementation of a path tracer running on a GPU, and other implementations have followed.{{Ref|pathGPUimplementations}} This was aided by the maturing of [[GPGPU]] programming toolkits such as [[CUDA]] and [[OpenCL]]. == Description == In the real world, many small amounts of light are emitted from light sources, and travel in straight lines (rays) from object to object, changing [[colour]] and [[Intensity (physics)|intensity]], until they are absorbed (possibly by an eye or camera). This process is simulated by path tracing, except that the paths are traced backwards, from the camera to the light. The inefficiency arises in the random nature of the bounces from many surfaces, as it is usually quite unlikely that a path will intersect a light. As a result, most traced paths do not contribute to the final image. This behaviour is described mathematically by the [[rendering equation]], which is the equation that path tracing algorithms try to solve. Path tracing is not simply ray tracing with infinite [[recursion]] depth. In conventional ray tracing, lights are sampled directly when a [[diffuse]] surface is hit by a ray. In path tracing, a new ray is ''randomly generated within the hemisphere of the object'' and then traced until it hits a light &mdash; possibly never. This type of path can hit many diffuse surfaces before interacting with a light. A simple path tracing pseudocode might look something like this: <code> Color TracePath(Ray r,depth) { if(depth == MaxDepth) return Black; // bounced enough times r.FindNearestObject(); if(r.hitSomething == false) return Black; // nothing was hit Material m = r.thingHit->material; Color emittance = m.emittance; // pick a random direction from here and keep going Ray newRay; newRay.origin = r.pointWhereObjWasHit; newRay.direction = RandomUnitVectorInHemisphereOf(r.normalWhereObjWasHit); float cos_omega = DotProduct(newRay.direction, r.normalWhereObjWasHit); Color BDRF = m.reflectance*cos_omega; Color reflected = TracePath(newRay,depth+1); return emittance + ( BDRF * cos_omega * reflected ); } </code> In the above example if every surface of a closed space emitted and reflected (0.5,0.5,0.5) then every pixel in the image would be white. == Bidirectional path tracing == In order to accelerate the convergence of images, bidirectional algorithms trace paths in both directions. In the forward direction, rays are traced from light sources until they are too faint to be seen or strike the camera. In the reverse direction (the usual one), rays are traced from the camera until they strike a light or too many bounces ("depth") have occurred. This approach normally results in an image that converges much more quickly than using only one direction. Veach and [[Leonidas J. Guibas|Guibas]] give a more accurate description{{Ref|veach1997metropolis}}: <blockquote>These methods generate one subpath starting at a light source and another starting at the lens, then they consider all the paths obtained by joining every prefix of one subpath to every suffix of the other. This leads to a family of different importance sampling techniques for paths, which are then combined to minimize variance.</blockquote> == Performance == A path tracer continuously samples [[pixel]]s of an [[image]]. The image starts to become recognisable after only a few samples per pixel, perhaps 100. However, for the 