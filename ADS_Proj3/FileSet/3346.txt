of the transitions of its components. For a simple example, we consider part of the semantics of a simple programming language; proper illustrations are given in [[#plotkin81|Plotkin81]] and [[#hennessybook|Hennessy90]], and other textbooks. Let <math>C_1, C_2</math> range over programs of the language, and let <math>s</math> range over states (e.g. functions from memory locations to values). If we have expressions (ranged over by <math>E</math>), values (<math>V</math>) and locations (<math>L</math>), then a memory update command would have semantics: <math> \frac{\langle E,s\rangle \Rightarrow V}{\langle L:=E\,,\,s\rangle\longrightarrow (s\uplus (L\mapsto V))} </math> <!-- NOTE: This is only a fragment of a full semantics. I've tried to include enough to illustrate the points but not so much that it takes a disproportionate amount of space. --> Informally, the rule says that "'''if''' the expression <math>E</math> in state <math>s</math> reduces to value <math>V</math>, '''then''' the program <math>L:=E</math> will update the state <math>s</math> with the assignment <math>L=V</math>". The semantics of sequencing can be given by the following three rules: <math> \frac{\langle C_1,s\rangle \longrightarrow s'} {\langle C_1;C_2 \,,s\rangle\longrightarrow \langle C_2, s'\rangle} \quad\quad \frac{\langle C_1,s\rangle \longrightarrow \langle C_1',s'\rangle} {\langle C_1;C_2 \,,s\rangle\longrightarrow \langle C_1';C_2\,, s'\rangle} \quad\quad \frac{} {\langle \mathbf{skip} ,s\rangle\longrightarrow s} </math> Informally, the first rule says that, if program <math>C_1</math> in state <math>s</math> finishes in state <math>s'</math>, then the program <math>C_1;C_2</math> in state <math>s</math> will reduce to the program <math>C_2</math> in state <math>s'</math>. (You can think of this as formalizing "You can run <math>C_1</math>, and then run <math>C_2</math> using the resulting memory store.) The second rule says that if the program <math>C_1</math> in state <math>s</math> can reduce to the program <math>C_1'</math> with state <math>s'</math>, then the program <math>C_1;C_2</math> in state <math>s</math> will reduce to the program <math>C_1';C_2</math> in state <math>s'</math>. (You can think of this as formalizing the principle for an optimizing compiler: "You are allowed to transform <math>C_1</math> as if it were stand-alone, even if it is just the first part of a program.") The semantics is structural, because the meaning of the sequential program <math>C_1;C_2</math>, is defined by the meaning of <math>C_1</math> and the meaning of <math>C_2</math>. If we also have Boolean expressions over the state, ranged over by <math>B</math>, then we can define the semantics of the '''while''' command: <math> \frac{\langle B,s\rangle \Rightarrow \mathbf{true}}{\langle\mathbf{while}\ B\ \mathbf{ do }\ C,s\rangle\longrightarrow \langle C;\mathbf{while}\ B\ \mathbf{do}\ C,s\rangle} \quad \frac{\langle B,s\rangle \Rightarrow \mathbf{false}}{\langle\mathbf{while}\ B\ \mathbf{ do }\ C,s\rangle\longrightarrow s} </math> Such a definition allows formal analysis of the behavior of programs, permitting the study of [[Relation (mathematics)|relations]] between programs. Important relations include [[simulation preorder]]s and [[bisimulation]]. These are especially useful in the context of [[Concurrency (computer science)|concurrency theory]]. Thanks to its intuitive look and easy to follow structure, SOS has gained great popularity and has become a de facto standard in defining operational semantics. As a sign of success, the original report (so-called Aarhus report) on SOS ([[#plotkin81|Plotkin81]]) has attracted more than 1000 citations according to the CiteSeer [http://citeseer.ist.psu.edu/673965.html], making it one of the most cited technical reports in [[Computer Science]]. == See also == * [[Algebraic semantics]] * [[Axiomatic semantics]] * [[Denotational semantics]] * [[Interpreter semantics]] * [[Formal semantics of programming languages]] == References == * <cite id=plotkin81> [[Gordon Plotkin|Gordon D. Plotkin.]] [http://citeseer.ist.psu.edu/673965.html A Structural Approach to Operational Semantics]. (1981) Tech. Rep. DAIMI FN-19, Computer Science Department, Aarhus University, Aarhus, Denmark. (Reprinted with corrections in J. Log. Algebr. Program. 60-61: 17-139 (2004), [http://homepages.inf.ed.ac.uk/gdp/publications/sos_jlap.pdf preprint]). </cite> * <cite id=plotkin04> [[Gordon Plotkin|Gordon D. Plotkin.]] The Origins of Structural Operational Semantics. J. Log. Algebr. Program. 60-61:3-15, 2004. ([http://homepages.inf.ed.ac.uk/gdp/publications/Origins_SOS.pdf preprint]). </cite> * <cite id=scott70> [[Dana Scott|Dana S. Scott.]] Outline of a Mathematical Theory of Computation, Programming Research Group, Technical Monograph PRG–2, Oxford University, 1970.</cite> * <cite id=algol68> [[Adriaan van Wijngaarden]] et al. [[ALGOL 68|Revised Report on the Algorithmic Language ALGOL 68. IFIP. 1968.]] ([http://vestein.arb-phys.uni-dortmund.de/~wb/RR/rr.pdf])</cite> * <cite id=hennessybook>[[Matthew Hennessy]]. Semantics of Programming Languages. Wiley, 1990. [http://www.cogs.susx.ac.uk/users/matthewh/semnotes.ps.gz available online].</cite> [[Category:Formal specification languages]] [[Category:Logic in computer science]] [[Category:Programming language semantics]] [[de:Operationelle Semantik]] [[fr:Sémantique opérationnelle]] [[hr:Operacijska semantika]] [[ja:操作的意味論]] [[pt:Semântica operacional]] [[uk:Операційна семантика]] [[zh:操作语义学]]</text> </page> <page> <id>27640</id> <title>Operator Grammar</title> <text>{{About||the class of formal computer languages|Operator-precedence_grammar}} '''Operator Grammar''' is a mathematical theory of human [[language]] that explains how language carries [[information]]. This theory is the culmination of the life work of [[Zellig Harris]], with major [[#Bibliography|publications]] toward the end of the last century. Operator Grammar proposes that each human language is a [[self-organizing]] system in which both the [[syntax|syntactic]] and [[semantics|semantic]] properties of a word are established purely in relation to other words. Thus, no external system ([[metalanguage]]) is required to define the rules of a language. Instead, these rules are learned through exposure to usage and through participation, as is the case with most [[social construction|social behavior]]. The theory is consistent with the idea that [[origin of language|language evolved]] gradually, with each successive generation introducing new complexity and variation. Operator Grammar posits three [[linguistic universal|universal]] constraints: [[#Dependency|Dependency]] (certain words depend on the presence of other words to form an utterance), [[#Likelihood|Likelihood]] (some combinations of words and their dependents are more likely than others) and [[#Reduction|Reduction]] (words in high likelihood combinations can be reduced to shorter forms, and sometimes omitted completely). Together these provide a theory of [[#Information|language information]]: dependency builds a [[relation (mathematics)|predicate-argument structure]]; likelihood creates distinct meanings; reduction allows compact forms for communication. ==Dependency== The fundamental mechanism of Operator Grammar is the dependency constraint: certain words ([[operator]]s) require that one or more words (arguments) be present in an utterance. In the sentence ''John wears boots'', the operator ''wears'' requires the presence of two arguments, such as ''John'' and ''boots''. (This definition of dependency differs from other [[dependency grammar]]s in which the arguments are said to depend on the operators.) In each language the dependency relation among words gives rise to [[part of speech|syntactic categories]] in which the allowable arguments of an operator are defined in terms of their dependency requirements. Class N contains words (e.g. ''John'', ''boots'') that do not require the presence of other words. Class O<sub>N</sub> contains the words (e.g. ''sleeps'') that require exactly one word of type N. Class O<sub>NN</sub> contains the words (e.g. ''wears'') that require 