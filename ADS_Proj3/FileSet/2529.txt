''θ''. == Example 1== [[Image:likelihoodFunctionAfterHH.png|thumb|400px|The likelihood function for estimating the probability of a coin landing heads-up without prior knowledge after observing HH]] [[Image:likelihoodFunctionAfterHHT.png|thumb|400px|The likelihood function for estimating the probability of a coin landing heads-up without prior knowledge after observing HHT]] Let <math>p_\text{H}</math> be the probability that a certain coin lands heads up (H) when tossed. So, the probability of getting two heads in two tosses (HH) is <math>p_\text{H}^2</math>. If <math>p_\text{H} = 0.5</math>, then the probability of seeing two heads is 0.25. In symbols, we can say the above as: :<math>P(\text{HH} | p_\text{H}=0.5) = 0.25</math>. Another way of saying this is to reverse it and say that "the likelihood that <math>p_\text{H} = 0.5</math>, given the observation HH, is 0.25"; that is: :<math>L(p_\text{H}=0.5 | \text{HH}) = P(\text{HH} | p_\text{H}=0.5) = 0.25.</math> But this is not the same as saying that the ''probability'' that <math>p_\text{H} = 0.5</math>, given the observation HH, is 0.25. Notice that the likelihood that <math>p_\text{H} = 1</math>, given the observation HH, is 1. But it is clearly not true that the ''probability'' that <math>p_\text{H} = 1</math>, given the observation HH, is 1. Two heads in a row hardly proves that the coin ''always'' comes up heads. In fact, two heads in a row is possible for any <math>p_\text{H} > 0</math>. The likelihood function is not a [[probability density function]]. Notice that the integral of a likelihood function is not in general 1. In this example, the integral of the likelihood over the interval [0, 1] in <math>p_\text{H}</math> is 1/3, demonstrating that the likelihood function cannot be interpreted as a probability density function for <math>p_\text{H}</math>. == Example 2== {{Main|German tank problem}} Consider a jar containing ''N'' lottery tickets numbered from 1 through ''N''. If you pick a ticket randomly then you get positive integer ''n'', with probability 1/''N'' if ''n'' ≤ ''N'' and with probability zero if ''n'' > ''N''. This can be written :<math>P(n|N)= \frac{[n \le N]}{N}</math> where the [[Iverson bracket]] [''n'' ≤ ''N''] is 1 when ''n'' ≤ ''N'' and 0 otherwise. When considered a function of ''n'' for fixed ''N'' this is the probability distribution, but when considered a function of ''N'' for fixed ''n'' this is a likelihood function. The [[maximum likelihood]] estimate for ''N'' is ''N''<sub>0</sub> = ''n'' (by contrast, the [[Bias_of_an_estimator#Maximum_of_a_discrete_uniform_distribution|unbiased estimate]] is 2''n'' &minus; 1). This likelihood function is not a probability distribution, because the total :<math>\sum_{N=1}^\infty P(n|N) = \sum_{N} \frac{[N \ge n]}{N} = \sum_{N=n}^\infty \frac{1}{N}</math> is a [[divergent series]]. Suppose, however, that you pick ''two'' tickets rather than ''one''. The probability of the outcome {''n''<sub>1</sub>, ''n''<sub>2</sub>}, where ''n''<sub>1</sub> < ''n''<sub>2</sub>, is :<math>P(\{n_1,n_2\}|N)= \frac{[n_2 \le N]}{\binom N 2} .</math> When considered a function of ''N'' for fixed ''n''<sub>2</sub>, this is a likelihood function. The [[maximum likelihood]] estimate for ''N'' is ''N''<sub>0</sub> = ''n''<sub>2</sub>. This time the total :<math>\sum_{N=1}^\infty P(\{n_1,n_2\}|N) = \sum_{N} \frac{[N\ge n_2]}{\binom N 2} =\frac 2 {n_2-1} </math> is a convergent series, and so this likelihood function can be normalized into a probability distribution. If you pick 3 or more tickets, the likelihood function has a well defined [[mean value]], which is larger than the maximum likelihood estimate. If you pick 4 or more tickets, the likelihood function has a well defined [[standard deviation]] too. ==Likelihoods that eliminate nuisance parameters== In many cases, the likelihood is a function of more than one parameter but interest focuses on the estimation of only one, or at most a few of them, with the others being considered as [[nuisance parameter]]s. Several alternative approaches have been developed to eliminate such [[nuisance parameter]]s so that a likelihood can be written as a function of only the [[Nuisance parameter|parameter (or parameters) of interest]]; the main approaches being marginal, conditional and profile likelihoods.<ref> {{cite book | title=In All Likelihood: Statistical Modelling and Inference Using Likelihood | first=Yudi | last=Pawitan | year=2001| publisher=Oxford University Press|isbn=0198507658 }}</ref><ref> {{cite web | author = Wen Hsiang Wei | url= http://web.thu.edu.tw/wenwei/www/glmpdfmargin.htm | title = Generalized linear model course notes | pages = Chapter 5 | publisher = Tung Hai University, Taichung, Taiwan | accessdate = 2007-01-23 }}</ref> These approaches are useful because standard likelihood methods can become unreliable or fail entirely when there are many nuisance parameters or when the nuisance parameters are high-dimensional. This is particularly true when the nuisance parameters can be considered to be "missing data"; they represent a non-negligible fraction of the number of observations and this fraction does not decrease when the sample size increases. Often these approaches can be used to derive closed-form formulae for statistical tests when direct use of maximum likelihood requires iterative numerical methods. These approaches find application in some specialized topics such as [[sequential analysis]]. ===Conditional likelihood=== Sometimes it is possible to find a [[sufficient statistic]] for the nuisance parameters, and conditioning on this statistic results in a likelihood which does not depend on the nuisance parameters. One example occurs in 2×2 tables, where conditioning on all four marginal totals leads to a conditional likelihood based on the non-central [[hypergeometric distribution]]. This form of conditioning is also the basis for [[Fisher's exact test]]. ===Marginal likelihood=== {{Main|Marginal likelihood}} <!-- needs expansion ~~~~ --> Sometimes we can remove the nuisance parameters by considering a likelihood based on only part of the information in the data, for example by using the set of ranks rather than the numerical values. Another example occurs in linear [[mixed model]]s, where considering a likelihood for the residuals only after fitting the fixed effects leads to [[residual maximum likelihood]] estimation of the variance components. ===Profile likelihood=== <!-- 1st 2 paras based on material previously headed "Concentrated likelihood" ~~~~~ --> It is often possible to write some parameters as functions of other parameters, thereby reducing the number of independent parameters. (The function is the parameter value which maximizes the likelihood given the value of the other parameters.) This procedure is called concentration of the parameters and results in the concentrated likelihood function, also occasionally known as the maximized likelihood function, but most often called the profile likelihood function. <!-- Suggestions for expansion: an 