RGB only float size; // ... and many other attributes may follow... </source> What happened is that those structures were then assembled in [[Array data structure|arrays]] to keep things nicely organized. This is '''AoS'''. When the structure is laid out in memory, the compiler will produce interleaved data, in the sense that all the structures will be contiguous but there will be a constant offset between, say, the "size" attribute of a structure instance and the same element of the following instance. The offset depends on the structure definition (and possibly other things not considered here such as compiler's policies). There are also other problems. For example, the three position variables cannot be SIMD-ized that way, because it's not sure they will be allocated in continuous memory space. To make sure SIMD operations can work on them, they shall be grouped in a 'packed memory location' or at least in an array. Another problem lies in both "color" and "xyz" to be defined in three-component vector quantities. SIMD processors usually have support for 4-component operations only (with some exceptions however). These kinds of problems and limitations made SIMD acceleration on standard CPUs quite nasty. The proposed solution, ''SoA'' follows as: <source lang="c"> struct particle_t float *x, *y, *z; unsigned byte *colorRed, *colorBlue, *colorGreen; float *size; </source> For readers not experienced with [[C (programming language)|C]], the '*' before each identifier means a pointer. In this case, they will be used to point to the first element of an array, which is to be allocated later. For [[Java (programming language)|Java]] programmers, this is roughly equivalent to "[]". The drawback here is that the various attributes could be spread in memory. To make sure this does not cause cache misses, we'll have to update all the various "reds", then all the "greens" and "blues". Although this is not so bad after all, it's simply overkill when compared to what most stream processors offer. For stream processors, the usage of structures is encouraged. From an application point of view, all the attributes can be defined with some flexibility. Taking GPUs as reference, there is a set of attributes (at least 16) available. For each attribute, the application can state the number of components and the format of the components (but only primitive data types are supported for now). The various attributes are then attached to a memory block, possibly defining a ''stride'' between 'consecutive' elements of the same attributes, effectively allowing interleaved data. When the GPU begins the stream processing, it will ''gather'' all the various attributes in a single set of parameters (usually this looks like a structure or a "magic global variable"), performs the operations and ''scatters'' the results to some memory area for later processing (or retrieving). Summing up, there's more flexibility on the application's side yet everything looks very organized on the stream processor's side. === Models-of-Computation for Stream Processing === Apart from specifying streaming applications in high-level language. Models-of-Computation (MoCs) also have been widely used such as dataflow models and process-based models. === Generic processor architecture === Historically, CPUs began implementing various tiers of memory access optimizations because of the ever increasing performance when compared to relatively slow growing external memory bandwidth. As this gap widened, big amounts of die area were dedicated to hiding memory latencies. Since fetching information and opcodes to those few ALUs is expensive, very little die area is dedicated to actual mathematical machinery (as a rough estimation, consider it to be less than 10%). A similar architecture exists on stream processors but thanks to the new programming model, the amount of transistors dedicated to management is actually very little. Beginning from a whole system point of view, stream processors usually exist in a controlled environment. GPUs do exist on an add-in board (this seems to also apply to [http://cva.stanford.edu/projects/imagine/ Imagine]). CPUs do the dirty job of managing system resources, running applications and such. The stream processor is usually equipped with a fast, efficient, proprietary memory bus (crossbar switches are now common, multi-buses have been employed in the past). The exact amount of memory lanes is dependent on the market range. As this is written, there are still 64-bit wide interconnections around (entry-level). Most mid-range models use a fast 128-bit crossbar switch matrix (4 or 2 segments), while high-end models deploy huge amounts of memory (actually up to 512MB) with a slightly slower crossbar that is 256 bits wide. By contrast, standard processors from [[Intel Pentium]] to some [[Athlon 64]] have only a single 64-bit wide data bus. Memory access patterns are much more predictable. While arrays do exist, their dimension is fixed at kernel invocation. The thing which most closely matches a multiple pointer indirection is an ''indirection chain'', which is however guaranteed to finally read or write from a specific memory area (inside a stream). Because of the SIMD nature of the stream processor's execution units (ALUs clusters), read/write operations are expected to happen in bulk, so memories are optimized for high bandwidth rather than low latency (this is a difference from [[Rambus]] and [[DDR SDRAM]], for example). This also allows for efficient memory bus negotiations. Most (90%) of a stream processor's work is done on-chip, requiring only 1% of the global data to be stored to memory. This is where knowing the kernel temporaries and dependencies pays. Internally, a stream processor features some clever communication and management circuits but what's interesting is the ''Stream Register File'' (SRF). This is conceptually a large cache in which stream data is stored to be transferred to external memory in bulks. As a cache-like software-controlled structure to the various [[arithmetic logic unit|ALU]]s, the SRF is shared between all the various ALU clusters. The key concept and innovation here done with Stanford's Imagine chip is that the compiler is able to automate and allocate memory in an optimal way, fully transparent to the programmer. The dependencies between kernel functions and data is known through the programming model which enables the compiler to perform flow analysis and optimally 