Beginning in the late 1970s, [[process calculi]] such as [[Calculus of Communicating Systems]] and [[Communicating Sequential Processes]] were developed to permit algebraic reasoning about systems composed of interacting components. More recent additions to the process calculus family, such as the [[pi calculus|&pi;-calculus]], have added the capability for reasoning about dynamic topologies. Logics such as Lamport's [[Temporal logic of actions|TLA+]], and mathematical models such as [[Trace theory|traces]] and [[Actor model theory|Actor event diagrams]], have also been developed to describe the behavior of concurrent systems. ===Flynn's taxonomy=== [[Michael J. Flynn]] created one of the earliest classification systems for parallel (and sequential) computers and programs, now known as [[Flynn's taxonomy]]. Flynn classified programs and computers by whether they were operating using a single set or multiple sets of instructions, whether or not those instructions were using a single or multiple sets of data. {{Flynn's Taxonomy}} The single-instruction-single-data (SISD) classification is equivalent to an entirely sequential program. The single-instruction-multiple-data (SIMD) classification is analogous to doing the same operation repeatedly over a large data set. This is commonly done in [[signal processing]] applications. Multiple-instruction-single-data (MISD) is a rarely used classification. While computer architectures to deal with this were devised (such as [[systolic array]]s), few applications that fit this class materialized. Multiple-instruction-multiple-data (MIMD) programs are by far the most common type of parallel programs. According to [[David A. Patterson (scientist)|David A. Patterson]] and [[John L. Hennessy]], "Some machines are hybrids of these categories, of course, but this classic model has survived because it is simple, easy to understand, and gives a good first approximation. It is also—perhaps because of its understandability—the most widely used scheme."<ref>Patterson and Hennessy, p. 748.</ref> ==Types of parallelism== ===Bit-level parallelism=== {{main|Bit-level parallelism}} From the advent of [[very-large-scale integration]] (VLSI) computer-chip fabrication technology in the 1970s until about 1986, speed-up in computer architecture was driven by doubling [[Word (computing)|computer word size]]—the amount of information the processor can manipulate per cycle.<ref>[[David Culler|Culler, David E.]]; Jaswinder Pal Singh and Anoop Gupta (1999). ''Parallel Computer Architecture - A Hardware/Software Approach''. Morgan Kaufmann Publishers, p. 15. ISBN 1558603433.</ref> Increasing the word size reduces the number of instructions the processor must execute to perform an operation on variables whose sizes are greater than the length of the word. For example, where an [[8-bit]] processor must add two [[16-bit]] [[integer]]s, the processor must first add the 8 lower-order bits from each integer using the standard addition instruction, then add the 8 higher-order bits using an add-with-carry instruction and the [[carry bit]] from the lower order addition; thus, an 8-bit processor requires two instructions to complete a single operation, where a 16-bit processor would be able to complete the operation with a single instruction. Historically, [[4-bit]] microprocessors were replaced with 8-bit, then 16-bit, then 32-bit microprocessors. This trend generally came to an end with the introduction of 32-bit processors, which has been a standard in general-purpose computing for two decades. Not until recently (c. 2003&ndash;2004), with the advent of [[x86-64]] architectures, have [[64-bit]] processors become commonplace. ===Instruction-level parallelism=== {{main|Instruction level parallelism}} [[Image:Fivestagespipeline.png|thumb|300px|A canonical five-stage pipeline in a [[RISC]] machine (IF = Instruction Fetch, ID = Instruction Decode, EX = Execute, MEM = Memory access, WB = Register write back)]] A computer program is, in essence, a stream of instructions executed by a processor. These instructions can be [[Out-of-order execution|re-ordered]] and combined into groups which are then executed in parallel without changing the result of the program. This is known as instruction-level parallelism. Advances in instruction-level parallelism dominated computer architecture from the mid-1980s until the mid-1990s.<ref>Culler et al. p. 15.</ref> Modern processors have multi-stage [[instruction pipeline]]s. Each stage in the pipeline corresponds to a different action the processor performs on that instruction in that stage; a processor with an N-stage pipeline can have up to N different instructions at different stages of completion. The canonical example of a pipelined processor is a [[Reduced Instruction Set Computer|RISC]] processor, with five stages: instruction fetch, decode, execute, memory access, and write back. The [[Pentium 4]] processor had a 35-stage pipeline.<ref>[[Yale Patt|Patt, Yale]] (April 2004). "[http://users.ece.utexas.edu/~patt/Videos/talk_videos/cmu_04-29-04.wmv The Microprocessor Ten Years From Now: What Are The Challenges, How Do We Meet Them?] (wmv). Distinguished Lecturer talk at [[Carnegie Mellon University]]. Retrieved on November 7, 2007.</ref> [[Image:Superscalarpipeline.svg|thumb|300px|A five-stage pipelined [[superscalar]] [[Microprocessor|processor]], capable of issuing two instructions per cycle. It can have two instructions in each stage of the pipeline, for a total of up to 10 instructions (shown in green) being simultaneously executed.]] In addition to instruction-level parallelism from pipelining, some processors can issue more than one instruction at a time. These are known as [[superscalar]] processors. Instructions can be grouped together only if there is no [[data dependency]] between them. [[Scoreboarding]] and the [[Tomasulo algorithm]] (which is similar to scoreboarding but makes use of [[register renaming]]) are two of the most common techniques for implementing out-of-order execution and instruction-level parallelism . ===Data parallelism=== {{main|Data parallelism}} Data parallelism is parallelism inherent in [[Control flow#Loops|program loops]], which focuses on distributing the data across different computing nodes to be processed in parallel. "Parallelizing loops often leads to similar (not necessarily identical) operation sequences or functions being performed on elements of a large data structure."<ref name=Culler124>Culler et al. p. 124.</ref> Many scientific and engineering applications exhibit data parallelism. A loop-carried dependency is the dependence of a loop iteration on the output of one or more previous iterations. Loop-carried dependencies prevent the parallelization of loops. For example, consider the following [[pseudocode]] that computes the first few [[Fibonacci number]]s: 1: PREV1 := 0 2: PREV2 := 1 4: do: 5: CUR := PREV1 + PREV2 6: PREV1 := PREV2 7: PREV2 := CUR 8: while (CUR < 10) This loop cannot be parallelized because CUR depends on itself (PREV2) and PREV1, which are computed in each loop iteration. Since each iteration depends on the result of the previous one, they cannot be performed in parallel. As the size of a problem gets bigger, the amount of data-parallelism available usually does as well.<ref name=Culler125>Culler et al. p. 125.</ref> ===Task parallelism=== {{main|Task parallelism}} Task 