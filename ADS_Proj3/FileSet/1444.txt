can develop new algorithms and systems that meet the FRGC goals. The development of the new algorithms and systems is facilitated by the FRGC challenge problems. ==Structure of the Face Recognition Grand Challenge== The FRGC is structured around challenge problems that are designed to challenge researchers to meet the FRGC performance goal. There are three aspects of the FRGC that will be new to the face recognition community. The first aspect is the size of the FRGC in terms of data. The FRGC data set contains 50,000 recordings. The second aspect is the complexity of the FRGC. Previous face recognition data sets have been restricted to still images. The FRGC will consist of three modes: *high resolution still images *3D images *multi-images of a person. The third new aspect is the infrastructure. The infrastructure for FRGC will be provided by the [[Biometric Experimentation Environment]] (BEE), an [[XML]] based [[Software framework|framework]] for describing and documenting computational experiments. The BEE will allow the description and distribution of experiments in a common format, recording of the raw results of an experiment in a common format, analysis and presentation of the raw results in a common format, and documentation of the experiment format in a common format. This is the first time that a computational-experimental environment has supported a challenge problem in face recognition or biometrics. ==The FRGC Data Set== The FRGC data distribution consists of three parts. The first is the FRGC data set. The second part is the FRGC BEE. The BEE distribution includes all the data sets for performing and scoring the six experiments. The third part is a set of baseline algorithms for experiments 1 through 4. With all three components, it is possible to run experiments 1 through 4, from processing the raw images to producing Receiver Operating Characteristics (ROCs). The data for FRGC consists of 50,000 recordings divided into training and validation partitions. The training partition is designed for training algorithms and the validation partition is for assessing performance of an approach in a laboratory setting. The validation partition consists of data from 4,003 subject sessions. A subject session is the set of all images of a person taken each time a person's biometric data is collected and consists of four controlled still images, two uncontrolled still images, and one three-dimensional image. The controlled images were taken in a studio setting, are full frontal facial images taken under two lighting conditions and with two facial expressions (smiling and neutral). The uncontrolled images were taken in varying illumination conditions; e.g., hallways, atriums, or outside. Each set of uncontrolled images contains two expressions, smiling and neutral. The 3D image was taken under controlled illumination conditions. The 3D images consist of both a range and a texture image. The 3D images were acquired by a Minolta Vivid 900/910 series sensor1. The FRGC distribution consists of six experiments. In experiment 1, the gallery consists of a single controlled still image of a person and each probe consists of a single controlled still image. Experiment 1 is the control experiment. Experiment 2 studies the effect of using multiple still images of a person on performance. In experiment 2, each biometric sample consists of the four controlled images of a person taken in a subject session. For example, the gallery is composed of four images of each person where all the images are taken in the same subject session. Likewise, a probe now consists of four images of a person. Experiment 3 measures the performance of 3D face recognition. In experiment 3, the gallery and probe set consist of 3D images of a person. Experiment 4 measures recognition performance from uncontrolled images. In experiment 4, the gallery consists of a single controlled still image, and the probe set consists of a single uncontrolled still image. Experiments 5 and 6 examine comparing 3D and 2D images. In both experiments, the gallery consists of 3D images. In experiment 5, the probe set consists of a single controlled still. In experiment 6, the probe set consists of a single uncontrolled still. ==Sponsors== *[[Intelligence Advanced Research Projects Activity|Intelligence Advanced Research Projects Agency (IARPA)]] *[[DHS|Department of Homeland Security (DHS)]] *[[Criminal Justice Information Services Division|FBI Criminal Justice Information Services Division]] *[[TSWG|Technical Support Working Group (TSWG)]] *[[National Institute of Justice]] == References == {{NIST-PD|article=NIST Face Recognition Grand Challenge|url=http://face.nist.gov/frgc/}} ==External links== *[http://face.nist.gov/mbgc/ MBGC Website] *[http://mbgc.bee-biometrics.org/ MBGC Blog] *[http://face.nist.gov/frgc/ FRGC Website] *[http://face.nist.gov/frvt/frvt2006/frvt2006.htm FRVT Website] *[http://iris.nist.gov/ice/ ICE Website] *[http://nist.gov National Institute of Standards and Technology] *[http://www.iarpa.gov Intelligence Advanced Research Projects Agency] *[http://www.dhs.gov/index.shtm Department of Homeland Security] *[http://www.fbi.gov/hq/cjisd/cjis.htm FBI Criminal Justice Information Services Division] *[http://www.tswg.gov/ Technical Support Working Group (TSWG)] [[Category:Biometrics]] [[Category:Face recognition]]</text> </page> <page> <id>13209</id> <title>Facial expression capture</title> <text>{{merge|facial motion capture|discuss=Talk:Facial motion capture#Merge discussion|date=November 2009}} '''Facial expression capture''' is a process of using visual or mechanical means to manipulate computer generated characters with input from [[human]] [[face]]s, or to recognize [[emotion]]s from a user. Digital video-based methods are becoming increasingly preferred, as mechanical systems tend to be cumbersome and difficult to use. ==Technology== Using [[digital camera]]s, the input user's expressions are processed to provide the head pose, which allows the software to then find the eyes, nose and mouth. The face is initially calibrated using a neutral expression. Then depending on the architecture, the eyebrows, eyelids, cheeks, and mouth can be processed as differences from the neutral expression. This is done by looking for the edges of the lips for instance and recognizing it as a unique object. Often contrast enhancing makeup or markers are worn, or some other method to make the processing faster. Like voice recognition, the best techniques are only good 90 percent of the time, requiring a great deal of tweaking by hand, or tolerance for errors. Since computer generated characters don't actually have [[muscle]]s, different techniques are used to achieve the same results. Some animators create bones or objects that are controlled by the capture software, and move them accordingly, which when the character is rigged correctly gives a good approximation. Since faces are very elastic this technique is 