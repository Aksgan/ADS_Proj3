in an article entitled "[[artificial stupidity|Artificial Stupidity]]" published shortly after the first Loebner prize competition in 1992. The article noted that the first Loebner winner's victory was due, at least in part, to its ability to "imitate human typing errors."{{sfn|"Artificial Stupidity"|1992}} Turing himself had suggested that programs add errors into their output, so as to be better "players" of the game.<ref>{{harvnb|Turing|1950|p=448}}</ref> ;Some intelligent behavior is inhuman: The Turing test does not test for highly intelligent behaviors, such as the ability to solve difficult problems or come up with original insights. In fact, it specifically requires deception on the part of the machine: if the machine is ''more'' intelligent than a human being it must deliberately avoid appearing too intelligent. If it were to solve a computational problem that is impossible for any human to solve, then the interrogator would know the program is not human, and the machine would fail the test. :Because it can't measure intelligence that is beyond the ability of humans, the test can't be used in order to build or evaluate systems that are more intelligent than humans. Because of this, several test alternatives that would be able to evaluate superintelligent systems have been proposed.<ref>{{Citation | title = Beyond the Turing Test | journal = Journal of Logic, Language and Information | author = Jose Hernandez-Orallo | url = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.8943 | accessdate = 2009-07-21 | year = 2000 | pages = 447–466 | volume = 9 | issue = 4 | doi = 10.1023/A:1008367325700 | postscript = . }}</ref><ref>{{Citation | title = A computational extension to the Turing Test | journal = Proceedings of the 4th Conference of the Australasian Cognitive Science Society | author = D L Dowe and A R Hajek | url = http://www.csse.monash.edu.au/publications/1997/tr-cs97-322-abs.html | accessdate = 2009-07-21 | year = 1997 | postscript = . }}</ref><ref>{{Citation | title = Universal Intelligence: A Definition of Machine Intelligence | journal = Minds and Machines | author = Shane Legg and Marcus Hutter | url = http://www.vetta.org/documents/UniversalIntelligence.pdf | format = PDF | accessdate = 2009-07-21 | year = 2007 | pages = 391–444 | volume = 17 | doi = 10.1007/s11023-007-9079-x | postscript = . }}</ref><ref name="J Hernandez-Orallo and D L Dowe 2010">{{cite journal | title = Measuring Universal Intelligence: Towards an Anytime Intelligence Test | journal = Artificial Intelligence Journal | author = J Hernandez-Orallo and D L Dowe | url = http://dx.doi.org/10.1016/j.artint.2010.09.006 | accessdate = 2010-10-01 | year = 2010| ref = harv }}</ref> ===Real intelligence vs simulated intelligence=== {{See also|synthetic intelligence}} The Turing test is concerned strictly with how the subject ''acts'' — the external behaviour of the machine. In this regard, it assumes a [[behaviourist]] or [[Functionalism (philosophy of mind)|functionalist]] definition of intelligence. The example of [[ELIZA]] suggested that a machine passing the test may be able to simulate human conversational behavior by following a simple (but large) list of mechanical rules, without thinking or having a mind at all. [[John Searle]] argued that external behavior cannot be used to determine if a machine is "actually" thinking or merely "simulating thinking."<ref name="Searle 1980"/> His [[chinese room]] argument is intended to show that, even if the Turing test is a good operational definition of intelligence, it may not indicate that the machine has a [[mind]], [[consciousness]], or [[intentionality]]. (Intentionality is a philosophical term for the power of thoughts to be "about" something.) Turing anticipated to this line of criticism in his original paper,<ref>{{Harvtxt|Russell|Norvig|2003|pp=958–960}} identify Searle's argument with the one Turing answers.</ref> writing that: {{quote|I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localise it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper.| Alan Turing, {{Harv|Turing|1950}} }} ===Naivete of interrogators and the anthropomorphic fallacy=== The Turing test assumes that the interrogator is sophisticated enough to determine the difference between the behaviour of a machine and the behaviour of a human being, though critics argue that this is not a skill most people have. Turing does not specify the precise skills and knowledge required by the interrogator in his description of the test, but he did use the term "average interrogator": "[the] average interrogator would not have more than 70 per cent chance of making the right identification after five minutes of questioning".{{sfn|Turing|1950|p=442}} {{harvtxt|Shah|Warwick|2009c}}{{citation not found}} show that experts are fooled, and that interrogator strategy, 'power' vs 'solidarity' affects correct identification, the latter being more successful. Chatterbot programs such as ELIZA have repeatedly fooled unsuspecting people into believing that they are communicating with human beings. In these cases, the "interrogator" is not even aware of the possibility that they are interacting with a computer. To successfully appear human, there is no need for the machine to have any intelligence whatsoever and only a superficial resemblance to human behaviour is required. Most would agree that a "true" Turing test has not been passed in "uninformed" situations like these.{{Citation needed|date=November 2010}} Early Loebner prize competitions used "unsophisticated" interrogators who were easily fooled by the machines.<ref name=SHAPIRO_SHIEBER/> Since 2004, the Loebner Prize organizers have deployed philosophers, computer scientists, and journalists among the interrogators. However, even some of these experts have been deceived by the machines.{{sfn|Shah|Warwick|2010}} [[Michael Shermer]] points out that human beings consistently choose to consider non-human objects as human whenever they are allowed the chance, a mistake called the [[anthropomorphic fallacy]]: They talk to their cars, ascribe desire and intentions to natural forces (e.g., "nature abhors a vacuum"), and worship the sun as a human-like being with intelligence. If the Turing test is applied to religious objects, Shermer argues, then, that inanimate statues, rocks, and places have consistently passed the test throughout history.<ref>{{Harvnb|Shermer|YEAR?}} [[CITATION IN PROGRESS]]</ref> This human tendency towards anthropomorphism effectively lowers the bar for the Turing test, unless interrogators are specifically trained to avoid it. ===Impracticality and irrelevance: the Turing test and AI research=== Mainstream 