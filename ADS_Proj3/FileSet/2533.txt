with example code]] [[Category:Text editor features]] [[ja:行番号]]</text> </page> <page> <id>21702</id> <title>Line search</title> <text>In [[optimization (mathematics)|optimization]], the '''line search''' strategy is one of two basic [[iteration|iterative]] approaches to finding a [[maxima and minima|local minimum]] <math>\mathbf{x}^*</math> of an [[objective function]] <math>f:\mathbb R^n\to\mathbb R</math>. The other approach is [[trust region]]. The line search approach first finds a descent direction along which the objective function <math>f</math> will be reduced and then computes a step size that decides how far <math>\mathbf{x}</math> should move along that direction. The descent direction can be computed by various methods, such as [[gradient descent]], [[newton's method]] and [[Quasi-Newton method]]. The step size can be determined either exactly or inexactly. ==Example use == Here is an example gradient method that uses a line search in step 4. # Set iteration counter <math>\displaystyle k=0</math>, and make an initial guess, <math>\mathbf{x}_0</math> for the minimum # Repeat: # Compute a [[descent direction]] <math>\mathbf{p}_k</math> # Choose <math>\displaystyle \alpha_k</math> to 'loosely' minimize <math>h(\alpha)=f(\mathbf{x}_k+\alpha\mathbf{p}_k)</math> over <math>\alpha\in\mathbb R_+</math> # Update <math>\mathbf{x}_{k+1}=\mathbf{x}_k+\alpha_k\mathbf{p}_k</math>, and <math>\displaystyle k=k+1</math> # Until <math>\|\nabla f(\mathbf{x}_k)\|</math> < tolerance At the line search step (4) the algorithm might either ''exactly'' minimize ''h'', by solving <math>h'(\alpha_k)=0</math>, or ''loosely'', by asking for a sufficient decrease in ''h''. One example of the former is [[conjugate gradient method]]. The latter may be performed in a number of ways, perhaps by doing a [[backtracking line search]] or using the [[Wolfe conditions]]. Like other optimization methods, line search may be combined with [[simulated annealing]] to allow it to jump over some [[local minimum|local minima]]. ==Algorithms== === Direct search methods === In this method, the minimum must first be bracketed, so the algorithm must identify points x<sub>1</sub> and x<sub>2</sub> that are above the minimum. The interval is then divided by computing f(x) at two internal points, x<sub>3</sub> and x<sub>4</sub>, and rejecting whichever of the two outer points has the highest function value. Subsequently only one extra internal point needs to be calculated. Of the various methods of dividing the interval<ref>M.J. Box, D. Davies and W.H. Swann, Non-Linear optimisation Techniques, Oliver & Boyd, 1969</ref>, those that use the [[golden section search|golden ratio]] are particularly simple and effective . :<math>x_4-x_1=x_2-x_3=\frac{1}{\tau}(x_2-x_1), \tau=\frac{1}{2}(1+\sqrt 5)</math> ==See also== * [[Secant method]] * [[Newton–Raphson method]] * [[Pattern search (optimization)]] ==References== <references /> [[Category:Mathematical optimization]] [[fr:Recherche linéaire]] [[sl:Minimizacija v dani smeri]]</text> </page> <page> <id>21706</id> <title>Line starve</title> <text>{{Unreferenced stub|auto=yes|date=December 2009}} {{Orphan|date=November 2006}} A '''line starve''' describes the feeding of paper in a [[line printer]] back one line. It is the opposite of a [[line feed]]. {{DEFAULTSORT:Line Starve}} [[Category:Computer hardware]] {{Compu-hardware-stub}}</text> </page> <page> <id>21711</id> <title>Linear approximation</title> <text>[[Image:TangentGraphic2.svg|thumb|300px|Tangent line at (''a'', ''f''(''a''))]] In [[mathematics]], a '''linear approximation''' is an approximation of a general [[function (mathematics)|function]] using a [[linear function]] (more precisely, an [[affine function]]). They are widely used in the method of [[finite differences]] to produce first order methods for solving or approximating solutions to equations. ==Definition== Given a twice continuously differentiable function ''f'' of one [[real number|real]] variable, [[Taylor's theorem]] for the case ''n'' = 1 states that :<math> f(x) = f(a) + f'(a)(x - a) + R_2\ </math> where <math>R_2</math> is the remainder term. The linear approximation is obtained by dropping the remainder: :<math> f(x) \approx f(a) + f'(a)(x - a).</math><ref>Some calculus textbooks write d''x'' for ''x''−''a'' (the change in ''x''), and then define d''f'':=''f''′(''a'')(''x''−''a'') so as to have a numerical equality d''f'' = ''f''′(''a'') d''x''. This may be useful as a mnemonic for the fact that ''f''(''x'')−f(''a'') (the change in ''f'') is approximated by ''f''′(''a'')(''x''−''a''), but it conflicts with the actual definition of d''f'' as a [[differential form]].</ref> This is a good approximation for ''x'' when it is close enough to ''a'' since a curve, when closely observed, will begin to resemble a straight line. Therefore, the expression on the right-hand side is just the equation for the [[tangent line]] to the graph of ''f'' at (''a'',''f''(''a'')). For this reason, this process is also called the '''tangent line approximation'''. Linear approximations for [[vector (geometric)|vector]] functions of a vector variable are obtained in the same way, with the derivative at a point replaced by the [[Jacobian matrix and determinant|Jacobian]] matrix. For example, given a differentiable function <math>f(x, y)</math> with real values, one can approximate <math>f(x, y)</math> for <math>(x, y)</math> close to <math>(a, b)</math> by the formula :<math>f\left(x,y\right)\approx f\left(a,b\right)+\frac{\partial f}{\partial x}\left(a,b\right)\left(x-a\right)+\frac{\partial f}{\partial y}\left(a,b\right)\left(y-b\right).</math> The right-hand side is the equation of the plane tangent to the graph of <math>z=f(x, y)</math> at <math>(a, b).</math> In the more general case of [[Banach space]]s, one has :<math> f(x) \approx f(a) + Df(a)(x - a)</math> where <math>Df(a)</math> is the [[Fréchet derivative]] of <math>f</math> at <math>a</math>. To find an approximation of <math>\sqrt[3]{25}</math> one can do as follows. # Consider the function <math> f(x)= x^{1/3}.\,</math> Hence, the problem is reduced to finding the value of <math>f(25)</math>. # We have #:<math>f'(x)=\frac{x^{-2/3}}{3}=\frac{1}{3\sqrt[3]{x^2}}</math> # According to linear approximation #:<math> f(25) \approx f(27) + f\ '(27)(25 - 27) = 3 - 2/27.</math> # The result, 2.926, lies fairly close to the actual value 2.924… == See also == * [[Taylor expansion]] * [[Power series]] === Applications === * [[:Category:First order methods]] * [[Euler's method]] * [[Finite differences]] * [[Finite difference methods]] * [[Newton's method]] == Notes == <references/> ==References== *{{cite book |author=Weinstein, Alan; Marsden, Jerrold E. |title=Calculus III |publisher=Springer-Verlag |location=Berlin |year=1984|isbn=0-387-90985-0 |oclc= |doi= |page= 775}} * {{cite book |author=Strang, Gilbert |title=Calculus |publisher=Wellesley College |location= |year=1991|isbn=0-9614088-2-0 |oclc= |doi= |page= 94}} *{{cite book |author=Bock, David; Hockett, Shirley O. |title=How to Prepare for the AP Calculus|publisher=Barrons Educational Series |location=Hauppauge, NY |year=2005 |isbn=0-7641-2382-3 |oclc= |doi= |page=118}} {{DEFAULTSORT:Linear Approximation}} [[Category:Differential calculus]] [[Category:Numerical analysis]] [[Category:First order methods]] [[ar:تقريب خطي]] [[ca:Aproximació lineal]] [[es:Aproximación lineal]] [[fr:Approximation affine]] [[he:קירוב לינארי]] [[nl:Lineaire benadering]] [[ja:線型近似]] [[pl:Aproksymacja liniowa]] [[pt:Aproximação linear]] [[zh:线性近似]]</text> </page> <page> <id>21723</id> <title>Linear congruential generator</title> <text>A '''Linear Congruential Generator''' ('''LCG''') represents one of the oldest and best-known [[pseudorandom number generator]] [[algorithm]]s.<ref>"[http://demonstrations.wolfram.com/LinearCongruentialGenerators/ Linear Congruential Generators]" by Joe Bolte, [[Wolfram Demonstrations Project]].</ref> The theory behind them is easy to understand, and they are easily implemented and fast. The generator is defined by the [[recurrence relation]]: : <math>X_{n+1} \equiv \left( a X_n + c \right)~~\pmod{m}</math> where <math>X_{n}</math> is 