[[Cannon's algorithm]]: a [[distributed algorithm]] for [[matrix multiplication]] especially suitable for computers laid out in an N × N mesh ** [[Coppersmith–Winograd algorithm]]: square [[matrix multiplication]] ** [[Freivald's algorithm]]: a randomized algorithm used to verify matrix multiplication ** [[Strassen algorithm]]: faster [[matrix multiplication]] * Solving [[system of linear equations|systems of linear equations]] ** [[Biconjugate gradient method]]: solves systems of linear equations ** [[Conjugate gradient]]: an algorithm for the numerical solution of particular systems of linear equations ** [[Gaussian elimination]] ** [[Gauss–Jordan elimination]]: solves systems of linear equations ** [[Gauss–Seidel method]]: solves systems of linear equations iteratively ** [[Levinson recursion]]: solves equation involving a [[Toeplitz matrix]] ** [[Stone's method]]: also known as the strongly implicit procedure or SIP, is an algorithm for solving a sparse linear system of equations ** [[Successive over-relaxation]] (SOR): method used to speed up convergence of the [[Gauss–Seidel method]] ** [[Tridiagonal matrix algorithm]] (Thomas algorithm): solves systems of tridiagonal equations * [[Sparse matrix]] algorithms ** [[Cuthill–McKee algorithm]]: reduce the [[bandwidth (matrix theory)|bandwidth]] of [[Sparse matrix|sparse]] [[symmetric matrices]] ** [[Minimum degree algorithm]]: permute the rows and columns of a symmetric sparse matrix before applying the [[Cholesky decomposition]] ** [[Symbolic Cholesky decomposition]]: Efficient way of storing sparse matrix ==== Monte Carlo ==== {{see|Monte Carlo method}} * [[Gibbs sampling]]: generate a sequence of samples from the joint probability distribution of two or more random variables * [[Metropolis–Hastings algorithm]]: used to generate a sequence of samples from the [[probability distribution]] of one or more variables * [[Wang and Landau algorithm]]: an extension of [[Metropolis–Hastings algorithm]] sampling ==== Root finding ==== {{main|Root-finding algorithm}} * [[False position method]]: approximates roots of a function * [[Newton's method]]: finds zeros of functions with [[calculus]] * [[Secant method]]: approximates roots of a function ===Optimization algorithms=== {{main|Optimization (mathematics)}} * [[Alpha-beta pruning]]: search to reduce number of nodes in minimax algorithm * [[Branch and bound]] * [[Chain matrix multiplication]] * [[Combinatorial optimization]]: optimization problems where the set of feasible solutions is discrete ** [[Greedy randomized adaptive search procedure]] (GRASP): successive constructions of a greedy randomized solution and subsequent iterative improvements of it through a local search ** [[Hungarian method]]: a combinatorial optimization algorithm which solves the [[assignment problem]] in polynomial time * [[Constraint satisfaction]] ** General algorithms for the constraint satisfaction *** [[AC-3 algorithm]] *** [[Difference map algorithm]] *** [[Min conflicts algorithm]] ** [[Chaff algorithm]]: an algorithm for solving instances of the boolean satisfiability problem ** [[Davis–Putnam algorithm]]: check the validity of a first-order logic formula ** [[DPLL algorithm|Davis–Putnam–Logemann–Loveland algorithm]] (DPLL): an algorithm for deciding the satisfiability of propositional logic formula in conjunctive normal form, i.e. for solving the [[CNF-SAT]] problem ** [[Exact cover]] problem *** [[Algorithm X]]: a [[nondeterministic algorithm]] *** [[Dancing Links]]: an efficient implementation of Algorithm X * [[Cross-entropy method]]: a general Monte Carlo approach to combinatorial and continuous multi-extremal optimization and [[importance sampling]] * [[Differential evolution]] * [[Dynamic Programming]]: problems exhibiting the properties of [[overlapping subproblem]]s and [[optimal substructure]] * [[Ellipsoid method]]: is an algorithm for solving convex optimization problems * [[Evolutionary computation]]: optimization inspired by biological mechanisms of evolution ** [[Evolution strategy]] ** [[Genetic algorithms]] *** [[Fitness proportionate selection]] - also known as roulette-wheel selection *** [[Stochastic universal sampling]] *** [[Truncation selection]] *** [[Tournament selection]] ** [[Memetic algorithm]] ** [[Swarm intelligence]] *** [[Ant colony optimization]] *** [[Bees algorithm]]: a search algorithm which mimics the food foraging behavior of swarms of honey bees *** [[Particle swarm optimization|Particle swarm]] * [[Gradient descent]] * [[Harmony search]] (HS): a [[metaheuristic]] algorithm mimicking the improvisation process of musicians * [[Interior point method]] * [[Linear programming]] ** [[Dantzig–Wolfe decomposition]]: an algorithm for solving linear programming problems with special structure ** [[Delayed column generation]] ** [[Integer linear programming]]: solve linear programming problems where some or all the unknowns are restricted to integer values *** [[Branch and cut]] *** [[Cutting-plane method]] ** [[Karmarkar's algorithm]]: The first reasonably efficient algorithm that solves the [[linear programming]] problem in [[polynomial time]]. ** [[Simplex algorithm]]: An algorithm for solving the [[linear programming]] problem * [[Line search]] * [[Local search (optimization)|Local search]]: a metaheuristic for solving computationally hard optimization problems ** [[Random-restart hill climbing]] ** [[Tabu search]] * [[Minimax#Minimax algorithm with alternate moves|Minimax]] used in game programming * [[Nearest neighbor search]] (NNS): find closest points in a [[metric space]] ** [[Best Bin First]]: find an approximate solution to the [[Nearest neighbor search]] problem in very high dimensional spaces * [[Newton's method in optimization]] * [[Nonlinear optimization]] ** [[BFGS method]]: A [[nonlinear optimization]] algorithm ** [[Gauss–Newton algorithm]]: An algorithm for solving nonlinear [[least squares]] problems. ** [[Levenberg–Marquardt algorithm]]: An algorithm for solving nonlinear [[least squares]] problems. ** [[Nelder–Mead method]] (downhill simplex method): A [[nonlinear optimization]] algorithm * [[Odds algorithm]] (Bruss algorithm) : Finds the optimal strategy to predict a last specific event in a random sequence event * [[Simulated annealing]] * [[Stochastic tunneling]] * [[Subset sum problem|Subset sum]] algorithm ==Computational science== {{see|Computational science}} ===Astronomy=== {{main|Astronomical algorithms}} * [[Doomsday algorithm]]: day of the week * [[Zeller's congruence]] is an algorithm to calculate the day of the week for any Julian or Gregorian calendar date * various [[Computus|Easter algorithms]] are used to calculate the day of Easter ===Bioinformatics=== {{see|Bioinformatics}} {{see also|List of algorithms#Sequence alignment|l1=Sequence alignment algorithms}} * [[Basic Local Alignment Search Tool]] also known as BLAST: an algorithm for comparing primary biological sequence information * [[Kabsch algorithm]]: calculate the optimal alignment of two sets of points in order to compute the [[RMSD|root mean squared deviation]] between two protein structures. * [[Velvet (algorithm)|Velvet]]: a set of algorithms manipulating [[de Bruijn graph]]s for genomic [[sequence assembly]] ===Geoscience=== {{see|Geoscience}} * [[Vincenty's formulae]]: a fast algorithm to calculate the distance between two latitude/longitude points on an ellipsoid ===Linguistics=== {{see|Computational linguistics|Natural language processing}} * [[Lesk algorithm]]: word sense disambiguation * [[Stemming|Stemming algorithm]]: a method of reducing words to their stem, base, or root form * [[Sukhotins Algorithm]]: a statistical classification algorithm for classifying characters in a text as vowels or consonants ===Medicine=== {{see|Medical algorithms}} * [[ESC algorithm]] for the diagnosis of heart failure * [[Manning Criteria]] for irritable bowel syndrome * [[Pulmonary_embolism#Algorithms|Pulmonary embolism]] diagnostic 