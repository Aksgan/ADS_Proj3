Architecture: A Quantitative Approach''. 3rd edition, Morgan Kaufmann, p. 43. ISBN 1558607242.</ref> However, power consumption by a chip is given by the equation P = C &times; V<sup>2</sup> &times; F, where P is power, C is the [[capacitance]] being switched per clock cycle (proportional to the number of transistors whose inputs change), V is [[voltage]], and F is the processor frequency (cycles per second).<ref>Rabaey, J. M. (1996). ''Digital Integrated Circuits''. Prentice Hall, p. 235. ISBN 0131786091.</ref> Increases in frequency increase the amount of power used in a processor. Increasing processor power consumption led ultimately to [[Intel]]'s May 2004 cancellation of its [[Tejas and Jayhawk]] processors, which is generally cited as the end of frequency scaling as the dominant computer architecture paradigm.<ref>Flynn, Laurie J. [http://www.nytimes.com/2004/05/08/business/08chip.html?ex=1399348800&en=98cc44ca97b1a562&ei=5007 "Intel Halts Development of 2 New Microprocessors"]. ''The New York Times'', May 8, 2004. Retrieved on April 22, 2008.</ref> [[Moore's Law]] is the empirical observation that transistor density in a microprocessor doubles every 18 to 24 months.<ref name="Moore1965paper">{{cite web| first=Gordon E.|last = Moore|year =1965|url=ftp://download.intel.com/museum/Moores_Law/Articles-Press_Releases/Gordon_Moore_1965_Article.pdf| title =Cramming more components onto integrated circuits| format =PDF| pages =4| work=[[Electronics (magazine)|Electronics Magazine]]| accessdate = 2006-11-11}} </ref> Despite power consumption issues, and repeated predictions of its end, Moore's law is still in effect. With the end of frequency scaling, these additional transistors (which are no longer used for frequency scaling) can be used to add extra hardware for parallel computing. ===Amdahl's law and Gustafson's law=== [[File:AmdahlsLaw.svg|right|thumbnail|300px|A graphical representation of [[Amdahl's law]]. The speed-up of a program from parallelization is limited by how much of the program can be parallelized. For example, if 90% of the program can be parallelized, the theoretical maximum speed-up using parallel computing would be 10x no matter how many processors are used.]] Optimally, the speed-up from parallelization would be linear—doubling the number of processing elements should halve the runtime, and doubling it a second time should again halve the runtime. However, very few parallel algorithms achieve optimal speed-up. Most of them have a near-linear speed-up for small numbers of processing elements, which flattens out into a constant value for large numbers of processing elements. The potential speed-up of an algorithm on a parallel computing platform is given by [[Amdahl's law]], originally formulated by [[Gene Amdahl]] in the 1960s.<ref>Amdahl, G. (April 1967) "The validity of the single processor approach to achieving large-scale computing capabilities". In ''Proceedings of AFIPS Spring Joint Computer Conference'', Atlantic City, N.J., AFIPS Press, pp. 483–85.</ref> It states that a small portion of the program which cannot be parallelized will limit the overall speed-up available from parallelization. Any large mathematical or engineering problem will typically consist of several parallelizable parts and several non-parallelizable (sequential) parts. This relationship is given by the equation: :<math>S = \frac{1}{1 - P}</math> where ''S'' is the speed-up of the program (as a factor of its original sequential runtime), and ''P'' is the fraction that is parallelizable. If the sequential portion of a program is 10% of the runtime, we can get no more than a 10&times; speed-up, regardless of how many processors are added. This puts an upper limit on the usefulness of adding more parallel execution units. "When a task cannot be partitioned because of sequential constraints, the application of more effort has no effect on the schedule. The bearing of a child takes nine months, no matter how many women are assigned."<ref>[[Fred Brooks|Brooks, Frederick P. Jr.]] ''[[The Mythical Man-Month|The Mythical Man-Month: Essays on Software Engineering]]''. Chapter 2 – The Mythical Man Month. ISBN 0201835959</ref> [[Gustafson's law]] is another law in computing, closely related to Amdahl's law. It can be formulated as: [[Image:Optimizing-different-parts.svg|thumb|400px|Assume that a task has two independent parts, A and B. B takes roughly 25% of the time of the whole computation. With effort, a [[programmer]] may be able to make this part five times faster, but this only reduces the time for the whole computation by a little. In contrast, one may need to perform less work to make part A twice as fast. This will make the computation much faster than by optimizing part B, even though B got a greater speed-up (5&times; versus 2&times;).]] :<math> S(P) = P - \alpha(P-1) \, </math> where ''P'' is the number of processors, ''S'' is the speed-up, and ''α'' the non-parallelizable part of the process.<ref>[http://www.scl.ameslab.gov/Publications/Gus/AmdahlsLaw/Amdahls.html Reevaluating Amdahl's Law] (1988). [[Communications of the ACM]] 31(5), pp. 532–33.</ref> Amdahl's law assumes a fixed problem size and that the size of the sequential section is independent of the number of processors, whereas Gustafson's law does not make these assumptions. ===Dependencies=== Understanding [[data dependency|data dependencies]] is fundamental in implementing [[parallel algorithm]]s. No program can run more quickly than the longest chain of dependent calculations (known as the [[Critical path method|critical path]]), since calculations that depend upon prior calculations in the chain must be executed in order. However, most algorithms do not consist of just a long chain of dependent calculations; there are usually opportunities to execute independent calculations in parallel. Let P<sub>i</sub> and P<sub>j</sub> be two program fragments. Bernstein's conditions<ref>Bernstein, A. J. (October 1966). "Program Analysis for Parallel Processing,' IEEE Trans. on Electronic Computers". EC-15, pp. 757–62.</ref> describe when the two are independent and can be executed in parallel. For ''P''<sub>''i''</sub>, let ''I''<sub>''i''</sub> be all of the input variables and ''O''<sub>''i''</sub> the output variables, and likewise for ''P''<sub>''j''</sub>. ''P'' <sub>''i''</sub> and ''P''<sub>''j''</sub> are independent if they satisfy * <math> I_j \cap O_i = \varnothing, \, </math> * <math> I_i \cap O_j = \varnothing, \, </math> * <math> O_i \cap O_j = \varnothing. \, </math> Violation of the first condition introduces a flow dependency, corresponding to the first statement producing a result used by the second statement. The second condition represents an [[anti-dependency]], when the second statement (''P''<sub>''j''</sub>) would overwrite a variable needed by the first expression (''P''<sub>''i''</sub>). The third and final condition represents an output dependency: When two statements write to the same location, the final result must come from the logically last executed statement.<ref>Roosta, Seyed H. (2000). "Parallel processing and parallel algorithms: theory and computation". Springer, p. 114. ISBN 0387987169.</ref> Consider 