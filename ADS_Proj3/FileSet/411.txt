end up much larger, and could in theory run much faster. For instance, a string of instructions carrying out a series of mathematical operations might require only a few loads from memory, while the majority of the numbers being used are either loaded in the instructions themselves, or intermediate values in the registers. But to the casual observer, it was not clear whether or not the RISC concept would improve performance, or even make it worse. The only way to be sure was to actually simulate it, and in test after test, every simulation showed an enormous overall benefit in performance from this design. Where the two projects, RISC and MIPS, differed was in the handling of the registers. MIPS simply added lots of them and left it to the compilers to make use of them. RISC, on the other hand, added circuitry to the CPU to "help" the compiler. RISC used the concept of [[register window]]s, in which the entire "register file" was broken down into blocks, allowing the compiler to "see" one block for global variables, and another for local variables. The idea was to make one particularly common instruction, the [[procedure call]], extremely easy to implement in the compilers. Almost all [[computer language]]s use a system known as a ''activation record'' or ''stack frame'' that contains the address of who called it, the data that was passed in, and any results that need to be returned. In the vast majority of cases these frames are small, typically with three or less inputs and one or no outputs. In the Berkeley design, then, the entire procedure stack would most likely fit entirely within the register window. In this case the call into and return from a procedure is simple and extremely fast. A single instruction is called to set up a new block of registers, operands passed in on the "low end" of the new frame, and then the code jumps into the procedure. On return, the results are placed in the frame at the same end, and the code exits. The register windows are set up to overlap at the ends, meaning that the results from the call simply "appear" in the window of the code that called it, ''with no data having to be copied''. Thus the common procedure call did not have to interact with main memory, greatly speeding it up. On the downside, this approach meant that procedures with large numbers of local variables were problematic, and ones with less led to registers -an expensive resource- being wasted. It was Stanford's work on compilers that led them to ignore the register window concept, believing that a smart compiler could make better use of the registers than a fixed system in hardware. ==RISC I== The first attempt to implement the RISC concept was originally known as '''Gold'''. Work on the design started in 1980 as part of a VLSI design course, but the then-complicated design crashed almost all existing design tools. The team had to spend considerable amounts of time improving or re-writing the tools, and even with these new tools it took just under an hour to extract the design on a [[VAX|VAX 11/780]]. The final design, as '''RISC I''', was published [[Association for Computing Machinery|ACM]] in 1981. It had 44,500 transistors implementing 31 instructions and a register file containing 78 32-bit registers. This allowed for six register windows containing 14 registers each, with an additional 18 globals. The control and instruction decode section occupied only 6% of the die, whereas the typical design of the era used about 50% for the same role. The register file took up most of that space. RISC I also featured a two-stage [[instruction pipeline]] for additional speed, but without the complex instruction re-ordering of more modern designs. This makes conditional branches a problem, because the compiler has to fill the instruction following a conditional branch (the so-called "[[branch delay slot]]"), with something selected to be "safe" (i.e., not dependent on the outcome of the conditional). Sometimes the only suitable instruction in this case is <code>[[NOP]]</code>. A notable number of later RISC-style designs still require the consideration of branch delay. After a month of validation and debugging, the design was sent to the innovative [[MOSIS]] [[Foundry (electronics)|fab]] for production on June 22, 1981, using a 2 μm (2,000 nm) process. A variety of delays forced them to abandon their masks four separate times, and wafers with working examples did not arrive back at Berkeley until May 1982. The first working RISC I "computer" (actually a checkout board) ran on June 11th. In testing, the chips proved to have lesser performance than expected. In general, an instruction would take 2 μs to complete, while the original design allotted for about 400 ns (five times as fast). The precise reasons for this problem were never fully explained. However, throughout testing it was clear that certain instructions did run at the expected speed, suggesting the problem was physical, not logical. Had the design worked at full speed, performance would have been excellent. Simulations using a variety of small programs compared the 4 MHz RISC I to the 5 MHz [[32-bit]] [[VAX|VAX 11/780]] and the 5 MHz [[16-bit]] [[Zilog Z8000]] showed this clearly. Program size was about 30% larger than the VAX but very close to that of the Z8000, validating the argument that the higher [[code density]] of CISC designs was not actually all that impressive in reality. In terms of overall performance, the RISC I was twice as fast as the VAX, and about four times that of the Z8000. More interestingly, the programs ended up performing about the same overall amount of memory access because the large register file dramatically improved the odds the needed operand was already on-chip. It is important to put this performance in context. Even though the RISC design had run slower than the VAX, it made no difference to the importance of the design. RISC allowed for the production of a true 32-bit processor 