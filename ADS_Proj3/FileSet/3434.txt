computer system that can execute the same instruction on large sets of data. "Vector processors have high-level operations that work on linear arrays of numbers or vectors. An example vector operation is ''A'' = ''B'' &times; ''C'', where ''A'', ''B'', and ''C'' are each 64-element vectors of 64-bit [[floating point|floating-point]] numbers."<ref name=PH751>Patterson and Hennessy, p. 751.</ref> They are closely related to Flynn's SIMD classification.<ref name=PH751/> [[Cray]] computers became famous for their vector-processing computers in the 1970s and 1980s. However, vector processors—both as CPUs and as full computer systems—have generally disappeared. Modern [[Instruction set|processor instruction sets]] do include some vector processing instructions, such as with [[AltiVec]] and [[Streaming SIMD Extensions]] (SSE). ==Software== ===Parallel programming languages=== {{main|parallel programming model}} [[:Category:Concurrent programming languages|Concurrent programming languages]], [[Library (computing)|libraries]], [[Application programming interface|APIs]], and [[parallel programming model]]s (such as [[Algorithmic skeleton|Algorithmic Skeletons]]) have been created for programming parallel computers. These can generally be divided into classes based on the assumptions they make about the underlying memory architecture—shared memory, distributed memory, or shared distributed memory. Shared memory programming languages communicate by manipulating shared memory variables. Distributed memory uses [[message passing]]. [[POSIX Threads]] and [[OpenMP]] are two of most widely used shared memory APIs, whereas [[Message Passing Interface]] (MPI) is the most widely used message-passing system API.<ref>The [http://awards.computer.org/ana/award/viewPastRecipients.action?id=16 Sidney Fernbach Award given to MPI inventor Bill Gropp] refers to MPI as the "the dominant HPC communications interface"</ref> One concept used in programming parallel programs is the [[Futures and promises|future concept]], where one part of a program promises to deliver a required datum to another part of a program at some future time. ===Automatic parallelization=== {{main|Automatic parallelization}} Automatic parallelization of a sequential program by a [[compiler]] is the [[holy grail]] of parallel computing. Despite decades of work by compiler researchers, automatic parallelization has had only limited success.<ref>Shen, John Paul and Mikko H. Lipasti (2005). ''Modern Processor Design: Fundamentals of Superscalar Processors''. McGraw-Hill Professional. p. 561. ISBN 0070570647. "However, the holy grail of such research - automated parallelization of serial programs - has yet to materialize. While automated parallelization of certain classes of algorithms has been demonstrated, such success has largely been limited to scientific and numeric applications with predictable flow control (e.g., nested loop structures with statically determined iteration counts) and statically analyzable memory access patterns. (e.g., walks over large multidimensional arrays of float-point data)."</ref> Mainstream parallel programming languages remain either [[Explicit parallelism|explicitly parallel]] or (at best) [[Implicit parallelism|partially implicit]], in which a programmer gives the compiler [[Directive (programming)|directives]] for parallelization. A few fully implicit parallel programming languages exist—[[SISAL]], Parallel [[Haskell (programming language)|Haskell]], and (for [[FPGA]]s) [[Mitrion-C]]. ===Application checkpointing=== {{main|Application checkpointing}} The larger and more complex a computer is, the more that can go wrong and the shorter the [[mean time between failures]]. [[Application checkpointing]] is a technique whereby the computer system takes a "snapshot" of the application—a record of all current resource allocations and variable states, akin to a [[core dump]]; this information can be used to restore the program if the computer should fail. Application checkpointing means that the program has to restart from only its last checkpoint rather than the beginning. For an application that may run for months, that is critical. Application checkpointing may be used to facilitate [[process migration]]. ==Algorithmic methods== As parallel computers become larger and faster, it becomes feasible to solve problems that previously took too long to run. Parallel computing is used in a wide range of fields, from [[bioinformatics]] ([[protein folding]] and [[sequence analysis]]) to economics ([[mathematical finance]]). Common types of problems found in parallel computing applications are:<ref>Asanovic, Krste, et al. (December 18, 2006). [http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.pdf The Landscape of Parallel Computing Research: A View from Berkeley] (PDF). University of California, Berkeley. Technical Report No. UCB/EECS-2006-183. See table on pages 17–19.</ref> <!--Note: do not add to or remove from this list. It is copied from the above source--> * Dense [[linear algebra]] * Sparse linear algebra * Spectral methods (such as [[Cooley–Tukey FFT algorithm|Cooley–Tukey fast Fourier transform]]) * [[n-body simulation|''n''-body problems]] (such as [[Barnes–Hut simulation]]) * [[Regular grid|Structured grid]] problems (such as [[Lattice Boltzmann methods]]) * [[Unstructured grid]] problems (such as found in [[finite element analysis]]) * [[Monte Carlo method|Monte Carlo simulation]] * [[Combinational logic]] (such as [[Brute force attack|brute-force cryptographic techniques]]) * [[Graph traversal]] (such as [[sorting algorithm]]s) * [[Dynamic programming]] * [[Branch and bound]] methods * [[Graphical model]]s (such as detecting [[hidden Markov model]]s and constructing [[Bayesian network]]s) * [[Finite-state machine]] simulation ==History== {{main|History of computing}} [[Image:ILLIAC 4 parallel computer.jpg|right|thumbnail|[[ILLIAC IV]], "perhaps the most infamous of Supercomputers"]] The origins of true (MIMD) parallelism go back to [[Federico Luigi, Conte Menabrea]] and his "Sketch of the [[Analytic Engine]] Invented by [[Charles Babbage]]".<ref>[[Federico Luigi, Conte Menabrea|Menabrea, L. F.]] (1842). [http://www.fourmilab.ch/babbage/sketch.html Sketch of the Analytic Engine Invented by Charles Babbage]. Bibliothèque Universelle de Genève. Retrieved on November 7, 2007.</ref><ref name=PH753>Patterson and Hennessy, p. 753.</ref> [[IBM]] introduced the [[IBM 704|704]] in 1954, through a project in which [[Gene Amdahl]] was one of the principal architects. It became the first commercially available computer to use fully automatic [[floating point]] arithmetic commands.<ref>{{cite web | url = http://www.columbia.edu/acis/history/704.html | title = Columbia University Computing History: The IBM 704 | accessdate = 2008-01-08 | year = 2003 | author = da Cruz, Frank | publisher = Columbia University}}</ref> In April 1958, S. Gill (Ferranti) discussed parallel programming and the need for branching and waiting.<ref>Parallel Programming, S. Gill, The Computer Journal Vol. 1 #1, pp2-10, British Computer Society, April 1958.</ref> Also in 1958, IBM researchers [[John Cocke]] and [[Daniel Slotnick]] discussed the use of parallelism in numerical calculations for the first time.<ref name=G_Wilson>{{cite web | url = http://ei.cs.vt.edu/~history/Parallel.html | title = The History of the Development of Parallel Computing | accessdate = 2008-01-08 | first = Gregory V | last = Wilson | year = 1994|publisher=Virginia Tech/Norfolk State University, Interactive Learning with a Digital Library in Computer Science}}</ref> [[Burroughs Corporation]] introduced the D825 in 1962, a four-processor computer that accessed up to 16 memory modules through a [[crossbar switch]].<ref>{{cite web | url = http://www.computerworld.com/action/article.do?command=viewArticleBasic&articleId=65878 | title = The Power 