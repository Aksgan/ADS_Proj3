url=http://xml.sys-con.com/read/43755.htm | accessdate=May 2, 2005 }} * {{cite web | title=Nuts And Bolts Of Transaction Processing | work=Article about Transaction Management | url=http://www.subbu.org/articles/transactions/NutsAndBoltsOfTP.html | accessdate=May 3, 2005 }} ==Further reading== * Gerhard Weikum, Gottfried Vossen, ''Transactional information systems: theory, algorithms, and the practice of concurrency control and recovery'', Morgan Kaufmann, 2002, ISBN 1558605088 {{DEFAULTSORT:Distributed Transaction}} [[Category:Data management]] [[Category:Transaction processing]] [[ko:분산 트랜잭션]] [[ja:分散トランザクション]]</text> </page> <page> <id>10821</id> <title>Distributive homomorphism</title> <text>A [[Congruence relation|congruence]] &theta; of a [[Semilattice|join-semilattice]] ''S'' is ''monomial'', if the &theta;-[[equivalence class]] of any element of ''S'' has a largest element. We say that &theta; is ''distributive'', if it is a [[Join (mathematics)|join]], in the [[Compact element|congruence lattice]] Con ''S'' of ''S'', of monomial join-congruences of ''S''. The following definition originates in Schmidt's 1968 work and was subsequently adjusted by Wehrung.<br /><br /> '''Definition (weakly distributive homomorphisms).''' A homomorphism ''&mu; : S → T'' between join-semilattices ''S'' and ''T'' is ''weakly distributive'', if for all ''a, b'' in ''S'' and all ''c'' in ''T'' such that ''&mu;(c)&le; a &or; b'', there are elements ''x'' and ''y'' of ''S'' such that ''c&le; x &or; y'', ''&mu;(x)&le; a'', and ''&mu;(y)&le; b''.<br /><br /> '''Examples:'''<br /> (1) For an [[Universal algebra|algebra]] ''B'' and a ''reduct'' ''A'' of ''B'' (that is, an algebra with same underlying set as ''B'' but whose set of operations is a subset of the one of ''B''), the canonical (&or;, 0)-homomorphism from Con<sub>c</sub> A to Con<sub>c</sub> B is weakly distributive. Here, Con<sub>c</sub> A denotes the (&or;, 0)-semilattice of all [[Compact element|compact congruences]] of ''A''. (2) For a [[Lattice (order)|convex sublattice]] ''K'' of a lattice ''L'', the canonical (&or;, 0)-homomorphism from Con<sub>c</sub> ''K'' to Con<sub>c</sub> ''L'' is weakly distributive. == References == E.T. Schmidt, ''Zur Charakterisierung der Kongruenzverb&auml;nde der Verb&auml;nde'', Mat. Casopis Sloven. Akad. Vied. '''18''' (1968), 3--20. F. Wehrung, ''A uniform refinement property for congruence lattices'', Proc. Amer. Math. Soc. '''127''', no. 2 (1999), 363--370. F. Wehrung, ''A solution to Dilworth's congruence lattice problem'', preprint 2006. [[Category:algebra]]</text> </page> <page> <id>10833</id> <title>Divide-and-conquer eigenvalue algorithm</title> <text>'''Divide-and-conquer eigenvalue algorithms''' are a class of [[eigenvalue algorithm]]s for [[Hermitian matrix|Hermitian]] or [[real number|real]] [[Symmetric matrix|symmetric matrices]] that have recently (circa 1990s) become competitive in terms of [[Numerical stability|stability]] and [[Computational complexity theory|efficiency]] with more traditional algorithms such as the [[QR algorithm]]. The basic concept behind these algorithms is the [[Divide and conquer algorithm|divide-and-conquer]] approach from [[computer science]]. An [[eigenvalue]] problem is divided into two problems of roughly half the size, each of these are solved [[Recursion|recursively]], and the eigenvalues of the original problem are computed from the results of these smaller problems. Here we present the simplest version of a divide-and-conquer algorithm, similar to the one originally proposed by Cuppen in 1981. Many details that lie outside the scope of this article will be omitted; however, without considering these details, the algorithm is not fully stable. ==Background== As with most eigenvalue algorithms for Hermitian matrices, divide-and-conquer begins with a reduction to [[Tridiagonal matrix|tridiagonal]] form. For an <math>m \times m</math> matrix, the standard method for this, via [[Householder reflection]]s, takes <math>\frac{4}{3}m^{3}</math> [[flops]], or <math>\frac{8}{3}m^{3}</math> if [[eigenvector]]s are needed as well. There are other algorithms, such as the [[Arnoldi iteration]], which may do better for certain classes of matrices; we will not consider this further here. In certain cases, it is possible to ''deflate'' an eigenvalue problem into smaller problems. Consider a [[block diagonal matrix]] :<math>T = \begin{bmatrix} T_{1} & 0 \\ 0 & T_{2}\end{bmatrix}.</math> The eigenvalues and eigenvectors of <math>T</math> are simply those of <math>T_{1}</math> and <math>T_{2}</math>, and it will almost always be faster to solve these two smaller problems than to solve the original problem all at once. This technique can be used to improve the efficiency of many eigenvalue algorithms, but it has special significance to divide-and-conquer. For the rest of this article, we will assume the input to the divide-and-conquer algorithm is an <math>m \times m</math> real symmetric tridiagonal matrix <math>T</math>. Although the algorithm can be modified for Hermitian matrices, we do not give the details here. ==Divide== The ''divide'' part of the divide-and-conquer algorithm comes from the realization that a tridiagonal matrix is "almost" block diagonal. <!-- For original TeX, see image description page --> :[[Image:Almost block diagonal.png]] The size of submatrix <math>T_{1}</math> we will call <math>n \times n</math>, and then <math>T_{2}</math> is <math>(m - n) \times (m - n)</math>. Note that the remark about <math>T</math> being almost block diagonal is true regardless of how <math>n</math> is chosen (i.e., there are many ways to so decompose the matrix). However, it makes sense, from an efficiency standpoint, to choose <math>n \approx m/2</math>. We write <math>T</math> as a block diagonal matrix, plus a [[Rank (linear algebra)|rank-1]] correction: <!-- For original TeX, see image description page --> :[[Image:Block diagonal plus correction.png]] The only difference between <math>T_{1}</math> and <math>\hat{T}_{1}</math> is that the lower right entry <math>t_{nn}</math> in <math>\hat{T}_{1}</math> has been replaced with <math>t_{nn} - \beta</math> and similarly, in <math>\hat{T}_{2}</math> the top left entry <math>t_{n+1,n+1}</math> has been replaced with <math>t_{n+1,n+1} - \beta</math>. The remainder of the divide step is to solve for the eigenvalues (and if desired the eigenvectors) of <math>\hat{T}_{1}</math> and <math>\hat{T}_{2}</math>, that is to find the [[diagonalizable matrix|diagonalization]]s <math>\hat{T}_{1} = Q_{1} D_{1} Q_{1}^{T}</math> and <math>\hat{T}_{2} = Q_{2} D_{2} Q_{2}^{T}</math>. This can be accomplished with recursive calls to the divide-and-conquer algorithm, although practical implementations often switch to the QR algorithm for small enough submatrices. ==Conquer== The ''conquer'' part of the algorithm is the unintuitive part. Given the diagonalizations of the submatrices, calculated above, how do we find the diagonalization of the original matrix? First, define <math>z^{T} = (q_{1}^{T},q_{2}^{T})</math>, where <math>q_{1}^{T}</math> is the last row of <math>Q_{1}</math> and <math>q_{2}^{T}</math> is the first row of <math>Q_{2}</math>. It is now elementary to show that :<math>T = \begin{bmatrix} Q_{1} & \\ & Q_{2} \end{bmatrix} \left( \begin{bmatrix} D_{1} & \\ & D_{2} \end{bmatrix} + \beta z z^{T} \right) \begin{bmatrix} Q_{1}^{T} & \\ & Q_{2}^{T} \end{bmatrix}</math> The remaining task has been reduced to finding the eigenvalues of a diagonal matrix plus a rank-one correction. Before showing how to do this, let us 