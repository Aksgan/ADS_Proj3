[[Modal realism]], [[Modal logic]]). ==See also== * [[Deixis]] * [[Semeiotic]] * [[Semiotics]] * [[Charles Sanders Peirce#Classes of signs]] * [[Semiotic elements and classes of signs (Peirce)]] * [[Yehoshua Bar-Hillel]] * [[Speech act theory]] ==References== <references /> ==External links== *[http://arche-wiki.st-and.ac.uk/~ahwiki/bin/view/Arche/IndexicalsBib Arché Bibliography of Indexicals] *[http://www.helsinki.fi/science/commens/dictionary.html Commens Dictionary of Peirce's Terms], consisting in Peirce's own definitions and characterizations. See "Index". [[Category:Semantics]] [[ca:Indexicalitat]] [[fr:Indexicalité]] [[is:Ábendingarorð]] [[fi:Indeksikaalisuus]] [[sv:Indexikalitet]]</text> </page> <page> <id>18167</id> <title>Indirect branch</title> <text>{{Unreferenced stub|auto=yes|date=December 2009}} An '''indirect branch''' (also known as a '''computed jump''', '''indirect jump''' and '''register-indirect''' jump) is a type of [[control flow|program control instruction]] present in some [[machine language]] [[instruction set]]s. Rather than specifying the [[memory address|address]] of the next [[instruction (computer science)|instruction]] to [[execution (computers)|execute]], as in a direct [[Branch (computer science)|branch]], the [[parameter (computer science)|argument]] specifies where the address is located. Thus an example could be to 'jump indirect on the r1 [[processor register|register]]', which would mean that the next instruction to be executed would be at the address whose value is in register r1. The address to be jumped to is not known until the instruction is executed. Indirect branches can also depend on the value of a [[memory (computers)|memory location]]. An indirect branch can be useful to make a [[conditional branch]], especially a [[multiway branch]]. For instance, based on program [[Information|input]], a value could be looked up in a [[jump table]] of pointers to [[machine code|code]] for handling the various cases implied by the data value. The [[data#Uses of data in computing|data]] value could be added to the address of the table, with the result stored in a register. An indirect jump could then be made based on the value of that register, efficiently dispatching program control to the code appropriate to the input. In a similar manner, [[subroutine]] call instructions can be indirect, with the address of the subroutine to be called specified in a register or memory location. ==Examples== <code> [[Z80]]: jp (hl)<br> [[SPARC]]: jmpl %o7<br> [[MIPS architecture|MIPS]]: jmpl %ra<br> [[X86]]: jmp *%eax<br> [[ARM architecture|ARM]]: mov pc, r2<br> [[IA64]]: br.ret.sptk.few rp<br> [[6502]]: jmp ($0DEA) </code> {{DEFAULTSORT:Indirect Branch}} [[Category:Control flow]] [[Category:Machine code]] {{Compu-prog-stub}} [[ru:Косвенный переход]]</text> </page> <page> <id>18176</id> <title>Inductive bias</title> <text>The '''inductive bias''' of a learning [[algorithm]] is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered (Mitchell, 1980). In [[machine learning]], one aims to construct algorithms that are able to ''learn'' to predict a certain target output. To achieve this, the learning algorithm is presented some training examples that demonstrate the intended relation of input and output values. Then the learner is supposed to approximate the correct output, even for examples that have not been shown during training. Without any additional assumptions, this task cannot be solved exactly since unseen situations might have an arbitrary output value. The kind of necessary assumptions about the nature of the target function are subsumed in the term ''inductive bias'' (Mitchell, 1980; desJardins and Gordon, 1995). A classical example of an inductive bias is [[Occam's Razor]], assuming that the simplest consistent hypothesis about the target function is actually the best. Here ''consistent'' means that the hypothesis of the learner yields correct outputs for all of the examples that have been given to the algorithm. Approaches to a more formal definition of inductive bias are based on mathematical [[logic]]. Here, the inductive bias is a logical formula that, together with the training data, logically entails the hypothesis generated by the learner. Unfortunately, this strict formalism fails in many practical cases, where the inductive bias can only be given as a rough description (e.g. in the case of [[neural networks]]), or not at all. ==Types of inductive biases== The following is a list of common inductive biases in machine learning algorithms. * '''Maximum [[conditional independence]]''': if the hypothesis can be cast in a [[Bayesian inference|Bayesian]] framework, try to maximize conditional independence. This is the bias used in the [[Naive Bayes classifier]]. * '''Minimum [[Cross-validation (statistics)|cross-validation]] error''': when trying to choose among hypotheses, select the hypothesis with the lowest cross-validation error. Although cross-validation may seem to be free of bias, the [[No free lunch in search and optimization|No Free Lunch]] theorems show that cross-validation must be biased. * '''Maximum margin''': when drawing a boundary between two classes, attempt to maximize the width of the boundary. This is the bias used in [[Support Vector Machines]]. The assumption is that distinct classes tend to be separated by wide boundaries. * '''[[Minimum description length]]''': when forming a hypothesis, attempt to minimize the length of the description of the hypothesis. The assumption is that simpler hypotheses are more likely to be true. See [[Occam's razor]]. * '''Minimum features''': unless there is good evidence that a [[feature space|feature]] is useful, it should be deleted. This is the assumption behind [[feature selection]] algorithms. * '''Nearest neighbors''': assume that most of the cases in a small neighborhood in [[feature space]] belong to the same class. Given a case for which the class is unknown, guess that it belongs to the same class as the majority in its immediate neighborhood. This is the bias used in the [[k-nearest neighbor algorithm]]. The assumption is that cases that are near each other tend to belong to the same class. ==Shift of bias== Although most learning algorithms have a static bias, some algorithms are designed to shift their bias as they acquire more data (Utgoff, 1984). This does not avoid bias, since the bias shifting process itself must have a bias. ==See also== * [[Bias]] * [[Cognitive bias]] * [[No free lunch in search and optimization]] ==References== desJardins, M., and Gordon, D.F. (1995). [http://citeseer.ist.psu.edu/article/desjardins95evaluation.html Evaluation and selection of biases in machine learning]. Machine Learning Journal, 5:1--17, 1995. Mitchell, T.M. (1980). [http://citeseer.ist.psu.edu/mitchell80need.html The need for biases in learning generalizations]. CBM-TR 5-110, Rutgers University, New Brunswick, NJ. Utgoff, P.E. (1984). Shift of bias for inductive concept learning. Doctoral dissertation, Department of Computer Science, Rutgers University, New Brunswick, NJ. {{Comp-sci-stub}} [[Category:Machine learning]] [[Category:Bias]] [[de:Induktiver Bias]] [[it:Bias induttivo]] [[vi:Thiên kiến quy nạp]] [[zh:歸納偏向]]</text> </page> <page> <id>18180</id> <title>Industrial-grade 