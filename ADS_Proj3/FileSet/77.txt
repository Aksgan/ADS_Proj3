an element to insert, num(T) the number of elements in T, and size(T) the allocated size of T. We assume the existence of operations create_table(n), which creates an empty table of size n, for now assumed to be free, and elementary_insert(T,E), which inserts element E into a table T that already has space allocated, with a cost of 1. The following [[pseudocode]] illustrates the table insertion procedure: '''function''' table_insert(T,E) '''if''' num(T) = size(T) U := create_table(2 &times; size(T)) '''for each''' F '''in''' T elementary_insert(U,F) T := U elementary_insert(T,E) Without amortized analysis, the best bound we can show for n insert operations is O(n<sup>2</sup>) &mdash; this is due to the loop at line 4 that performs num(T) elementary insertions. For analysis using the accounting method, we assign a payment of 3 to each table insertion. Although the reason for this is not clear now, it will become clear during the course of the analysis. Assume that initially the table is empty with size(T) = m. The first m insertions therefore do not require reallocation and only have cost 1 (for the elementary insert). Therefore, when num(T) = m, the pool has (3 - 1)&times;m = 2m. Inserting element m + 1 requires reallocation of the table. Creating the new table on line 3 is free (for now). The loop on line 4 requires m elementary insertions, for a cost of m. Including the insertion on the last line, the total cost for this operation is m + 1. After this operation, the pool therefore has 2m + 3 - (m + 1) = m + 2. Next, we add another m - 1 elements to the table. At this point the pool has m + 2 + 2&times;(m - 1) = 3m. Inserting an additional element (that is, element 2m + 1) can be seen to have cost 2m + 1 and a payment of 3. After this operation, the pool has 3m + 3 - (2m + 1) = m + 2. Note that this is the same amount as after inserting element m + 1. In fact, we can show that this will be the case for any number of reallocations. It can now be made clear why the payment for an insertion is 3. 1 goes to inserting the element the first time it is added to the table, 1 goes to moving it the next time the table is expanded, and 1 goes to moving one of the elements that was already in the table the next time the table is expanded. We initially assumed that creating a table was free. In reality, creating a table of size n may be as expensive as O(n). Let us say that the cost of creating a table of size n is n. Does this new cost present a difficulty? Not really; it turns out we use the same method to show the amortized O(1) bounds. All we have to do is change the payment. When a new table is created, there is an old table with m entries. The new table will be of size 2m. As long as the entries currently in the table have added enough to the pool to pay for creating the new table, we will be all right. We cannot expect the first <math>\frac{m}{2}</math> entries to help pay for the new table. Those entries already paid for the current table. We must then rely on the last <math>\frac{m}{2}</math> entries to pay the cost <math>2m</math>. This means we must add <math>\frac{2m}{m/2} = 4</math> to the payment for each entry, for a total payment of 3 + 4 = 7. == References == * [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 17.2: The accounting method, pp. 410&ndash;412. [[Category:Analysis of algorithms]] [[de:Account-Methode]]</text> </page> <page> <id>808</id> <title>Accuracy paradox</title> <text>{{Refimprove|date=December 2009}} The '''accuracy paradox''' for [[predictive analytics]] states that predictive models with a given level of [[accuracy]] may have greater [[predictive power]] than models with higher accuracy. It may be better to avoid the accuracy metric in favor of other metrics such as [[Accuracy and precision|precision]] and [[Recall (information retrieval)|recall]]. Accuracy is often the starting point for analyzing the quality of a predictive model, as well as an obvious criterion for prediction. Accuracy measures the ratio of correct predictions to the total number of cases evaluated. It may seem obvious that the ratio of correct predictions to cases should be a key metric. A predictive model may have high accuracy, but be useless. In an example predictive model for an [[insurance fraud]] application, all cases that are predicted as high-risk by the model will be investigated. To evaluate the performance of the model, the insurance company has created a sample data set of 10,000 claims. All 10,000 cases in the [[validation]] sample have been carefully checked and it is known which cases are fraudulent. To analyze the quality of the model, the insurance uses the [[table of confusion]]. The definition of accuracy, the table of confusion for model M<sub>1</sub><sup>Fraud</sup>, and the calculation of accuracy for model M<sub>1</sub><sup>Fraud</sup> is shown below. <math>\mathrm{A}(M) = \frac{TN + TP}{TN + FP + FN + TP}</math> where : TN is the number of true negative cases : FP is the number of false positive cases : FN is the number of false negative cases : TP is the number of true positive cases ''Formula 1: Definition of Accuracy'' {|class="prettytable" ! !Predicted Negative !Predicted Positive |- |Negative Cases||9,700||150 |- |Positive Cases||50||100 |} ''Table 1: Table of Confusion for Fraud Model M<sub>1</sub><sup>Fraud</sup>.'' <math>\mathrm A (M) = \frac{9,700 + 100}{9,700 + 150 + 50 + 100} = 98.0%</math> ''Formula 2: Accuracy for model M<sub>1</sub><sup>Fraud</sup>'' With an accuracy of 98.0% model M<sub>1</sub><sup>Fraud</sup> appears to perform fairly well. The paradox lies in the fact that accuracy can be easily improved to 98.5% by always predicting "no fraud". The table of confusion and the accuracy for this trivial “always predict negative” model 