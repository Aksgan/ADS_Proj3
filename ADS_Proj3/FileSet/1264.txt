modern meaning, referring specifically to nesting smaller decision problems inside larger decisions,<ref>http://www.wu-wien.ac.at/usr/h99c/h9951826/bellman_dynprog.pdf</ref> and the field was thereafter recognized by the [[IEEE]] as a [[systems analysis]] and [[engineering]] topic. Bellman's contribution is remembered in the name of the [[Bellman equation]], a central result of dynamic programming which restates an optimization problem in [[Recursion (computer science)|recursive]] form. The word ''dynamic'' was chosen by Bellman because it sounded impressive, not because it described how the method worked.<ref name="Eddy">Eddy, S. R., What is dynamic programming?, Nature Biotechnology, 22, 909-910 (2004).</ref> The word ''programming'' referred to the use of the method to find an optimal ''program'', in the sense of a military schedule for training or logistics. This usage is the same as that in the phrases ''[[linear programming]]'' and ''mathematical programming'', a synonym for [[optimization (mathematics)|optimization]].<ref>Nocedal, J.; Wright, S. J.: Numerical Optimization, page 9, Springer, 2006..</ref> ==Overview== [[Image:Shortest path optimal substructure.png|200px|left|thumb|'''Figure 1.''' Finding the shortest path in a graph using optimal substructure; a straight line indicates a single edge; a wavy line indicates a shortest path between the two vertices it connects (other nodes on these paths are not shown); the bold line is the overall shortest path from start to goal.]] Dynamic programming is both a mathematical optimization method and a computer programming method. In both contexts it refers to simplifying a complicated problem by breaking it down into simpler subproblems in a [[Recursion|recursive]] manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively; Bellman called this the "[[Principle of optimality|Principle of Optimality]]". Likewise, in computer science, a problem which can be broken down recursively is said to have [[optimal substructure]]. If subproblems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the subproblems.<ref>Cormen, T. H.; Leiserson, C. E.; Rivest, R. L.; Stein, C. (2001), Introduction to Algorithms (2nd ed.), MIT Press & McGraw-Hill, ISBN 0-262-03293-7 . pp. 327â€“8.</ref> In the optimization literature this relationship is called the [[Bellman equation]]. ===Dynamic programming in mathematical optimization=== In terms of mathematical [[Optimization (mathematics)|optimization]], dynamic programming usually refers to simplifying a decision by breaking it down into a sequence of decision steps over time. This is done by defining a sequence of '''value functions''' V<sub>1</sub> , V<sub>2</sub> , ... V<sub>n</sub> , with an argument ''y'' representing the '''[[State variable|state]]''' of the system at times ''i'' from 1 to ''n''. The definition of V<sub>n</sub>(y) is the value obtained in state ''y'' at the last time ''n''. The values V<sub>i</sub> at earlier times ''i=n-1,n-2,...,2,1'' can be found by working backwards, using a [[Recursion|recursive]] relationship called the [[Bellman equation]]. For i=2,...n, V<sub>i -1</sub> at any state y is calculated from V<sub>i</sub> by maximizing a simple function (usually the sum) of the gain from decision ''i-1'' and the function V<sub>i</sub> at the new state of the system if this decision is made. Since V<sub>i</sub> has already been calculated for the needed states, the above operation yields V<sub>i -1</sub> for those states. Finally, V<sub>1</sub> at the initial state of the system is the value of the optimal solution. The optimal values of the decision variables can be recovered, one by one, by tracking back the calculations already performed. ===Dynamic programming in computer programming=== There are two key attributes that a problem must have in order for dynamic programming to be applicable: [[optimal substructure]] and [[overlapping subproblem]]s which are only slightly smaller. When the overlapping problems are, say, half the size of the original problem the strategy is called "[[Divide and conquer algorithm|divide and conquer]]" rather than "dynamic programming". This is why [[mergesort]], [[quicksort]], and finding all matches of a [[regular expression]] are not classified as dynamic programming problems. ''Optimal substructure'' means that the solution to a given optimization problem can be obtained by the combination of optimal solutions to its subproblems. Consequently, the first step towards devising a dynamic programming solution is to check whether the problem exhibits such optimal substructure. Such optimal substructures are usually described by means of [[recursion]]. For example, given a graph ''G=(V,E)'', the shortest path ''p'' from a vertex ''u'' to a vertex ''v'' exhibits optimal substructure: take any intermediate vertex ''w'' on this shortest path ''p''. If ''p'' is truly the shortest path, then the path ''p<sub>1</sub>'' from ''u'' to ''w'' and ''p<sub>2</sub>'' from ''w'' to ''v'' are indeed the shortest paths between the corresponding vertices (by the simple cut-and-paste argument described in [[CLRS]]). Hence, one can easily formulate the solution for finding shortest paths in a recursive manner, which is what the [[Bellman-Ford algorithm]] does. ''Overlapping'' subproblems means that the space of subproblems must be small, that is, any recursive algorithm solving the problem should solve the same subproblems over and over, rather than generating new subproblems. For example, consider the recursive formulation for generating the Fibonacci series: F<sub>i</sub> = F<sub>i-1</sub> + F<sub>i-2</sub>, with base case F<sub>1</sub>=F<sub>2</sub>=1. Then F<sub>43</sub> = F<sub>42</sub> + F<sub>41</sub>, and F<sub>42</sub> = F<sub>41</sub> + F<sub>40</sub>. Now F<sub>41</sub> is being solved in the recursive subtrees of both F<sub>43</sub> as well as F<sub>42</sub>. Even though the total number of subproblems is actually small (only 43 of them), we end up solving the same problems over and over if we adopt a naive recursive solution such as this. Dynamic programming takes account of this fact and solves each subproblem only once. Note that the subproblems must be only ''slightly'' smaller (typically taken to mean a constant additive factor{{Citation needed|date=June 2009}}) than the larger problem; when they are a multiplicative factor smaller the problem is no longer classified as dynamic programming. [[Image:Fibonacci dynamic programming.svg|thumb|108px|'''Figure 2.''' The subproblem graph for the Fibonacci sequence. The fact that it is not a [[tree structure|tree]] indicates overlapping subproblems.]] This can be achieved in either of two ways:{{Citation needed|date=May 2009}} * ''[[top-down|Top-down approach]]'': This is the direct fall-out of the recursive formulation of any problem. If the solution to any problem can be formulated recursively 