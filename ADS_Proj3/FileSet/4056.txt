is unitary. Define :<math> \Sigma = \begin{bmatrix} \begin{bmatrix} D^{1/2} & 0 \\ 0 & 0\end{bmatrix} \\ 0 \end{bmatrix} </math> where extra zero rows are added '''or removed''' to make the number of zero rows equal the number of columns of ''U''<sub>2</sub>. Then :<math> \begin{bmatrix} U_1 & U_2 \end{bmatrix} \begin{bmatrix} \begin{bmatrix} D^{1/2} & 0 \\ 0 & 0\end{bmatrix} \\ 0 \end{bmatrix} \begin{bmatrix} V_1 & V_2 \end{bmatrix}^* = \begin{bmatrix} U_1 & U_2 \end{bmatrix} \begin{bmatrix} D^{1/2} V_1^* \\ 0 \end{bmatrix} = U_1 D^{1/2} V_1^* = M , </math> which is the desired result: :<math> M = U \Sigma V^* .\!</math> Notice the argument could begin with diagonalizing ''MM*'' rather than ''M*M'' (This shows directly that ''MM*'' and ''M*M'' have the same non-zero eigenvalues). === Based on variational characterization === The singular values can also be characterized as the maxima of ''u''<sup>T</sup>''Mv'', considered as a function of ''u'' and ''v'', over particular subspaces. The singular vectors are the values of ''u'' and ''v'' where these maxima are attained. Let ''M'' denote an ''m'' &times; ''n'' matrix with real entries. Let <math>S^{m-1}</math> and <math>S^{n-1}</math> denote the sets of unit 2-norm vectors in '''R'''<sup>''m''</sup> and '''R'''<sup>''n''</sup> respectively. Define the function :<math> \sigma(u,v) = u^{T} M v \, </math> for vectors ''u'' ∈ <math>S^{m-1}</math> and ''v'' ∈ <math>S^{n-1}</math>. Consider the function ''σ'' restricted to <math>S^{m-1}</math> &times; <math>S^{n-1}</math>. Since both <math>S^{m-1}</math> and <math>S^{n-1}</math> are [[Compact space|compact]] sets, their [[Product topology|product]] is also compact. Furthermore, since ''σ'' is continuous, it attains a largest value for at least one pair of vectors ''u'' ∈ <math>S^{m-1}</math> and ''v'' ∈ <math>S^{n-1}</math>. This largest value is denoted ''σ''<sub>1</sub> and the corresponding vectors are denoted ''u''<sub>1</sub> and ''v''<sub>1</sub>. Since <math>\sigma_{1}</math> is the largest value of <math>\sigma(u,v)</math> it must be non-negative. If it were negative, changing the sign of either ''u''<sub>1</sub> or ''v''<sub>1</sub> would make it positive and therefore larger. '''Statement:''' ''u''<sub>1</sub>, ''v''<sub>1</sub> are left and right singular vectors of ''M'' with corresponding singular value ''σ''<sub>1</sub>. '''Proof:''' Similar to the eigenvalues case, by assumption the two vectors satisfy the Lagrange multiplier equation: :<math>\nabla \sigma = \nabla \; u^T M v = \lambda_1 \cdot \nabla \; u^T u + \lambda_2 \cdot \nabla \; v^T v.</math> After some algebra, this becomes :<math> M v_{1} = 2 \lambda_{1} u_{1} + 0, \,</math> and :<math> M^{T} u_{1} = 0 + 2 \lambda_{2} v_{1}. \,</math> Multiplying the first equation from left by <math>u_{1}^{T}</math> and the second equation from left by <math>v_{1}^{T}</math> and taking ||''u''|| = ||''v''|| = 1 into account gives :<math> u_{1}^{T} M v_{1} = \sigma_{1} = 2 \lambda_{1}, </math> :<math> v_{1}^{T} M^{T} u_{1} = \sigma_{1} = 2 \lambda_{2}. </math> So ''σ''<sub>1</sub> = 2 ''λ''<sub>1</sub> = 2 ''λ''<sub>2</sub>. By properties of the functional ''φ'' defined by :<math>\phi(w) = u_1 ^T w, \,</math> we have :<math> M v_{1} = \sigma_{1} u_{1}. \,</math> Similarly, :<math> M^{T} u_{1} = \sigma_{1} v_{1}. \,</math> This proves the statement. More singular vectors and singular values can be found by maximizing ''σ''(''u'', ''v'') over normalized ''u'', ''v'' which are orthogonal to ''u''<sub>1</sub> and ''v''<sub>1</sub>, respectively. The passage from real to complex is similar to the eigenvalue case. == Geometric meaning == Because ''U'' and ''V'' are unitary, we know that the columns ''u''<sub>1</sub>,...,''u<sub>m</sub>'' of ''U'' yield an [[orthonormal basis]] of ''K''<sup>''m''</sup> and the columns ''v''<sub>1</sub>,...,''v<sub>n</sub>'' of ''V'' yield an orthonormal basis of ''K''<sup>''n''</sup> (with respect to the standard [[scalar product]]s on these spaces). The [[linear transformation]] ''T'' :''K''<sup>''n''</sup> → ''K''<sup>''m''</sup> that takes a vector ''x'' to ''Mx'' has a particularly simple description with respect to these orthonormal bases: we have ''T''(''v<sub>i</sub>'') = ''σ<sub>i</sub> u<sub>i</sub>'', for ''i'' = 1,...,min(''m'',''n''), where ''σ<sub>i</sub>'' is the ''i''-th diagonal entry of Σ, and ''T''(''v''<sub>''i''</sub>) = 0 for ''i'' > min(''m'',''n''). The geometric content of the SVD theorem can thus be summarized as follows: for every linear map ''T'' :''K''<sup>''n''</sup> → ''K''<sup>''m''</sup> one can find orthonormal bases of ''K''<sup>''n''</sup> and ''K''<sup>''m''</sup> such that ''T'' maps the ''i''-th basis vector of ''K''<sup>''n''</sup> to a non-negative multiple of the ''i''-th basis vector of ''K''<sup>''m''</sup>, and sends the left-over basis vectors to zero. With respect to these bases, the map ''T'' is therefore represented by a diagonal matrix with non-negative real diagonal entries. To get a more visual flavour of singular values and SVD decomposition —at least when working on real vector spaces— consider the sphere ''S'' of radius one in '''R'''<sup>''n''</sup>. The linear map ''T'' maps this sphere onto an [[ellipsoid]] in '''R'''<sup>''m''</sup>. Non-zero singular values are simply the lengths of the [[Semi-minor axis|semi-axes]] of this ellipsoid. Especially when ''n''=''m'', and all the singular values are distinct and non-zero, the SVD of the linear map ''T'' can be easily analysed as a succession of three consecutive moves : consider the ellipsoid ''T''(''S'') and specifically its axes ; then consider the directions in '''R'''<sup>''n''</sup> sent by ''T'' onto these axes. These directions happen to be mutually orthogonal. Apply first an isometry ''v*'' sending these directions to the coordinate axes of '''R'''<sup>''n''</sup>. On a second move, apply an [[endomorphism]] ''d'' diagonalized along the coordinate axes and stretching or shrinking in each direction, using the semi-axes lengths of ''T''(''S'') as stretching coefficients. The composition ''d''<small> o </small>''v*'' then sends the unit-sphere onto an ellipsoid isometric to ''T''(''S''). To define the third and last move ''u'', just apply an isometry to this ellipsoid so as to carry it over ''T''(''S''). As can be easily checked, the composition ''u''<small> o </small> ''d''<small> o </small>''v*'' coincides with ''T''. == Calculating the SVD == The SVD of a matrix ''M'' is typically computed by a two-step procedure. In the first step, the matrix is reduced to a [[bidiagonal matrix]]. This takes O(''mn''<sup>2</sup>) floating-point operations, assuming that ''m'' ≥ ''n'' (this formulation uses the [[big O notation]]). The second step is to compute the SVD of the bidiagonal matrix. This step can only be done with an [[iterative method]] (as with [[eigenvalue algorithm]]s). However, in practice it suffices to compute the SVD up to a certain precision, like the [[machine epsilon]]. If this precision is considered constant, then the second step takes 