upon the concept of compressibility. Informally, from the point of view of algorithmic information theory, the information content of a string is equivalent to the length of the shortest possible self-contained representation of that string. A self-contained representation is essentially a [[program (computing)|program]] – in some fixed but otherwise irrelevant universal [[programming language]] – that, when run, outputs the original string. From this point of view, a 3000 page encyclopedia actually contains less information than 3000 pages of completely random letters, despite the fact that the encyclopedia is much more useful. This is because to reconstruct the entire sequence of random letters, one must know, more or less, what every single letter is. On the other hand, if every vowel were removed from the encyclopedia, someone with reasonable knowledge of the English language could reconstruct it, just as one could likely reconstruct the sentence "Ths sntnc hs lw nfrmtn cntnt" from the context and consonants present. For this reason, high-information strings and sequences are sometimes called "random"; people also sometimes attempt to distinguish between "information" and "useful information" and attempt to provide rigorous definitions for the latter, with the idea that the random letters may have more information than the encyclopedia, but the encyclopedia has more "useful" information. Unlike classical information theory, algorithmic information theory gives [[Formal system|formal]], [[Rigour#Mathematical_rigour|rigorous]] definitions of a [[Kolmogorov complexity|random string]] and a [[algorithmically random sequence|random infinite sequence]] that do not depend on physical or philosophical [[Intuition (knowledge)|intuition]]s about [[nondeterminism]] or [[likelihood]]. (The set of random strings depends on the choice of the universal Turing machine used to define Kolmogorov complexity, but any choice gives identical asymptotic results because the Kolmogorov complexity of a string is invariant up to an additive constant depending only on the choice of universal Turing machine. For this reason the set of random infinite sequences is independent of the choice of universal machine.) Some of the results of algorithmic information theory, such as [[Kolmogorov complexity#Chaitin's incompleteness theorem|Chaitin's incompleteness theorem]], appear to challenge common mathematical and philosophical intuitions. Most notable among these is the construction of [[Chaitin's constant]] '''Ω''', a real number which expresses the probability that a self-delimiting universal Turing machine will [[halting problem|halt]] when its input is supplied by flips of a fair coin (sometimes thought of as the probability that a random computer program will eventually halt). Although '''Ω''' is easily defined, in any [[consistent]] [[axiom]]atizable [[theory (mathematical logic)|theory]] one can only compute finitely many digits of '''Ω''', so it is in some sense ''unknowable'', providing an absolute limit on knowledge that is reminiscent of [[Gödel's Incompleteness Theorem]]. Although the digits of '''Ω''' cannot be determined, many properties of '''Ω''' are known; for example, it is an [[algorithmically random sequence]] and thus its binary digits are evenly distributed (in fact it is [[normal number|normal]]). ==History== Algorithmic information theory was founded by [[Ray Solomonoff]]<ref>Vitanyi, P. "[http://homepages.cwi.nl/~paulv/obituary.html Obituary: Ray Solomonoff, Founding Father of Algorithmic Information Theory"]</ref>, who published the basic ideas on which the field is based as part of his invention of [[algorithmic probability]] - a way to overcome serious problems associated with the application of Bayes rules in statistics. He first described his results at a Conference at [[Caltech]] in 1960,<ref>Paper from conference on "Cerebral Systems and Computers", California Institute of Technology, Feb 8-11, 1960, cited in "A Formal Theory of Inductive Inference, Part 1, 1964, p. 1</ref> and in a report, Feb. 1960, "A Preliminary Report on a General Theory of Inductive Inference."<ref>Solomonoff, R., "[http://world.std.com/~rjs/z138.pdf A Preliminary Report on a General Theory of Inductive Inference]", Report V-131, Zator Co., Cambridge, Ma., (November Revision of Feb 4, 1960 report.)</ref> Algorithmic information theory was later developed independently by [[Andrey Kolmogorov]], in 1965 and [[Gregory Chaitin]], around 1966. There are several variants of Kolmogorov complexity or algorithmic information; the most widely used one is based on [[self-delimiting program]]s and is mainly due to [[Leonid Levin]] (1974). [[Per Martin-Löf]] also contributed significantly to the information theory of infinite sequences. An axiomatic approach to algorithmic information theory based on [[Blum axioms]] (Blum 1967) was introduced by Mark Burgin in a paper presented for publication by [[Andrey Kolmogorov]] (Burgin 1982). The axiomatic approach encompasses other approaches in the algorithmic information theory. It is possible to treat different measures of algorithmic information as particular cases of axiomatically defined measures of algorithmic information. Instead of proving similar theorems, such as the basic invariance theorem, for each particular measure, it is possible to easily deduce all such results from one corresponding theorem proved in the axiomatic setting. This is a general advantage of the axiomatic approach in mathematics. The axiomatic approach to algorithmic information theory was further developed in the book (Burgin 2005) and applied to software metrics (Burgin and Debnath, 2003; Debnath and Burgin, 2003). == Precise Definitions == {{Main|Kolmogorov Complexity}} A binary string is said to be random if the [[Kolmogorov complexity]] of the string is at least the length of the string. A simple counting argument shows that some strings of any given length are random, and almost all strings are very close to being random. Since Kolmogorov complexity depends on a fixed choice of universal Turing machine (informally, a fixed "description language" in which the "descriptions" are given), the collection of random strings does depend on the choice of fixed universal machine. Nevertheless, the collection of random strings, as a whole, has similar properties regardless of the fixed machine, so one can (and often does) talk about the properties of random strings as a group without having to first specify a universal machine. {{Main|Algorithmically random sequence}} An infinite binary sequence is said to be random if, for some constant ''c'', for all n, the [[Kolmogorov complexity]] of the initial segment of length ''n'' of the sequence is at least ''n-c''. Importantly, the complexity used here is prefix-free complexity; if plain complexity were used, there would be no random sequences. However, with this definition, it can be shown that almost every sequence (from the point of view of the standard [[measure (mathematics)|measure]] - "fair 