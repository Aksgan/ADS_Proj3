Bounded operators on Hilbert spaces == The factorization <math>M = U\Sigma V^*</math> can be extended to a [[bounded operator]] ''M'' on a separable Hilbert space ''H''. Namely, for any bounded operator ''M'', there exist a [[partial isometry]] ''U'', a unitary ''V'', a measure space (''X'', ''μ''), and a non-negative measurable ''f'' such that :<math>\; M = U T_f V^*, </math> where <math>T_f</math> is the [[multiplication operator|multiplication by ''f'']] on ''L''<sup>2</sup>(''X'', ''μ''). This can be shown by mimicking the linear algebraic argument for the matricial case above. ''VT<sub>f</sub> V*'' is the unique positive square root of ''M*M'', as given by the [[Borel functional calculus]] for [[self adjoint operator]]s. The reason why ''U'' need not be unitary is because, unlike the finite dimensional case, given an isometry ''U''<sub>1</sub> with non trivial kernel, a suitable ''U''<sub>2</sub> may not be found such that :<math>\begin{bmatrix} U_1 \\ U_2 \end{bmatrix}</math> is a unitary operator. As for matrices, the singular value factorization is equivalent to the [[polar decomposition]] for operators: we can simply write :<math>M = U V^* \cdot V T_f V^*</math> and notice that ''U V*'' is still a partial isometry while ''VT<sub>f</sub> V*'' is positive. === Singular values and compact operators === To extend notion of singular values and left/right-singular vectors to the operator case, one needs to restrict to [[compact operator on Hilbert space|compact operators]]. It is a general fact that compact operators on [[Banach space]]s have only discrete spectrum. This is also true for compact operators on Hilbert spaces, since [[Hilbert space]]s are a special case of Banach spaces. If ''T'' is compact, every nonzero ''λ'' in its spectrum is an eigenvalue. Furthermore, a compact self adjoint operator can be diagonalized by its eigenvectors. If ''M'' is compact, so is ''M*M''. Applying the diagonalization result, the unitary image of its positive square root ''T<sub>f</sub>'' has a set of orthonormal eigenvectors {''e<sub>i</sub>''} corresponding to strictly positive eigenvalues {''σ<sub>i</sub>''}. For any ''ψ'' ∈ ''H'', :<math> \; M \psi = U T_f V^* \psi = \sum_i \langle U T_f V^* \psi, U e_i \rangle U e_i = \sum_i \sigma_i \langle \psi, V e_i \rangle U e_i, </math> where the series converges in the norm topology on ''H''. Notice how this resembles the expression from the finite dimensional case. The ''σ<sub>i</sub>'' 's are called the singular values of ''M''. {''U e<sub>i</sub>''} and {''V e<sub>i</sub>''} can be considered the left- and right-singular vectors of ''M'' respectively. [[Compact operator on Hilbert space|Compact operators on a Hilbert space]] are the closure of [[finite rank operator]]s in the uniform operator topology. The above series expression gives an explicit such representation. An immediate consequence of this is: '''Theorem''' ''M'' is compact if and only if ''M*M'' is compact. ==History== The singular value decomposition was originally developed by [[differential geometry|differential geometers]], who wished to determine whether a real [[bilinear form]] could be made equal to another by independent orthogonal transformations of the two spaces it acts on. [[Eugenio Beltrami]] and [[Camille Jordan]] discovered independently, in 1873 and 1874 respectively, that the singular values of the bilinear forms, represented as a matrix, form a complete set of [[invariant (mathematics)|invariant]]s for bilinear forms under orthogonal substitutions. [[James Joseph Sylvester]] also arrived at the singular value decomposition for real square matrices in 1889, apparently independent of both Beltrami and Jordan. Sylvester called the singular values the ''canonical multipliers'' of the matrix ''A''. The fourth mathematician to discover the singular value decomposition independently is Autonne in 1915, who arrived at it via the [[polar decomposition]]. The first proof of the singular value decomposition for rectangular and complex matrices seems to be by [[Carl Eckart]] and Gale Young in 1936;<ref>{{Citation |last1=Eckart |first1=C.|authorlink1=Carl Eckart |last2=Young |first2=G. |year=1936 |title=The approximation of one matrix by another of lower rank |journal=[[Psychometrika]] |volume=1 |issue=3 |pages=211&ndash;218 |doi=10.1007/BF02288367 |postscript=. }}</ref> they saw it as a generalization of the [[principal axis]] transformation for [[Hermitian matrix|Hermitian matrices]]. In 1907, [[Erhard Schmidt]] defined an analog of singular values for [[integral operator]]s (which are compact, under some weak technical assumptions); it seems he was unaware of the parallel work on singular values of finite matrices. This theory was further developed by [[Émile Picard]] in 1910, who is the first to call the numbers <math>\sigma_k</math> ''singular values'' (or rather, ''valeurs singulières''). Practical methods for computing the SVD date back to [[Ervand Kogbetliantz|Kogbetliantz]] in 1954, 1955 and [[Magnus Hestenes|Hestenes]] in 1958.<ref>{{Citation |first=M. R. |last=Hestenes |authorlink=Magnus Hestenes |title=Inversion of Matrices by Biorthogonalization and Related Results |journal=Journal of the Society for Industrial and Applied Mathematics |year=1958 |volume=6 |issue=1 |pages=51&ndash;90 |doi=10.1137/0106005 |id={{MR|0092215}}. {{JSTOR|2098862}} |postscript=. }}</ref> resembling closely the [[Jacobi eigenvalue algorithm]], which uses plane rotations or [[Givens rotation]]s. However, these were replaced by the method of [[Gene H. Golub|Gene Golub]] and [[William Kahan]] published in 1965,<ref>{{Citation | last1=Golub | first1=G. H. | author1-link=Gene H. Golub | last2=Kahan | first2=W. | author2-link=William Kahan | title=Calculating the singular values and pseudo-inverse of a matrix | year=1965 | journal=Journal of the Society for Industrial and Applied Mathematics: Series B, Numerical Analysis | volume=2 | issue=2 | pages=205–224 | doi=10.1137/0702016 |id={{MR|0183105}}. {{JSTOR|2949777}} }}</ref> which uses [[Householder transformation]]s or reflections. In 1970, Golub and Christian Reinsch<ref>{{Citation |title=Singular value decomposition and least squares solutions |first1=G. H. |last1=Golub |authorlink1=Gene H. Golub |first2=C. |last2=Reinsch |year=1970 |journal=Numerische Mathematik |volume=14 |issue=5 |pages=403&ndash;420 |doi=10.1007/BF02163027 |id={{MR|1553974}} |postscript=. }}</ref> published a variant of the Golub/Kahan algorithm that is still the one most-used today. ==See also== *[[matrix decomposition]] *[[Canonical form]] *[[Singular value]] *[[eigendecomposition]] *[[generalized singular value decomposition]] *[[polar decomposition]] *[[principal components analysis]] (PCA) *[[correspondence analysis]] (CA) *[[empirical orthogonal functions]] (EOFs) *[[canonical correlation analysis]] (CCA) *[[latent semantic analysis]] *[[Time series]] *[[Nearest neighbor search]] *[[Fourier analysis]] *[[Cluster analysis]] *[[Linear least squares]] *[[Curse of dimensionality]] *[[Dimension reduction]] *[[Digital signal processing]] *[[Fourier-related transforms]] *[[Wavelet compression]] *[[Locality sensitive hashing]] *[[von Neumann's trace inequality]] *[[Non-linear iterative partial least squares]] ==Notes== {{Reflist}} ==References== * {{Citation | last2=Bau III | first2=David | last1=Trefethen | first1=Lloyd N. | author1-link = Lloyd Nicholas Trefethen | title=Numerical linear algebra | publisher=Society for Industrial and Applied Mathematics | location=Philadelphia | isbn=978-0-89871-361-9 | year=1997}}. * {{Citation | last1=Demmel | first1=James | 