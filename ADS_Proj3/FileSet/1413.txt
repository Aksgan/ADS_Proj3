values and the mean value of the response variable: :<math>\text{ESS} = \sum_{i=1}^n \left(\hat{y}_i - \bar{y}\right)^2.</math> In general: [[total sum of squares]] = '''explained sum of squares''' + [[residual sum of squares]]. ==Partitioning in simple linear regression== The following equality, stating that the total sum of squares equals the residual sum of squares plus the explained sum of squares, is generally true in simple linear regression: :<math>\sum_{i=1}^n \left(y_i - \bar{y}\right)^2 = \sum_{i=1}^n \left(y_i - \hat{y}_i\right)^2 + \sum_{i=1}^n \left(\hat{y}_i - \bar{y}\right)^2.</math> === Simple derivation=== :<math> \begin{align} (y_i - \bar{y}) = (y_{i}-\hat{y}_i)+(\hat{y}_i - \bar{y}). \end{align} </math> Square both sides and sum over all ''i'': :<math> \sum_{i=1}^n (y_{i}-\bar{y})^2=\sum_{i=1}^n (y_i - \hat{y}_{i})^2+\sum_{i=1}^n (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^n 2(\hat{y}_{i}-\bar{y})(y_i - \hat{y}_i). </math> [[Simple linear regression]] gives<ref name=Mendenhall>Mendenhall, William. Introduction to Probability and Statistics, Brooks/Cole ,2009, p. 507</ref> <math>\hat{a}=\bar{y}-\hat{b}\bar{x}</math>. What follows depends on this. :<math> \begin{align} \sum_{i=1}^n 2(\hat{y}_{i}-\bar{y})(y_{i}-\hat{y}_i) & = \sum_{i=1}^{n}2((\bar{y}-\hat{b}\bar{x}+\hat{b}x_{i})-\bar{y})(y_{i}-\hat{y}_{i}) \\ & = \sum_{i=1}^{n}2((\bar{y}+\hat{b}(x_{i}-\bar{x}))-\bar{y})(y_{i}-\hat{y}_{i}) \\ & = \sum_{i=1}^{n}2(\hat{b}(x_{i}-\bar{x}))(y_{i}-\hat{y}_{i}) \\ & = \sum_{i=1}^{n}2\hat{b}(x_{i}-\bar{x})(y_{i}-(\bar{y}+\hat{b}(x_{i}-\bar{x}))) \\ & = \sum_{i=1}^{n}2\hat{b}((y_{i}-\bar{y})(x_{i}-\bar{x})-\hat{b}(x_{i}-\bar{x})^2) . \end{align} </math> Again [[simple linear regression]] gives<ref name=Mendenhall/> :<math>\hat{b}=(\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y}))/(\sum_{i=1}^{n}(x_{i}-\bar{x})^2), </math> :<math> \begin{align} \sum_{i=1}^{n}2(\hat{y}_{i}-\bar{y})(y_{i}-\hat{y}_{i}) & = \sum_{i=1}^{n}2\hat{b}((y_{i}-\bar{y})(x_{i}-\bar{x})-\hat{b}(x_{i}-\bar{x})^2) \\ & = 2\hat{b}\sum_{i=1}^{n}((y_{i}-\bar{y})(x_{i}-\bar{x})-(y_{i}-\bar{y})(x_{i}-\bar{x})) \\ & = 2\hat{b}\cdot 0 = 0. \end{align} </math> ==Partitioning in the general OLS model== The general regression model with ''n'' observations and ''k'' explanators, the first of which is a constant unit vector whose coefficient is the regression intercept, is :<math> y = X \beta + e</math> where ''y'' is an ''n'' × 1 vector of dependent variable observations, each column of the ''n'' × ''k'' matrix ''X'' is a vector of observations on one of the ''k'' explanators, <math>\beta </math> is a ''k'' × 1 vector of true coefficients, and ''e'' is an ''n''× 1 vector of the true underlying errors. The [[ordinary least squares]] estimator for <math>\beta</math> is :<math> \hat \beta = (X^T X)^{-1}X^T y.</math> The residual vector <math>\hat e</math> is <math>y - X \hat \beta = y - X (X^T X)^{-1}X^T y</math>, so the residual sum of squares <math>\hat e ^T \hat e</math> is, after simplification, :<math> RSS = y^T y - y^T X(X^T X)^{-1} X^T y.</math> Denote as <math>\bar y</math> the constant vector all of whose elements are the sample mean <math>y_m</math> of the dependent variable values in the vector ''y''. Then the total sum of squares is :<math> TSS = (y - \bar y)^T(y - \bar y) = y^T y - 2y^T \bar y + \bar y ^T \bar y.</math> The explained sum of squares, defined as the sum of squared deviations of the predicted values from the observed mean of ''y'', is :<math> ESS = (\hat y - \bar y)^T(\hat y - \bar y) = \hat y^T \hat y - 2\hat y^T \bar y + \bar y ^T \bar y.</math> Using <math> \hat y = X \hat \beta</math> in this, and simplifying to obtain <math>\hat y^T \hat y = y^TX(X^T X)^{-1}X^Ty </math>, gives the result that ''TSS'' = ''ESS'' + ''RSS'' if and only if <math>y^T \bar y = \hat y^T \bar y</math>. The left side of this is <math>y_m</math> times the sum of the elements of ''y'', and the right side is <math>y_m</math> times the sum of the elements of <math>\hat y</math>, so the condition is that the sum of the elements of ''y'' equals the sum of the elements of <math>\hat y</math>, or equivalently that the sum of the prediction errors (residuals) <math>y_i - \hat y_i</math> is zero. This can be seen to be true by noting the well-known OLS property that the ''k'' × 1 vector <math>X^T \hat e = X^T [I - X(X^T X)^{-1}X^T]y= 0</math>: since the first column of ''X'' is a vector of ones, the first element of this vector <math>X^T \hat e</math> is the sum of the residuals and is equal to zero. This proves that the condition holds for the result that ''TSS'' = ''ESS'' + ''RSS''. ==See also== *[[Sum of squares]] {{morefootnotes|date=December 2010}} ==Notes== <references/> ==References== * S. E. Maxwell and H. D. Delaney (1990), "Designing experiments and analyzing data: A model comparison perspective". Wadsworth. pp. 289–290. * G. A. Milliken and D. E. Johnson (1984), "Analysis of messy data", Vol. I: Designed experiments. Van Nostrand Reinhold. pp. 146–151. * B. G. Tabachnick and L. S. Fidell (2007), "Experimental design using ANOVA". Duxbury. p. 220. * B. G. Tabachnick and L. S. Fidell (2007), "Using multivariate statistics", 5th ed. Pearson Education. pp. 217–218. {{DEFAULTSORT:Explained Sum Of Squares}} [[Category:Regression analysis]] [[Category:Least squares]] [[it:Somma dei quadrati spiegata]]</text> </page> <page> <id>12847</id> <title>Explanatory gap</title> <text>The '''explanatory gap''' is the claim that [[consciousness]] and human experiences such as [[qualia]] cannot be fully [[Explanation|explained]] only by [[Mechanics|physical mechanical processes]]. Proponents of this view claim that the [[mind]] is substantially and qualitatively different from the [[brain]] and that the existence of something [[metaphysics|metaphysically]] extra-physical is required to 'fill the gap.' The explanatory gap has vexed and intrigued [[philosopher]]s and [[artificial intelligence|AI]] researchers alike for decades and caused considerable debate. To take a condition in which there is no gap, imagine a modern computer: as marvelous as these devices are, their behavior can be fully explained by their circuitry, and vice versa. By contrast, it is thought by some that consciousness constitutes a separate effect that demands another cause, and that this cause is either outside of the physical world ([[dualism]]) or due to as yet unknown physical phenomena (see for instance [[Quantum mind]], [[Indirect realism]]). A noted proponent of the explanatory gap is Joseph Levine, who fleshes out the theory in his papers ''Conceivability, Identity, and the Explanatory Gap'' and ''Materialism and Qualia: The Explanatory Gap''. Levine agrees that conceivability (as used in the [[philosophical zombie|Zombie]] and [[inverted spectrum]] arguments) is flawed as a means of establishing metaphysical realities; but he points out that even if we come to the ''metaphysical'' conclusion that qualia are physical, they still present an ''explanatory'' problem. <blockquote>While I think this materialist response is right in the end, it does not suffice to put the mind-body problem to rest. Even if conceivability considerations do not establish that the mind is in fact distinct from the 