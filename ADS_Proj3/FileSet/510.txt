* [[Wayne Selcher]] * [[Wilhelm Schmidt]] * [[Philippe C. Schmitter]] * [[Ronald Schneider]] * [[Stuart Schwartz]] * [[Anthony Seeger]] * [[Robert Weaver Shirley]] * [[Thomas Skidmore]] * [[T. Lynn Smith]] * [[Stanley J. Stein]] * [[Alfred Stepan]] * [[Nancy Stepan]] * [[Verena Stolcke]] * [[Georg Thomas]] * [[Angelo Trento]] * [[Pierre Fatumbi Verger]] * [[Charles Wagley]] * [[Hermann Watjen]] * [[Emilio Willems]] * [[John Wirth]] * [[Jordan Marten Young]] * [[Jean Ziegler]] ==External links== * [http://www.brazilianist.com The Brazilianist Online] * [http://www.cpdoc.fgv.br/revista/arq/63.pdf Brasilianismo, Brazilianists e Discursos Brasileiros (Brazilianism, Brazilianists and Brazilian Discourses)] (PDF) by Fernanda Peixoto Massi. ==References== <references/> [[Category:Brazilian culture]] [[Category:Interdisciplinary fields]]</text> </page> <page> <id>4475</id> <title>Bregman divergence</title> <text>In [[mathematics]], the '''Bregman divergence''' or '''Bregman distance''' is similar to a [[metric (mathematics)|metric]], but does not satisfy the [[triangle inequality]] nor symmetry. There are two ways in which Bregman divergences are important. Firstly, they generalize squared Euclidean distance to a class of distances that all share similar properties. Secondly, they bear a strong connection to [[exponential family|exponential families]] of distributions; as has been shown by (Banerjee et al. 2005), there is a [[bijection]] between regular exponential families and regular Bregman divergences. Bregman divergences are named after [[L. M. Bregman]], who introduced the concept in 1967. More recently researchers in geometric algorithms have shown that many important algorithms can be generalized from Euclidean metrics to distances defined by Bregman divergence (Banerjee et al. 2005; Nielsen and Nock 2006; Nielsen et al. 2007). == Definition == Let <math>F: \Delta \to \Re </math> be a continuously-differentiable real-valued and strictly [[convex function]] defined on a closed [[convex set]] <math>\Delta</math> The Bregman distance associated with <math>F</math> for points <math>p, q \in \Delta</math> is: <math>B_F(p \rVert q) = F(p)-F(q)-\langle \nabla F(q),(p-q)\rangle </math> Intuitively this can be thought of as the difference between the value of F at point p and the value of the first-order [[Taylor expansion]] of F around point q evaluated at point p. == Properties == * '''Non-negativity''': <math>B_F(p \rVert q) \ge 0</math> for all p,q. This is a consequence of the convexity of F. * '''Convexity''':<math>B_F(p \rVert q)</math> is convex in its first argument, but not necessarily in the second argument. * '''Linearity''': If we think of the Bregman distance as an operator on the function F, then it is linear with respect to non-negative coefficients. In other words, for <math>F_1, F_2</math> strictly convex and differentiable, and <math>\lambda > 0</math>, ::<math>B_{F_1 + \lambda F_2}(p \rVert q) = B_{F_1}(p \rVert q) + \lambda B_{F_2}(p \rVert q)</math> * '''Duality''': The function F has a [[convex conjugate]] <math>F^*</math>. The Bregman distance defined with respect to <math>F^*</math> has an interesting relationship to <math>B_F(p \rVert q)</math> ::<math>B_{F^*}(p^* \rVert q^*) = B_F(q \rVert p)</math> :Here, <math>p^* = \nabla F(p)</math> is the dual point corresponding to p * A key result about Bregman divergences is that, given a random vector, the mean vector minimizes the expected Bregman divergence from the random vector. This result generalizes the textbook result that the mean of a set minimizes total squared error to elements in the set. This result was proved for the vector case by (Banerjee et al. 2005), and extended to the case of functions/distributions by (Frigyik et al. 2008). This result is important because it further justifies using a mean as a representative of a random set, particularly in Bayesian estimation. == Examples == * Squared Euclidean distance <math>B_F(x,y) = \|x - y\|^2</math> is the canonical example of a Bregman distance, generated by the convex function <math>F(x) = \|x\|^2</math> * The squared [[Mahalanobis distance]], <math>B_F(x,y)=\frac{1}{2}(x-y)^T Q (x-y)</math> which is generated by the convex function <math>F(x) = \frac{1}{2} x^T Q x</math>. This can be thought of as a generalization of the above squared Euclidean distance. * The generalized [[Kullback–Leibler divergence]] ::<math>B_F(p,q) = \sum p(i) \log \frac{p(i)}{q(i)} - \sum p(i) + \sum q(i)</math> :is generated by the convex function ::<math>F(p) = \sum_i p(i)\log p(i) - \sum p(i)</math> * The [[Itakura–Saito distance]], ::<math>B_F(p,q) = \sum_i \left(\frac {p(i)}{q(i)} - \log \frac{p(i)}{q(i)} - 1 \right)</math> :is generated by the convex function ::<math>F(p) = - \sum \log p(i)</math> == Generalizing projective duality == A key tool in [[computational geometry]] is the idea of [[projective duality]], which maps points to hyperplanes and vice versa, while preserving incidence and above-below relationships. There are numerous analytical forms of the projective dual: one common form maps the point <math>p = (p_1, \ldots p_d)</math> to the hyperplane <math>x_{d+1} = \sum_1^d 2p_i x_i</math>. This mapping can be interpreted (identifying the hyperplane with its normal) as the convex conjugate mapping that takes the point p to its dual point <math>p^* = \nabla F(p)</math>, where F defines the d-dimensional paraboloid <math>x_{d+1} = \sum x_i^2</math>. If we now replace the paraboloid by an arbitrary convex function, we obtain a different dual mapping that retains the incidence and above-below properties of the standard projective dual. This implies that natural dual concepts in computational geometry like [[Voronoi diagram]]s and [[Delaunay triangulation]]s retain their meaning in distance spaces defined by an arbitrary Bregman divergence. Thus, algorithms from "normal" geometry extend directly to these spaces (Nielsen, Nock and Boissonnat, 2006) == Bregman Divergences between Matrices, Functions, Distributions == Bregman divergences can also be defined between matrices, between functions, and between measures (distributions). Bregman divergences between matrices include the [[Stein's loss]] and [[von Neumann entropy]]. Bregman divergences between functions include total squared error, relative entropy, and squared bias; see the references by Frigyik et al. below for definitions and properties. == References == {{refbegin|2}} *{{cite journal | last1 = Banerjee | first1 = Arindam | last2 = Merugu | first2 = Srujana | last3 = Dhillon | first3 = Inderjit S. | last4 = Ghosh | first4 = Joydeep | journal = [[Journal of Machine Learning Research]] | pages = 1705–1749 | title = Clustering with Bregman divergences | url = http://jmlr.csail.mit.edu/papers/v6/banerjee05b.html | volume = 6 | year = 2005}} *{{cite journal | last = Bregman | first = L. M. | doi = 10.1016/0041-5553(67)90040-7 | journal = USSR Computational Mathematics and Mathematical Physics | pages = 200–217 | title = The 