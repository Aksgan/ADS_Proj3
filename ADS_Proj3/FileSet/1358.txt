* [[Sun-Joo Shin|Shin, Sun-Joo]] (2002), ''The Iconic Logic of Peirce's Graphs'', MIT Press, Cambrodge, MA. ==See also== * [[Charles Sanders Peirce]] * [[Charles Sanders Peirce bibliography]] * [[Existential graph]]s * [[Laws of Form]] * [[Logical graph]]s [[Category:Logic]] [[Category:Mathematical logic]] [[Category:Philosophical logic]] [[Category:History of logic]] [[Category:History of mathematics]] [[Category:Charles Sanders Peirce]] [[zh:实体图]]</text> </page> <page> <id>12277</id> <title>Entity integrity</title> <text>In the [[relational data model]], '''entity integrity''' is one of the [[database integrity|three inherent integrity rules]]. Entity integrity is an integrity rule which states that every table must have a [[primary key]] and that the column or columns chosen to be the primary key should be unique and not null.<ref>Beynon-Davies P. (2004). Database Systems 3rd Edition. Palgrave, Basingstoke, UK. ISBN 1-4039-1601-2</ref> A direct consequence of this integrity rule is that duplicate rows are forbidden in a table. If each value of a primary key must be unique no duplicate rows can logically appear in a table. The NOT NULL characteristic of a primary key ensures that a value can be used to identify all rows in a table. Within relational databases using [[SQL]], entity integrity is enforced by adding a primary key clause to a schema definition. The system enforces Entity Integrity by not allowing operations (INSERT, UPDATE) to produce an invalid primary key. Any operation that is likely to create a duplicate primary key or one containing nulls is rejected. The Entity Integrity ensures that the data that you store remains in the proper format as well as comprehendible. ==See also== *[[Relational database management system]] *[[SQL|Structured Query Language (SQL)]] *[[Referential integrity]] ==References== <references /> [[Category:Data modeling]] {{database-stub}} [[cs:Entitní integrita]] [[ja:実体完全性]] [[pt:Integridade da relação]]</text> </page> <page> <id>12290</id> <title>Entropy estimation</title> <text>Estimating the [[differential entropy]] of a system or process, given some observations, is useful in various science/engineering applications, such as [[Independent Component Analysis]],<ref>Dinh-Tuan Pham (2004) Fast algorithms for mutual information based independent component analysis. In ''Signal Processing''. Volume 52, Issue 10, 2690 - 2700, {{doi|10.1109/TSP.2004.834398}}</ref> [[image analysis]],<ref>Chang, C.-I.; Du, Y.; Wang, J.; Guo, S.-M.; Thouin, P.D. (2006) Survey and comparative analysis of entropy and relative entropy thresholding techniques. In ''Vision, Image and Signal Processing'', Volume 153, Issue 6, 837 - 850, {{doi|10.1049/ip-vis:20050032}}</ref> [[genetic analysis]],<ref>Martins, D. C. ''et al.'' (2008) Intrinsically Multivariate Predictive Genes. In ''Selected Topics in Signal Processing''. Volume 2, Issue 3, 424 - 439, {{doi|10.1109/JSTSP.2008.923841}}</ref> [[speech recognition]],<ref>Gue Jun Jung; Yung-Hwan Oh (2008) Information Distance-Based Subvector Clustering for ASR Parameter Quantization. In ''Signal Processing Letters'', Volume 15, 209 - 212, {{doi|10.1109/LSP.2007.913132 }}</ref> [[manifold learning]],<ref>Costa, J.A.; Hero, A.O. (2004), Geodesic entropic graphs for dimension and entropy estimation in manifold learning. In ''Signal Processing'', Volume 52, Issue 8, 2210 - 2221, {{doi|10.1109/TSP.2004.831130}}</ref> and time delay estimation.<ref>Benesty, J.; Yiteng Huang; Jingdong Chen (2007) Time Delay Estimation via Minimum Entropy. In ''Signal Processing Letters'', Volume 14, Issue 3, March 2007 157 - 160 {{doi|10.1109/LSP.2006.884038 }}</ref> The simplest and most common approach uses histogram-based estimation, but other approaches have been developed and used, each with their own benefits and drawbacks.<ref name="beirlant">J. Beirlant, E. J. Dudewicz, L. Gyorfi, and E. C. van der Meulen (1997) [http://www.its.caltech.edu/~jimbeck/summerlectures/references/Entropy%20estimation.pdf Nonparametric entropy estimation: An overview]. In ''International Journal of Mathematical and Statistical Sciences'', Volume 6, pp. 17– 39.</ref> The main factor in choosing a method is often a trade-off between the bias and the variance of the estimate<ref name="schurmann">T. Schürmann, Bias analysis in entropy estimation. In ''J. Phys. A: Math. Gen'', 37 (2004), pp. L295–L301. {{doi|10.1088/0305-4470/37/27/L02}}</ref> although the nature of the (suspected) distribution of the data may also be a factor.<ref name="beirlant"/> ==Histogram estimator== The histogram approach uses the idea that the differential entropy, :<math>H(X) = -\int_\mathbb{X} f(x)\log f(x)\,dx</math> can be approximated by producing a [[histogram]] of the observations, and then finding the discrete entropy :<math> \begin{matrix} H(X) = - \displaystyle{\sum_{i=1}^nf(x_i)\log f(x_i)} \qquad \end{matrix} </math> of that histogram (which is itself a [[Maximum likelihood|maximum-likelihood estimate]] of the discretized frequency distribution). Histograms can be quick to calculate, and simple, so this approach has some attractions. However, the estimate produced is [[bias]]ed, and although corrections can be made to the estimate, they may not always be satisfactory.<ref name="miller55">G. Miller (1955) Note on the bias of information estimates. In ''Information Theory in Psychology: Problems and Methods'', pp. 95–100.</ref> A method better suited for multidimensional pdf's is to first make a pdf estimate with some method, and then, from the pdf estimate, compute the entropy. A useful pdf estimate method is e.g. Gaussian Mixture Modeling (GMM), where the [[Expectation Maximization]] (EM) algorithm is used to find an ML estimate of a weighted sum of Gaussian pdf's approximating the data pdf. ==Estimates based on sample-spacings== If the data is one-dimensional, we can imagine taking all the observations and putting them in order of their value. The spacing between one value and the next then gives us a rough idea of (the [[Multiplicative inverse|reciprocal]] of) the probability density in that region: the closer together the values are, the higher the probability density. This is a very rough estimate with high [[variance]], but can be improved, for example by thinking about the space between a given value and the one ''m'' away from it, where ''m'' is some fixed number.<ref name="beirlant"/> The probability density estimated in this way can then be used to calculate the entropy estimate, in a similar way to that given above for the histogram, but with some slight tweaks. One of the main drawbacks with this approach is going beyond one dimension: the idea of lining the data points up in order falls apart in more than one dimension. However, using analogous methods, some multidimensional entropy estimators have been developed.<ref name="lm2003">E. G. Learned-Miller (2003) A new class of entropy estimators for multi-dimensional densities, in ''Proceedings of the [[International Conference on Acoustics, Speech, and Signal Processing]] (ICASSP’03)'', vol. 3, April 2003, pp. 297–300.</ref><ref name="il2010">I. Lee (2010) Sample-spacings based density and entropy estimators for spherically invariant multidimensional data, In ''Neural Computation'', vol. 22, issue 8, April 2010, pp. 2208–2227.</ref> ==Estimates based on nearest-neighbours== For each point in our dataset, we can find the distance to its [[nearest neighbour]]. We 