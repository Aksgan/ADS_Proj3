''m'' and [[covariance matrix|moment matrix]] ''M''. The samples are tested for fail or pass. The first- and second-order moments of the Gaussian restricted to the pass samples are ''m*'' and ''M*''. The outcome of ''x'' as a pass sample is determined by a function ''s''(''x''), 0 < ''s''(''x'') < ''q'' ≤ 1, such that ''s''(''x'') is the probability that x will be selected as a pass sample. The average probability of finding pass samples (yield) is :<math> P(m) = \int s(x) N(x - m)\, dx </math> Then the theorem of GA states: <blockquote>For any ''s''(''x'') and for any value of ''P ''< ''q'', there always exist a Gaussian p. d. f. that is adapted for maximum dispersion. The necessary conditions for a local optimum are ''m'' = ''m''* and ''M'' proportional to ''M''*. The dual problem is also solved: ''P'' is maximized while keeping the dispersion constant (Kjellström, 1991). </blockquote> Proofs of the theorem may be found in the papers by Kjellström, 1970, and Kjellström & Taxén, 1981. Since dispersion is defined as the exponential of entropy/disorder/[[average information]] it immediately follows that the theorem is valid also for those concepts. Altogether, this means that Gaussian adaptation may carry out a simultateous maximisation of yield and [[average information]] (without any need for the yield or the [[average information]] to be defined as criterion functions). '''The theorem is valid for all regions of acceptability and all Gaussian distributions'''. It may be used by cyclic repetition of random variation and selection (like the natural evolution). In every cycle a sufficiently large number of Gaussian distributed points are sampled and tested for membership in the region of acceptability. The centre of gravity of the Gaussian, ''m'', is then moved to the centre of gravity of the approved (selected) points, ''m''*. Thus, the process converges to a state of equilibrium fulfilling the theorem. A solution is always approximate because the centre of gravity is always determined for a limited number of points. It was used for the first time in 1969 as a pure optimization algorithm making the regions of acceptability smaller and smaller (in analogy to [[simulated annealing]], Kirkpatrick 1983). Since 1970 it has been used for both ordinary optimization and yield maximization. ==Natural evolution and Gaussian adaptation== It has also been compared to the natural evolution of populations of living organisms. In this case ''s''(''x'') is the probability that the individual having an array ''x'' of phenotypes will survive by giving offspring to the next generation; a definition of individual fitness given by Hartl 1981. The yield, ''P'', is replaced by the [[mean fitness]] determined as a mean over the set of individuals in a large population. Phenotypes are often Gaussian distributed in a large population and a necessary condition for the natural evolution to be able to fulfill the theorem of Gaussian adaptation, with respect to all Gaussian quantitative characters, is that it may push the centre of gravity of the Gaussian to the centre of gravity of the selected individuals. This may be accomplished by the [[Hardy&ndash;Weinberg law]]. This is possible because the theorem of Gaussian adaptation is valid for any region of acceptability independent of the structure (Kjellström, 1996). In this case the rules of genetic variation such as crossover, inversion, transposition etcetera may be seen as random number generators for the phenotypes. So, in this sense Gaussian adaptation may be seen as a genetic algorithm. ==How to climb a mountain== Mean fitness may be calculated provided that the distribution of parameters and the structure of the landscape is known. The real landscape is not known, but figure below shows a fictitious profile (blue) of a landscape along a line (x) in a room spanned by such parameters. The red curve is the mean based on the red bell curve at the bottom of figure. It is obtained by letting the bell curve slide along the ''x''-axis, calculating the mean at every location. As can be seen, small peaks and pits are smoothed out. Thus, if evolution is started at A with a relatively small variance (the red bell curve), then climbing will take place on the red curve. The process may get stuck for millions of years at B or C, as long as the hollows to the right of these points remain, and the mutation rate is too small. [[Image:Fraktal.gif]] If the mutation rate is sufficiently high, the disorder or variance may increase and the parameter(s) may become distributed like the green bell curve. Then the climbing will take place on the green curve, which is even more smoothed out. Because the hollows to the right of B and C have now disappeared, the process may continue up to the peaks at D. But of course the landscape puts a limit on the disorder or variability. Besides &mdash; dependent on the landscape &mdash; the process may become very jerky, and if the ratio between the time spent by the process at a local peak and the time of transition to the next peak is very high, it may as well look like a [[punctuated equilibrium]] as suggested by Gould (see Ridley). ==Computer simulation of Gaussian adaptation== Thus far the theory only considers mean values of continuous distributions corresponding to an infinite number of individuals. In reality however, the number of individuals is always limited, which gives rise to an uncertainty in the estimation of ''m'' and ''M'' (the moment matrix of the Gaussian). And this may also affect the efficiency of the process. Unfortunately very little is known about this, at least theoretically. The implementation of normal adaptation on a computer is a fairly simple task. The adaptation of m may be done by one sample (individual) at a time, for example : ''m''(''i'' + 1) = (1 – ''a'') ''m''(''i'') + ''ax'' where ''x'' is a pass sample, and ''a'' < 1 a suitable constant so that the inverse of a represents the number of individuals in the population. ''M'' may in principle be updated after every 