this extra power available. While stream processing is a branch of SIMD/MIMD processing, they must not be confused, although SIMD implementations can often work in a "streaming" manner, their performance is not comparable: the model envisions a much different usage pattern which allows far greater performance by itself. It has been noted{{ref|GPPasStream}} that when applied on generic processors such as standard CPU, only a 1.5x speedup can be reached. By contrast, ad-hoc stream processors easily reach over 10x performance, mainly attributed to the more efficient memory access and higher levels of parallel processing. Although there are various degrees of flexibility allowed by the model, stream processors usually impose some limitations on the kernel or stream size. For example, consumer hardware often lacks the ability to perform high-precision math, lacks complex indirection chains or presents limits on the number of instructions which can be executed. == Stream processing considerations == Available documentation on Stream processing is very scarce as of this writing (September 12, 2005). Only a few specialized institutions seem to have understood the implied power of the model. [[Stanford University]] has been historically involved in a variety of projects on this, beginning from the [http://graphics.stanford.edu/projects/shading/ Stanford Shading language] and deploying a flexible, stand-alone stream processor called [http://cva.stanford.edu/projects/imagine/ Imagine]. Both those projects revealed the paradigm has a great potential so a much larger scale project has been started. With the name of [http://merrimac.stanford.edu/ Merrimac], a Stream-based supercomputer is now being researched. [[AT&T]] also recognized the wide adoption of stream-enhanced processors as [[Graphics processing unit|GPU]]s rapidly evolved in both speed and functionality.{{ref|GPUasSTREAM}} === Data dependencies and parallelism === A great advantage of the stream programming model lies in the ''kernel'' defining <u>independent</u> and <u>local</u> data usage. Kernel operations define the basic data unit, both as input and output. This allows the hardware to better allocate resources and schedule global I/O. Although usually not exposed in the programming model, the I/O operations seems to be much more advanced on stream processors (at least, on GPUs). I/O operations are also usually pipelined by themselves while chip structure can help hide latencies. Definition of the data unit is usually explicit in the kernel, which is expected to have well-defined inputs (possibly using structures, which is encouraged) and outputs. In some environments, output values are fixed (in GPUs for example, there is a fixed set of output attributes, unless this is relaxed). Having each computing block clearly independent and defined allows to schedule bulk read or write operations, greatly increasing cache and memory bus efficiency. Data locality is also explicit in the kernel. This concept is usually referred to as ''kernel locality'', identifying all the values which are short-lived to a single kernel ''invocation''. All the temporaries are simply assumed to be local to each kernel invocation so, hardware or software can easily allocate them on fast registers. This is strictly related to degree of parallelism that can be exploited. {{Confusing|date=December 2006}} <!-- and I'm the guy ''writing'' the Cell article --> Inside each kernel, producer-consumer relationships can be individuated by usual means while, when kernels are chained one after the another, this relationship is given by the model. This allows easier scheduling decisions because it's clear that if kernel ''B'' requires output from kernel ''A'', it's obvious that ''A'' must be completed before ''B'' can be run (at least on the data unit being used). The Imagine chip's on-board stream controller module manages kernel loads and execution in hardware at runtime keeping a scoreboard of kernel dependencies (as told by the compiler) and can allow out-of-order execution to minimize stalls ''producer-consumer locality''. This is another major new paradigm for high performance processing. The [[Cell processor]] allows this by routing data between various SPEs for example. In comparison, since the Imagine is a pure SIMD machine, inter-cluster communication and kernel execution is always explicit with much lower silicon overhead than a MIMD machine, such as Cell. Imagine uses 8 clusters (a.k.a lanes) of ALUs (similar to Cell's SPEs), but the clusters run in data-parallel mode executing a single kernel at a time. Task switching is done using conventional time-multiplexing. There is only one instruction decode for instance. The tradeoff here is that for kernels that can exploit lower levels of data-parallelism, the efficiency drops as not all clusters will do useful work. For a vast majority of DSP processing though this trade off pays off very well. The parallelism between two kernel instances is similar to a [[thread (computer science)|thread]] level parallelism. Each kernel instance gets data parallelism. Inside each kernel, it is still possible to use instruction level parallelism. Task parallelism (such as overlapped I/O) can still happen. It's easy to have thousands of kernel instances but it's simply impossible to have the same amounts of threads. === Programming model notes === The most immediate challenge in the realm of parallel processing does not lie as much in the type of hardware architecture used, but in how easy it will be to program the system in question in a real-world environment with acceptable performance. Machines like Imagine use a straightforward single-threaded model with automated dependencies, memory allocation and DMA scheduling. This in itself is a result of the research at MIT and Stanford in finding an optimal ''layering of tasks'' between programmer, tools and hardware. Programmers beat tools in mapping algorithms to parallel hardware, and tools beat programmers in figuring out smartest memory allocation schemes, etc. Of particular concern are MIMD designs such as Cell, for which the programmer needs to deal with application partitioning across multiple cores and deal with process synchronization and load balancing. Efficient multi-core programming tools are severely lacking today. A drawback of SIMD programming was the issue of ''Array-of-Structures (AoS)'' and ''Structure-of-Arrays (SoA)''. Programmers often wanted to build data structures with a 'real' meaning, for example: <source lang="c"> // A particle in a three dimensional space. struct particle_t float x, y, z; // not even an array! unsigned byte color[3]; // 8 bit per channel, say we care about 