enable new functionality, but instead (it is claimed), make good programming practices more prominent and more natural. Some programmers feel that these features are either unimportant or outright frivolous. For example, [[Alan Perlis]] once quipped, in a reference to [[Curly bracket programming language|bracket-delimited languages]], that "syntactic sugar causes cancer of the [[semicolon]]" (see [[Epigrams on Programming]]). An extension of this is the term "[[Syntactic_sugar#Syntactic_saccharin|syntactic saccharin]]", meaning gratuitous syntax which does not actually make programming easier<ref>{{cite web|url=http://www.retrologic.com/jargon/S/syntactic-sugar.html|title= The Jargon File v4.4.7: "syntactic sugar"}}</ref>. == Performance comparison == Purely in terms of total [[instruction path length]], a program coded in an imperative style, without using any subroutines at all would have the lowest count. However, the [[binary file|binary]] size of such a program might be larger than the same program coded using subroutines (as in functional and procedural programming) and would reference more "[[locality of reference|"non-local"]] ''physical'' instructions that may increase [[cache misses]] and increase instruction fetch [[Computational overhead|overhead]] in modern processors. The paradigms that use subroutines extensively (including functional, procedural and object oriented) and do not also use significant [[inlining]] (via [[compiler optimization]]s) will, consequently, use a greater percentage of total resources on the subroutine linkages themselves. Object oriented programs - that deliberately do not alter [[program state]] directly - instead using [[mutator method]]s (or "setters") to encapsulate these state changes - will, as a direct consequence, have a greater overhead (since [[message passing]] is essentially a subroutine call - but with three more additional overheads; [[dynamic memory allocation]], parameter copying and [[dynamic dispatch]]). Obtaining memory from the [[dynamic memory allocation|heap]] and copying parameters for message passing may involve significant resources that far exceed those required for the state change itself. Assessors (or "getters") - that merely return the values of private member variables - also depend upon similar message passing subroutines, instead of using a more direct assignment (or comparison) adding to total path length. === Pseudocode examples comparing various paradigms === A pseudocode comparison of imperative, procedural, and object oriented approaches used to calculate the area of a circle (<math> \pi r^2.\, </math>), assuming no subroutine [[inlining]], no [[macro (computer science)|macro]] preprocessors, register arithmetic and weighting each instruction 'step' as just 1 instruction - as a crude measure of [[instruction path length]] - is presented below. The instruction step that is conceptually performing the actual state change is highlighted in bold typeface in each case. Note that the actual arithmetic operations used to compute the area of the circle are the same in all three paradigms, with the difference being that the procedural and object-oriented paradigms wrap those operations in a subroutine call that makes the computation general and reusable. The same effect could be achieved in a [[purely imperative]] program using a macro preprocessor at just the cost of increased program size (only at each macro invocation site) without a corresponding [[pro rata]] runtime cost (proportional to ''n'' invocations - that maybe situated within an [[inner loop]] for instance). Conversely, subroutine inlining by a compiler could reduce procedural programs to something similar in size to the purely imperative code. However, for object-oriented programs, even with inlining, messages still have to be built (from copies of the arguments) for processing by the object oriented methods. The overhead of calls, virtual or otherwise, is not dominated by the [[control flow]] alteration itself - but by the surrounding [[calling convention]] costs, like [[function prologue|prologue and epilogue]] code, stack setup and [[Parameter_(computer_science)#Parameters_and_arguments|argument]] passing<ref>{{cite web|url=http://hbfs.wordpress.com/2008/12/30/the-true-cost-of-calls/ |title=The True Cost of Calls|date=2008-12-30|publisher=wordpress.com}}</ref> (see here<ref>http://en.wikibooks.org/wiki/X86_Disassembly/Functions_and_Stack_Frames</ref> for more realistic instruction path length, stack and other costs associated with calls on an [[x86]] platform). See also here<ref>{{cite web|url=http://www-cs-faculty.stanford.edu/~eroberts/books/ArtAndScienceOfJava/slides/07-ObjectsAndMemory.ppt |title=Art and Science of Java; Chapter 7: Objects and Memory|last=Roberts|first=Eric S.|publisher=Stanford University|date=2008}}</ref> for a slide presentation by [[Eric S. Roberts]] ("The Allocation of Memory to Variables", chapter 7)<ref>{{cite book|url=http://www-cs-faculty.stanford.edu/~eroberts/books/ArtAndScienceOfJava/slides/07-ObjectsAndMemory.ppt |title=Art and Science of Java|last=Roberts|first=Eric S.|publisher=Addison-Wesley|date=2008|ISBN=978-0321486127}}</ref> - illustrating the use of stack and heap memory usage when summing three [[rational number]]s in the [[Java (programming language)|Java]] object oriented language. {| class="wikitable" border="1" cellpadding="1" !Imperative !Procedural !Object-oriented |- | <code> load r; 1 r2 = r * r; 2 '''result = r2 * "3.142";''' 3 . . . . . . . . . . . . . . . . . . .... storage ............. result variable constant "3.142" </code> | <code> area proc(r2,res): push stack 5 load r2; 6 r3 = r2 * r2; 7 '''res = r3 * "3.142";''' 8 pop stack 9 return; 10 ............................................... main proc: load r; 1 call area(r,result); +load p = address of parameter list; 2 +load v = address of subroutine 'area'; 3 +goto v with return; 4 . . . . .... storage ............. result variable constant "3.142" parameter list variable function pointer (==>area) stack storage </code> | <code> circle.area method(r2): push stack 7 load r2; 8 r3 = r2 * r2; 9 res = r3 * "3.142"; 10 pop stack 11 '''return(res);''' 12,13 ............................................... main proc: load r; 1 result = circle.area(r); +allocate heap storage; 2<ref group="See">See section-"[[Comparison of programming paradigms#Allocation of dynamic memory for Message storage and object storage|Allocation of dynamic memory for Message storage and object storage]]"</ref> +copy r to message; 3 +load p = address of message; 4 +load v = addr. of method 'circle.area' 5 +goto v with return; 6 . . .... storage ............. result variable (assumed pre-allocated) immutable variable "3.142" (final) (heap) message variable for circle method call vtable(==>area) stack storage </code> |} <references group="See" /> The advantages of procedural abstraction and object-oriented-style polymorphism are not well illustrated by a small example like the one above. This example is designed principally to illustrate some intrinsic performance differences, not abstraction or code re-use. ==== Subroutine/Method call overhead ==== The presence of a (called) subroutine in a program contributes nothing extra to the functionality of the program regardless of paradigm, but may contribute greatly to the structuring and generality of the program, making it much easier to write, modify, and extend<ref name="steele1997">Guy Lewis Steele, Jr. "Debunking the 'Expensive Procedure Call' Myth, or, Procedure Call Implementations Considered Harmful, 