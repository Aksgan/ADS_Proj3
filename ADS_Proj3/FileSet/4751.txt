file named webappsstore.sqlite. The sqlite3 command can be used to show the elements stored therein. <ref>http://www.cerias.purdue.edu/site/blog/post/firefoxs-super-cookies CERIAS Blog -- Firefox’s Super Cookies </ref> There are browser add-ons for [[Mozilla Firefox]] available that let the user deal with web Storage, such as "BetterPrivacy" which can be configured to remove the whole Web Storage automatically on a regular basis.<ref >[https://addons.mozilla.org/en-US/firefox/addon/6623?from=%2Fde%2Ffirefox%2Faddon%2F6623 Mozilla add-ons page for "Better Privacy"]</ref> <ref>[http://netticat.ath.cx/extensions.html Homepage of "Better Privacy", with some further references to blogs and articles]</ref> ==Similar technologies== * [[HTTP cookie|HTTP cookies]] * [[Indexed Database API]]<ref>http://www.w3.org/TR/IndexedDB/</ref> (formerly WebSimpleDB) * [[Web SQL Database]] * [[Local Shared Object|Local Shared Objects]] in [[Adobe Flash]] * [[userData Behavior]] in Internet Explorer * [[Google Gears]] for IE, Firefox, Safari and Windows Mobile ==References== {{reflist}} ==External links== * [http://www.w3.org/TR/webstorage/ W3C: Web Storage] * [http://msdn.microsoft.com/en-us/library/cc197062(VS.85).aspx MSDN: Introduction to DOM Storage] * [https://developer.mozilla.org/En/DOM:Storage Mozilla Developer Center: DOM Storage] * [http://dev.opera.com/articles/view/web-storage/ Opera: Web Storage: easier, more powerful client-side data storage] <!-- * [http://www.domcached.com/ DOMCached - a memcached like caching system for JavaScript using DOM storage] * Dive Into JavaScript: [http://www.diveintojavascript.com/javascript-apis/web-storage-dom-storage Web Storage (DOM Storage) Reference] and [http://www.diveintojavascript.com/tutorials/web-storage-tutorial-creating-an-address-book-application Web Storage Tutorial: Creating an Address Book Application] --> [[Category:World Wide Web Consortium standards]] [[Category:Internet privacy]] [[de:DOM Storage]]</text> </page> <page> <id>39957</id> <title>Web archiving</title> <text>{{Redirect|Web Archive|the archiving service at web.archive.org|Wayback Machine|the Safari format of the same name|webarchive|the KDE format|KDE WAR (file format)}} {{No footnotes|date=January 2010}} '''Web archiving''' is the process of collecting portions of the [[World Wide Web]] and ensuring the collection is [[digital preservation|preserved]] in an [[archive]], such as an [[archive site]], for future researchers, historians, and the public. Due to the massive size of the Web, web archivists typically employ [[web crawler]]s for automated collection. The largest web archiving organization based on a crawling approach is the [[Internet Archive]] which strives to maintain an archive of the entire Web. [[National library|National libraries]], [[national archive]]s and various consortia of organizations are also involved in archiving culturally important Web content. Commercial web archiving software and services are also available to organizations who need to archive their own web content for corporate heritage, regulatory, or legal purposes. ==Collecting the web== Web archivists generally archive all types of web content including [[HTML]] web pages, [[style sheet (web development)|style sheets]], [[JavaScript]], [[digital image|images]], and [[digital video|video]]. They also archive [[metadata]] about the collected resources such as access time, [[MIME type]], and content length. This metadata is useful in establishing [[Authentication|authenticity]] and [[provenance]] of the archived collection. ==Methods of collection== ===Remote harvesting=== The most common web archiving technique uses [[web crawler]]s to automate the process of collecting [[web page]]s. Web crawlers typically view web pages in the same manner that users with a browser see the Web, and therefore provide a comparatively simple method of remotely harvesting web content. Examples of web crawlers frequently used for web archiving include: * [http://www.biterscripting.com/helppages_automatedinternet.html Automated Internet Sessions] in [http://www.biterscripting.com/ biterScripting] * [[Heritrix]] * [[HTTrack]] * [[Wget]] ====On-demand==== There are numerous services that may be used to archive web resources "on-demand", using web crawling techniques. Those offering archives for use as legal evidence comply with Federal laws and standards. * [[WebCite]], a free service specifically for scholarly authors, journal editors and publishers to permanently archive and retrieve cited Internet references.<ref name="Eysenbach">Eysenbach and Trudel (2005).</ref> * [http://www.archive-it.org/ Archive-It], a subscription service which allows institutions to build, manage and search their own web archive. * [http://www.hanzoarchives.com/ Hanzo Archives], offers commercial web archiving tools and services for regulatory compliance, litigation-support, [[electronic discovery]], and corporate heritage. * [http://www.backupurl.com/ BackupURL.com], allows creation of "a copy of any website that you can share and view any time knowing it will last forever." <ref>{{As of |2010|07|05}}: BackupURL.com is working. Previously, as of 2009-12-23 reported "Service will resume within 1 week."</ref> * [http://www.freezepage.com/ freezePAGE snapshots], a free/subscription service. To preserve snapshots, requires login every 30 days for unregistered users, 60 days for registered users.<ref>[http://www.freezepage.com/faq.htm FAQ] FreezePage.com.</ref> * [http://www.website-archive.com/ Website-Archive.com], a subscription service. Captures screen-shots of pages, transactions and user journeys using "actual browsers". Screen-shots can be viewed online or downloaded in a monthly archive. Uses [http://www.cloudtesting.com Cloud Testing] technology. * [http://www.iterasi.net/ Iterasi], a subscription service to create web archives for the corporate, legal and government industries. * [http://www.pagefreezer.com/ PageFreezer], a subscription [[Software_as_a_service|SaaS]] service to archive, replay and search websites, blogs, web 2.0, Flash & social media for marketing, [[eDiscovery]] and regulatory compliance with U.S. [[Food and Drug Administration]] (FDA), [[Financial Industry Regulatory_Authority]], [[U.S. Securities and Exchange Commission]], [[Sarbanes–Oxley_Act]] [[Federal Rules of Evidence]] and records management laws. Archives can be used as legal evidence. * [http://www.sitequesttech.com/ Compliance WatchDog by SiteQuest Technologies], a subscription service that archives websites and allows users to browse the site as it appeared in the past. It also monitors sites for changes and alerts compliance personnel if a change is detected. * [http://aleph-archives.com/ Aleph Archives], offers web archiving services for regulatory compliance and [[ eDiscovery ]] aimed to corporate (Global 500 market), legal and government industries. * [http://www.nextpoint.com/preservation.html Nextpoint], offers an automated cloud-based, SaaS for marketing, compliance and litigation related needs including electronic discovery * [http://archivethe.net/ Archivethe.net], a shared web-archiving platform operated by the [http://internetmemory.org/ Internet Memory Foundation] (formerly European Archive Foundation). It brings the best technology, scalability and economy of scale to any cultural institution willing to get engaged in Web Archiving. * [http://www.reedtechwebarchiving.com/ Reed Technology Web Archiving Services powered by Iterasi], offers litigation protection, regulatory compliance & [[ eDiscovery ]] in the corporate, legal and government industries.<ref>[[Nexpoint]]</ref> <!-- no adspeak, please --> ===Database archiving=== Database archiving refers to methods for archiving the underlying content of database-driven websites. It typically requires the extraction of the [[database]] content into a standard [[logical schema|schema]], often using [[XML]]. Once stored in that standard format, the archived content of multiple databases can then be made available using a single access system. This approach is exemplified by the [http://deeparc.sourceforge.net/ DeepArc] and [http://www.nla.gov.au/xinq/ Xinq] tools developed by the [[Bibliothèque nationale de France]] and the [[National Library of Australia]] respectively. DeepArc enables the structure of a [[relational database]] to be mapped to an [[XML schema]], and the content exported into an 