stable and valid. Often this refers to the time required for the output to reach from 10% to 90% of its final output level when the input changes. Reducing gate delays in [[digital circuit]]s allows them to process data at a faster rate and improve overall performance. The difference in ''propagation delays'' of logic elements is the major contributor to [[glitch]]es in [[asynchronous circuit]]s as a result of [[race condition]]s. The [[principle of logical effort]] utilizes propagation delays to compare designs implementing the same logical statement. Propagation delay increases with [[operating temperature]], marginal supply voltage as well as an increased output load capacitance. The latter is the largest contributor to the increase of propagation delay. If the output of a logic gate is connected to a long trace or used to drive many other gates (high [[fanout]]) the propagation delay increases substantially. Wires have an approximate propagation delay of 1 ns for every 6 in of length.<ref name='Complete digital design'>{{Cite book | last = Balch | first = Mark | authorlink = | coauthors = | title = Mcgraw Hill - Complete Digital Design A Comprehensive Guide To Digital Electronics And Computer System Architecture | publisher = McGraw-Hill Professional | date = 2003 | location = | pages = 430 | url = http://books.google.co.uk/books?id=uFSRT-OIxyoC&printsec=frontcover&source=gbs_v2_summary_r&cad=0#v=onepage&q=&f=false | doi = | id = | isbn = 9780071409278 }}</ref> Logic gates can have propagation delays ranging from more than 10 ns down to the picosecond range, depending on the technology being used.<ref name='Complete digital design'/> ==Physics== In [[physics]], particularly in the [[electromagnetism]] field, the '''propagation delay''' is the length of time it takes for a signal to travel to its destination. For example, in the case of an electric signal, it is the time taken for the signal to travel through a wire. See also, [[velocity of propagation]]. ==See also== * [[Delay calculation]] * [[Contamination delay]] * [[Transmission delay]] * [[Latency (engineering)]] ==References== {{Reflist}} {{DEFAULTSORT:Propagation Delay}} [[Category:Digital circuits]] [[Category:Digital electronics]] [[Category:Timing in electronic circuits]] [[ar:زمن الانتشار]] [[de:Gatterlaufzeit]] [[ko:전달 지연]] [[pl:Czas propagacji]] [[tr:Propagation delay]]</text> </page> <page> <id>30323</id> <title>Properties of polynomial roots</title> <text>In [[mathematics]], a [[polynomial]] is a function of the form :<math> p(x) = a_0 + a_1 x + \cdots + a_n x^n, \quad x\in \mathbb{C} </math> where the coefficients <math>a_0, \ldots, a_n</math> are complex numbers and <math>a_n\neq 0</math>. The [[fundamental theorem of algebra]] states that polynomial ''p'' has ''n'' roots. The aim of this page is to list various properties of these [[root of a function|roots]]. ==Continuous dependence of coefficients== The ''n'' roots of a polynomial of degree ''n'' depend [[continuous function|continuously]] on the coefficients. This means that there are ''n'' continuous functions <math>r_1,\ldots, r_n</math> depending on the coefficients that parametrize the roots with correct multiplicity. This result implies that the [[eigenvalues]] of a [[matrix (mathematics)|matrix]] depend continuously on the matrix. A [[Mathematical proof|proof]] can be found in Tyrtyshnikov(1997). The problem of approximating the roots given the coefficients is [[condition number|ill-conditioned]]. See, for example, [[Wilkinson's polynomial]]. ==Complex conjugate root theorem== {{main|Complex conjugate root theorem}} The [[complex conjugate root theorem]] states that if the coefficients of a polynomial are real, then the roots appear in pairs of the type ''a'' ± ''ib''. For example, the equation ''x''<sup>2</sup> + 1 = 0 has roots ±''i''. == Radical conjugate roots == It can be proved that if a polynomial ''P''(''x'') with rational coefficients has ''a'' + √''b'' as a zero, where ''a'', ''b'' are rational and √''b'' is irrational, then ''a'' − √''b'' is also a zero. First observe that :<math>\left(x - \left [ a + \sqrt b \right ] \right) \left(x - \left [ a - \sqrt b \right ] \right) = (x - a)^2 - b.</math> Denote this quadratic polynomial by ''D''(''x''). Then, by the [[division transformation|division transformation for polynomials]], :<math>P(x) = D(x)Q(x) + cx + d = ((x - a)^2 - b)Q(x) + cx + d, \,\!</math> where ''c'', ''d'' are rational numbers (by virtue of the fact that the coefficients of ''P''(''x'') and ''D''(''x'') are all rational). But ''a'' + √''b'' is a zero of ''P''(''x''): :<math>P\left( a + \sqrt b \right) = c\left(a + \sqrt b \right) + d = (ac + d) + c \sqrt b = 0.</math> It follows that ''c'', ''d'' must be zero, since otherwise the final equality could be arranged to suggest the irrationality of rational values (and vice versa). Hence ''P''(''x'') = ''D''(''x'')''Q''(''x''), for some quotient polynomial ''Q''(''x''), and ''D''(''x'') is a factor of ''P''(''x'').<ref>{{cite book|author=S. Sastry|title=Engineering Mathematics|publisher=PHI Learning|year=2004|isbn=8120325796|pages=72–73}}</ref> == Bounds on (complex) polynomial roots== A very general class of bounds on the magnitude of roots is implied by the [[Rouché theorem]]. If there is a positive real number ''R'' and a coefficient index ''k'' such that ::<math>|a_k|\,R^k > |a_0|+\cdots+|a_{k-1}|\,R^{k-1}+|a_{k+1}|\,R^{k+1}+\cdots+|a_n|\,R^n</math> then there are exactly ''k'' (counted with multiplicity) roots of absolute value less than ''R''. For ''k=0,n'' there is always a solution to this inequality, for example *for ''k=n'', ::<math>R=1+\frac1{|a_n|}\max\{|a_0|,|a_1|,\dots, |a_{n-1}|\}</math> or ::<math>R=\max\left(1,\,\frac1{|a_n|}\left(|a_0|+|a_1|+\cdots+|a_{n-1}|\right)\right)</math> : are upper bounds for the size of all roots, *for ''k=0'', ::<math>R=\frac{|a_0|}{|a_0|+\max\{|a_1|,|a_2|,\dots, |a_{n}|\}}</math> or ::<math>R=\frac{|a_0|}{\max(|a_0|,\,|a_1|+|a_2|+\cdots+|a_{n}|)}</math> are lower bounds for the size of all of the roots. *for all other indices, the function ::<math>h(R)=|a_0|\,R^{-k}+\cdots+|a_{k-1}|\,R^{-1}-|a_k|+|a_{k+1}|\,R+\cdots+|a_n|\,R^{n-k}</math> :is convex on the positive real numbers, thus the minimizing point is easy to determine numerically. If the minimal value is negative, one has found additional information on the location of the roots. One can increase the separation of the roots and thus the ability to find additional separating circles from the coefficients, by applying the root squaring operation of the [[Graeffe's method|Dandelin-Graeffe iteration]] to the polynomial. A different approach is by using the [[Gershgorin circle theorem]] applied to some [[companion matrix]] of the polynomial, as it is used in the [[Durand–Kerner method|Weierstraß–(Durand–Kerner) method]]. From initial estimates of the roots, that might be quite random, one gets unions of circles that contain the roots of the polynomial. ==Gauss&ndash;Lucas theorem== {{main|Gauss&ndash;Lucas theorem}} The Gauss&ndash;Lucas theorem states that the [[convex hull]] of the roots of a polynomial contains the roots of the [[derivative]] of the polynomial. A sometimes useful corollary is 