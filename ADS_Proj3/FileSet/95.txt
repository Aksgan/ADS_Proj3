for the binary classification task == Given: <math>(x_{1},y_{1}),\ldots,(x_{m},y_{m})</math> where <math>x_{i} \in X,\, y_{i} \in Y = \{-1, +1\}</math> Initialize <math>D_{1}(i) = \frac{1}{m}, i=1,\ldots,m.</math> For <math>t = 1,\ldots,T</math>: * Find the classifier <math>h_{t} : X \to \{-1,+1\}</math> that minimizes the error with respect to the distribution <math>D_{t}</math>: : <math>h_{t} = \underset{h_{t} \in \mathcal{H}}{\operatorname{argmin}} \; \epsilon_{t}</math>, where <math> \epsilon_{t} = \sum_{i=1}^{m} D_{t}(i)[y_i \ne h_{t}(x_{i})]</math> * if <math>\epsilon_{t} \geq 0.5</math> then stop. * Choose <math>\alpha_{t} \in \mathbb{R}</math>, typically <math>\alpha_{t}=\frac{1}{2}\textrm{ln}\frac{1-\epsilon_{t}}{\epsilon_{t}}</math> where <math>\epsilon_{t}</math> is the weighted error rate of classifier <math>h_{t}</math>. * Update: : <math>D_{t+1}(i) = \frac{ D_t(i) \exp(-\alpha_t y_i h_t(x_i)) }{ Z_t }</math><br> where <math>Z_{t}</math> is a normalization factor (chosen so that <math>D_{t+1}</math> will be a [[probability distribution]], i.e. sum one over all x). Output the final classifier: : <math>H(x) = \textrm{sign}\left( \sum_{t=1}^{T} \alpha_{t}h_{t}(x)\right)</math> The equation to update the distribution <math>D_{t}</math> is constructed so that: : <math>- \alpha_{t} y_{i} h_{t}(x_{i}) \begin{cases} <0, & y(i)=h_{t}(x_{i}) \\ >0, & y(i) \ne h_{t}(x_{i}) \end{cases}</math> Thus, after selecting an optimal classifier <math>h_{t} \,</math> for the distribution <math>D_{t} \,</math>, the examples <math>x_{i} \,</math> that the classifier <math>h_{t} \,</math> identified correctly are weighted less and those that it identified incorrectly are weighted more. Therefore, when the algorithm is testing the classifiers on the distribution <math>D_{t+1} \,</math>, it will select a classifier that better identifies those examples that the previous classifer missed. ==Statistical Understanding of Boosting== Boosting can be seen as minimization of a [[convex loss function]] over a [[convex set]] of functions. <ref>T. Zhang, "Statistical behavior and consistency of classification methods based on convex risk minimization", Annals of Statistics 32 (1), pp. 56-85, 2004.</ref> Specifically, the loss being minimized is the exponential loss :<math>\sum_i e^{-y_i f(x_i)}</math> and we are seeking a function :<math>f = \sum_t \alpha_t h_t</math> ==See also== * [[Bootstrap aggregating]] * [[LPBoost]] * [[GentleBoost]] ==References== {{reflist}} ==Implementations== *[http://www.inf.fu-berlin.de/inst/ag-ki/adaboost4.pdf AdaBoost and the Super Bowl of Classifiers - A Tutorial on AdaBoost.] *[http://codingplayground.blogspot.com/2009/03/adaboost-improve-your-performance.html Adaboost in C++], an implementation of Adaboost in C++ and boost by Antonio Gulli *[http://www.mathworks.com/matlabcentral/fileexchange/27813 Easy readable Matlab Implementation of Classic AdaBoost] *[http://code.google.com/p/icsiboost/ icsiboost], an open source implementation of Boostexter *[http://jboost.sourceforge.net JBoost], a site offering a classification and visualization package, implementing AdaBoost among other boosting algorithms. *[http://graphics.cs.msu.ru/en/science/research/machinelearning/adaboosttoolbox MATLAB AdaBoost toolbox. Includes Real AdaBoost, Gentle AdaBoost and Modest AdaBoost implementations.] *[http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=21317&objectType=file A Matlab Implementation of AdaBoost] *[http://luispedro.org/software/milk milk] for Python implements [http://packages.python.org/milk/adaboost.html AdaBoost]. *[http://www.esuli.it/mpboost MPBoost++], a C++ implementation of the original AdaBoost.MH algorithm and of an improved variant, the MPBoost algorithm. *[http://npatternrecognizer.codeplex.com/ NPatternRecognizer ], a fast machine learning algorithm library written in C#. It contains support vector machine, neural networks, bayes, boost, k-nearest neighbor, decision tree, ..., etc. *[http://opencv.willowgarage.com/documentation/boosting.html OpenCV implementation of several boosting variants] ==External links== *[http://www.boosting.org Boosting.org], a site on boosting and related ensemble learning methods *[http://cmp.felk.cvut.cz/~sochmj1/adaboost_talk.pdf AdaBoost] Presentation summarizing Adaboost (see page 4 for an illustrated example of performance) *[http://www.site.uottawa.ca/~stan/csi5387/boost-tut-ppr.pdf A Short Introduction to Boosting] Introduction to Adaboost by Freund and Schapire from 1999 *[http://citeseer.ist.psu.edu/cache/papers/cs/2215/http:zSzzSzwww.first.gmd.dezSzpersonszSzMueller.Klaus-RobertzSzseminarzSzFreundSc95.pdf/freund95decisiontheoretic.pdf A decision-theoretic generalization of on-line learning and an application to boosting] ''Journal of Computer and System Sciences'', no. 55. 1997 (Original paper of Yoav Freund and Robert E.Schapire where Adaboost is first introduced.) *[http://www.cs.ucsd.edu/~yfreund/adaboost/index.html An applet demonstrating AdaBoost] *[http://engineering.rowan.edu/~polikar/RESEARCH/PUBLICATIONS/csm06.pdf Ensemble Based Systems in Decision Making], R. Polikar, IEEE Circuits and Systems Magazine, vol.6, no.3, pp. 21-45, 2006. A tutorial article on ensemble systems including pseudocode, block diagrams and implementation issues for AdaBoost and other ensemble learning algorithms. *[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.51.9525 Additive logistic regression: a statistical view of boosting] by Jerome Friedman, Trevor Hastie, Robert Tibshirani. Paper introducing probabilistic theory for AdaBoost, and introducing GentleBoost [[Category:Classification algorithms]] [[Category:Ensemble learning]] [[fr:AdaBoost]] [[ja:AdaBoost]] [[pl:AdaBoost]] [[pt:AdaBoost]] [[ru:AdaBoost]]</text> </page> <page> <id>938</id> <title>Ada Semantic Interface Specification</title> <text>{{wiktionarypar|ASIS}} The '''Ada Semantic Interface Specification''' ('''ASIS''') is a layered, open architecture providing vendor-independent access to the [[Ada programming language|Ada]] Library Environment. It allows for the static analysis of Ada programs and libraries. == References == * ''ISO/IEC 15291: Information technology — Programming languages — Ada Semantic Interface Specification (ASIS)'' * [http://www.acm.org/sigada/WG/asiswg/specs/asis20s.txt ASIS 2.0 specification] == External links == * [http://www.acm.org/sigada/WG/asiswg/ ASIS Working Group] == See also == * [[Ada programming language]] {{compu-lang-stub}} {{standard-stub}} [[Category:Computer and telecommunication standards]] [[yo:ISO/IEC 15291]]</text> </page> <page> <id>955</id> <title>Adaptive-additive algorithm</title> <text>In the studies of [[Fourier optics]], [[sound synthesis]], [[Stellar astronomy|stellar]] [[interferometry]], [[optical tweezers]], and diffractive optical elements (DOEs) it is often important to know the [[spatial frequency]] phase of an observed wave source. In order to reconstruct this [[phase (waves)|phase]] the '''Adaptive-Additive Algorithm''' (or '''AA algorithm'''), which derives from a group of adaptive (input-output) algorithms, can be used. The AA algorithm is an [[iterative]] [[algorithm]] that utilizes the [[Fourier Transform]] to calculate an unknown part of a propagating wave, normally the [[spatial frequency]] [[phase (waves)|phase]] (k space). This can be done when given the phase’s known counterparts, usually an observed [[amplitude]] (position space) and an assumed starting [[amplitude]] (k space). To find the correct [[phase (waves)|phase]] the [[algorithm]] uses error conversion, or the error between the desired and the theoretical [[intensity (physics)|intensities]]. The AA algorithm is currently being implemented by Dr. Wendell Hill III, Alex Robel, V. Kotlyar Soifer, and David G Grier. ==The algorithm== ===History=== The adaptive-additive algorithm was originally created to reconstruct the [[spatial frequency]] [[phase (waves)|phase]] of light intensity in the study of stellar [[interferometry]]. Since then, the AA algorithm has been adapted to work in the fields of [[Fourier Optics]] by Soifer and Dr. Hill, [[soft matter]] and [[optical tweezers]] by Dr. Grier, and [[sound synthesis]] by Robel. ===Pseudo-code algorithm=== 1. Define input amplitude and random phase 2. Forward Fourier Transform 3. Separate transformed amplitude and phase 4. Compare transformed amplitude/intensity to desired output amplitude/intensity 5. Check convergence conditions 6. Mix transformed amplitude with desired output amplitude and combine with transformed phase 7. Inverse Fourier Transform 8. Separate new amplitude and new phase 9. Combine new phase with original input amplitude 10. Loop back to Forward Fourier Transform ===Example=== For the problem of reconstructing the [[spatial frequency]] phase (''k''-space) for a desired [[intensity (physics)|intensity]] in the image plane (''x''-space). Assume the [[amplitude]] and the starting phase of the wave in ''k''-space is a 