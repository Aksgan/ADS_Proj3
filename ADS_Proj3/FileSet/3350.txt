stated as follows. Minimize the ''quadratic'' continuous-time cost functional :<math>J=\tfrac{1}{2} \textbf{x}^{\text{T}}(t_f)\textbf{S}_f\textbf{x}(t_f) + \tfrac{1}{2} \int_{t_0}^{t_f} ( \textbf{x}^{\text{T}}(t)\textbf{Q}(t)\textbf{x}(t) + \textbf{u}^{\text{T}}(t)\textbf{R}(t)\textbf{u}(t) )\, \operatorname{d}t</math> Subject to the ''linear'' first-order dynamic constraints :<math>\dot{\textbf{x}}(t)=\textbf{A}(t) \textbf{x}(t) + \textbf{B}(t) \textbf{u}(t), </math> and the initial condition :<math> \textbf{x}(t_0) = \textbf{x}_0</math> A particular form of the LQ problem that arises in many control system problems is that of the ''linear quadratic regulator'' (LQR) where all of the matrices (i.e., <math>\textbf{A}</math>, <math>\textbf{B}</math>, <math>, \textbf{Q}</math>, and <math>\textbf{R}</math>) are ''constant'', the initial time is arbitrarily set to zero, and the terminal time is taken in the limit <math>t_f\rightarrow\infty</math> (this last assumption is what is known as ''infinite horizon''). The LQR problem is stated as follows. Minimize the infinite horizon quadratic continuous-time cost functional :<math>J=\tfrac{1}{2} \int_{0}^{\infty} ( \textbf{x}^{\text{T}}(t)\textbf{Q}\textbf{x}(t) + \textbf{u}^{\text{T}}(t)\textbf{R}\textbf{u}(t) )\, \operatorname{d}t</math> Subject to the ''linear time-invariant'' first-order dynamic constraints :<math>\dot{\textbf{x}}(t)=\textbf{A} \textbf{x}(t) + \textbf{B} \textbf{u}(t), </math> and the initial condition :<math> \textbf{x}(t_0) = \textbf{x}_0</math> In the finite-horizon case the matrices are restricted in that <math>\textbf{Q}(t)</math> and <math>\textbf{R}(t)</math> are positive semi-definite and positive definite, respectively. In the infinite-horizon case, however, the [[matrix (mathematics)|matrices]] <math>\textbf{Q}</math> and <math>\textbf{R}</math> are not only positive-semidefinite and positive-definite, respectively, but are also ''constant''. These additional restrictions on <math>\textbf{Q}</math> and <math>\textbf{R}</math> in the infinite-horizon case are enforced to ensure that the cost functional remains positive. Furthermore, in order to ensure that the cost function is ''bounded'', the additional restriction is imposed that the pair <math>(\textbf{A},\textbf{B})</math> is ''controllable''. Note that the LQ or LQR cost functional can be thought of physically as attempting to minimize the ''control energy'' (measured as a quadratic form). The infinite horizon problem (i.e., LQR) may seem overly restrictive and essentially useless because it assumes that the operator is driving the system to zero-state and hence driving the output of the system to zero. This is indeed correct. However the problem of driving the output to a desired nonzero level can be solved ''after'' the zero output one is. In fact, it can be proved that this secondary LQR problem can be solved in a very straightforward manner. It has been shown in classical optimal control theory that the LQ (or LQR) optimal control has the feedback form :<math>\textbf{u}(t)=-\textbf{K}(t)\textbf{x}(t)</math> where <math>\textbf{K}(t)</math> is a properly dimensioned matrix, given as :<math>\textbf{K}(t)=\textbf{R}^{-1}\textbf{B}^{\text{T}}\textbf{S},</math> and <math>\textbf{S}</math> is the solution of the differential [[Riccati equation]]. The differential Riccati equation is given as :<math>\dot{\textbf{S}} = -\textbf{S}\textbf{A}-\textbf{A}^{\text{T}}\textbf{S}+\textbf{S}\textbf{B}\textbf{R}^{-1}\textbf{B}^{\text{T}}\textbf{S}-\textbf{Q}</math> For the finite horizon LQ problem, the Riccati equation is integrated backward in time using the terminal boundary condition :<math>\textbf{S}(t_f) = \textbf{S}_f</math> For the infinite horizon LQR problem, the differential Riccati equation is replaced with the ''algebraic'' Riccati equation (ARE) given as :<math>\textbf{0} = -\textbf{S}\textbf{A}-\textbf{A}^{\text{T}}\textbf{S}+\textbf{S}\textbf{B}\textbf{R}^{-1}\textbf{B}^{\text{T}}\textbf{S}-\textbf{Q}</math> Understanding that the ARE arises from infinite horizon problem, the matrices <math>\textbf{A}</math>, <math>\textbf{B}</math>, <math>\textbf{Q}</math>, and <math>\textbf{R}</math> are all ''constant''. It is noted that there are in general ''two'' solutions to the algebraic Riccati equation and the ''positive definite'' (or positive semi-definite) solution is the one that is used to compute the feedback gain. It is noted that the LQ (LQR) problem was elegantly solved by [[Rudolf Kalman]].<ref>Kalman, Rudolf. ''A new approach to linear filtering and prediction problems''. Transactions of the ASME, Journal of Basic Engineering, 82:34â€“45, 1960</ref> ==Numerical methods for optimal control== Optimal control problems are generally nonlinear and therefore, generally do not have analytic solutions (e.g., like the linear-quadratic optimal control problem). As a result, it is necessary to employ numerical methods to solve optimal control problems. In the early years of optimal control (circa 1950s to 1980s) the favored approach for solving optimal control problems was that of ''indirect methods''. In an indirect method, the calculus of variations is employed to obtain the first-order optimality conditions. These conditions result in a two-point (or, in the case of a complex problem, a multi-point) boundary-value problem. This boundary-value problem actually has a special structure because it arises from taking the derivative of a Hamiltonian. Thus, the resulting dynamical system is a ''Hamiltonian system'' of the form : <math>\begin{array}{lcl} \dot{\textbf{x}} & = & \partial H/\partial\boldsymbol{\lambda} \\ \dot{\boldsymbol{\lambda}} & = & -\partial H/\partial\textbf{x} \end{array}</math> where : <math>H=\mathcal{L}+\boldsymbol{\lambda}^{\text{T}}\textbf{a}-\boldsymbol{\mu}^{\text{T}}\textbf{b}</math> is the ''augmented Hamiltonian'' and in an indirect method, the boundary-value problem is solved (using the appropriate boundary or ''transversality'' conditions). The beauty of using an indirect method is that the state and adjoint (i.e., <math>\boldsymbol{\lambda}</math>) are solved for and the resulting solution is readily verified to be an extremal trajectory. The disadvantage of indirect methods is that the boundary-value problem is often extremely difficult to solve (particularly for problems that span large time intervals or problems with interior point constraints). A well-known software program that implements indirect methods is BNDSCO.<ref>Oberle, H. J. and Grimm, W., "BNDSCO-A Program for the Numerical Solution of Optimal Control Problems," Institute for Flight Systems Dynamics, DLR, Oberpfaffenhofen, 1989</ref> The approach that has risen to prominence in numerical optimal control over the past two decades (i.e., from the 1980s to the present) is that of so called ''direct methods''. In a direct method, the state and/or control are approximated using an appropriate function approximation (e.g., polynomial approximation or piecewise constant parameterization). Simultaneously, the cost functional is approximated as a ''cost function''. Then, the coefficients of the function approximations are treated as optimization variables and the problem is "transcribed" to a nonlinear optimization problem of the form: Minimize : <math> F(\textbf{z})\,</math> subject to the algebraic constraints : <math> \begin{array}{lcl} \textbf{g}(\textbf{z}) & = & \textbf{0} \\ \textbf{h}(\textbf{z}) & \leq & \textbf{0} \end{array} </math> Depending upon the type of direct method employed, the size of the nonlinear optimization problem can be quite small (e.g., as in a direct shooting or quasilinearization method) or may be quite large (e.g., a direct collocation method<ref>Betts, J. T., ''Practical Methods for Optimal Control Using Nonlinear Programming,'' SIAM Press, Philadelphia, Pennsylvania, 2001</ref>). In the latter case (i.e., a collocation method), the nonlinear optimization problem may be literally thousands to tens of thousands of variables and constraints. Given the size of many NLPs arising from a direct method, it may appear somewhat counter-intuitive that solving the nonlinear optimization problem is easier than solving the 