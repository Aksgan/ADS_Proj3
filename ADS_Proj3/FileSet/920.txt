inequality constraint and the second is an equality constraint. These two constraints define the feasible set of [[candidate solution]]s. Without the constraints, the solution would be <math>(0,0)\,</math> where <math>f(\bold x)</math> has the lowest value. But this solution does not satisfy the constraints. The solution of the '''constrained optimization problem''' stated above but <math> \bold x = (1,1)</math>, which is the point with the smallest value of <math>f(\bold x)</math> that satisfies the two constraints. == Terminology == * If a constraint is an ''equality'' at a given point, the constraint is said to be '''{{visible anchor|binding}}''', as the point ''cannot'' be varied in the direction of the constraint. * If a constraint is an ''inequality'' at a given point, the constraint is said to be '''{{visible anchor|non-binding}}''', as the point ''can'' be varied in the direction of the constraint. * If a constraint is not satisfied, the point is said to be '''[[Feasible region|infeasible]]'''. == See also == * [[Constraint satisfaction problem]] * [[Karush&ndash;Kuhn&ndash;Tucker conditions]] * [[Lagrange multipliers]] * [[Level set]] * [[Linear programming]] * [[Nonlinear programming]] == External links == *[http://www-unix.mcs.anl.gov/otc/Guide/faq/nonlinear-programming-faq.html Nonlinear programming FAQ] *[http://glossary.computing.society.informs.org/ Mathematical Programming Glossary] [[Category:Mathematical optimization]] [[ca:Restricció]] [[de:Nebenbedingung]] [[el:Περιορισμός]] [[es:restricción (matemáticas)]] [[io:Koakto]] [[nl:Randvoorwaarde (wiskunde)]] [[ro:Constrângere]] [[sl:Omejitev (matematika)]]</text> </page> <page> <id>7859</id> <title>Constraint learning</title> <text>In [[constraint satisfaction problem|constraint satisfaction]] [[backtracking]] [[algorithm]]s, '''constraint learning''' is a technique for improving efficiency. It works by recording new constraints whenever an inconsistency is found. This new constraint may reduce the [[search space]], as future partial evaluations may be found inconsistent without further search. '''Clause learning''' is the name of this technique when applied to [[propositional satisfiability]]. ==Definition== Backtracking algorithms work by choosing an unassigned variable and recursively solve the problems obtained by assigning a value to this variable. Whenever the current partial solution is found inconsistent, the algorithm goes back to the previously assigned variable, as expected by recursion. A constraint learning algorithm differs because it tries to record some information, before backtracking, in form of a new constraint. This can reduce the further search because the subsequent search may encounter another partial solution that is inconsistent with this new constraint. If the algorithm has learned the new constraint, it will backtrack from this solution, while the original backtracking algorithm would do a subsequent search. If the partial solution <math>x_1=a_1,\ldots,x_k=a_k</math> is inconsistent, the problem instance implies the constraint stating that <math>x_i=a_i</math> cannot be true for all <math>i \in [1,k]</math> at the same time. However, recording this constraint is not useful, as this partial solution will not be encountered again due to the way backtracking proceed. On the other hand, if a subset of this evaluation is inconsistent, the corresponding constraint may be useful in the subsequent search, as the same subset of the partial evaluation may occur again in the search. For example, the algorithm may encounter an evaluation extending the subset <math>x_2=a_2, x_5=a_5, x_{k-1}=a_{k-1}</math> of the previous partial evaluation. If this subset is inconsistent and the algorithm has stored this fact in form of a constraint, no further search is needed to conclude that the new partial evaluation cannot be extended to form a solution. {| cellpadding=20 |- | [[Image:Constraint-learning-1.svg]] | [[Image:Constraint-learning-2.svg]] | [[Image:Constraint-learning-3.svg]] |- | Search has reached a dead end. | Inconsistency may be caused by the values of <math>x_1</math> and <math>x_4</math> only. This fact can be stored in a new constraint. | If the algorithm reaches the same values of <math>x_1</math> and <math>x_4</math> again, the new constraint blocks the search. |} ==Efficiency of constraint learning== The efficiency of constraint learning algorithm is balanced between two factors. On one hand, the more often a recorded constraint is violated, the more often backtracking avoids doing useless search. Small inconsistent subsets of the current partial solution are usually better than large ones, as they correspond to constraints that are easier to violate. On the other hand, finding a small inconsistent subset of the current partial evaluation may require time, and the benefit may not be balanced by the subsequent reduction of the search time. Size is however not the only feature of learned constraints to take into account. Indeed, a small constraint may be useless in a particular state of the search space because the values that violate it will not be encountered again. A larger constraint whose violating values are more similar to the current partial assignment may be preferred in such cases. Various constraint learning techniques exist, differing in strictness of recorded constraints and cost of finding them. ==Graph-based learning== If the algorithm proves all values of <math>x_{k+1}</math> to be inconsistent with <math>x_1=a_1,\ldots,x_k=a_k</math>, then this evaluation was consistent, as otherwise the algorithm would not have evaluated <math>x_{k+1}</math> at all; as a result, the constraints violated by a value of <math>x_{k+1}</math> together with <math>x_1=a_1,\ldots,x_k=a_k</math> all contain <math>x_{k+1}</math>. As a result, an inconsistent evaluation is the restriction of the truth evaluation of <math>x_1,\ldots,x_k</math> to variables that are in a constraint with <math>x_{k+1}</math>, provided that this constraint contains no unassigned variable. Learning constraints representing these partial evaluation is called graph-based learning. It uses the same rationale of [[graph-based backjumping]]. These methods are called "graph-based" because they are based on pairs of variables are in the same constraint, which can be found out from the graph associated to the constraint satisfaction problem. ==Jumpback learning== Jumpback learning is based on storing as constraints the inconsistent assignments that would be found by [[conflict-based backjumping]]. Whenever a partial assignment is found inconsistent, this algorithm selects the violated constraint that is minimal according to an ordering based on the order of instantiation of variables. The evaluation restricted of the variables that are in this constraint is inconsistent and is usually shorter than the complete evaluation. Jumpback learning stores this fact as a new constraint. The ordering on constraints is based on the order of assignment of variable. In particular, the least of two constraint is the one whose latest non-common variable has been instantiated first. When an inconsistent assignment is reached, jumpback learning selects the violated constraint that is minimal according to this ordering, and restricts the 