step ''y'' leading to a feasible point : ''x'' = ''m'' + ''y'' according to: : ''M''(''i'' + 1) = (1 – 2''b'') ''M''(''i'') + 2''byy''<sup>T</sup>, where ''y''<sup>T</sup> is the transpose of ''y'' and ''b'' << 1 is another suitable constant. In order to guarantee a suitable increase of [[average information]], ''y'' should be [[normal distribution|normally distributed]] with moment matrix ''μ''<sup>2</sup>''M'', where the scalar ''μ'' > 1 is used to increase [[average information]] ([[information entropy]], disorder, diversity) at a suitable rate. But ''M'' will never be used in the calculations. Instead we use the matrix ''W'' defined by ''WW''<sup>T</sup> = ''M''. Thus, we have ''y'' = ''Wg'', where ''g'' is normally distributed with the moment matrix ''μU'', and ''U'' is the unit matrix. ''W'' and ''W''<sup>T</sup> may be updated by the formulas : ''W'' = (1 – ''b'')''W'' + ''byg''<sup>T</sup> and ''W''<sup>''T''</sup> = (1 – ''b'')''W''<sup>T</sup> + ''bgy''<sup>T</sup> because multiplication gives : ''M'' = (1 – 2''b'')''M'' + 2''byy''<sup>T</sup>, where terms including ''b''<sup>2</sup> have been neglected. Thus, ''M'' will be indirectly adapted with good approximation. In practice it will suffice to update ''W'' only : ''W''(''i'' + 1) = (1 – ''b'')''W''(''i'') + ''byg''<sup>T</sup>. This is the formula used in a simple 2-dimensional model of a brain satisfying the Hebbian rule of associative learning; see the next section (Kjellström, 1996 and 1999). The figure below illustrates the effect of increased [[average information]] in a Gaussian p.d.f. used to climb a mountain Crest (the two lines represent the contour line). Both the red and green cluster have equal mean fitness, about 65%, but the green cluster has a much higher [[average information]] making the green process much more efficient. The effect of this adaptation is not very salient in a 2-dimensional case, but in a high-dimensional case, the efficiency of the search process may be increased by many orders of magnitude. [[Image:Mountain crest.GIF]] ==The evolution in the brain== In the brain the evolution of DNA-messages is supposed to be replaced by an evolution of signal patterns and the phenotypic landscape is replaced by a mental landscape, the complexity of which will hardly be second to the former. The metaphor with the mental landscape is based on the assumption that certain signal patterns give rise to a better well-being or performance. For instance, the control of a group of muscles leads to a better pronunciation of a word or performance of a piece of music. In this simple model it is assumed that the brain consists of interconnected components that may add, multiply and delay signal values. * A nerve cell kernel may add signal values, * a synapse may multiply with a constant and * An axon may delay values. This is a basis of the theory of digital filters and neural networks consisting of components that may add, multiply and delay signalvalues and also of many brain models, Levine 1991. In the figure below the brain stem is supposed to deliver Gaussian distributed signal patterns. This may be possible since certain neurons fire at random (Kandel et al.). The stem also constitutes a disordered structure surrounded by more ordered shells (Bergström, 1969), and according to the [[central limit theorem]] the sum of signals from many neurons may be Gaussian distributed. The triangular boxes represent synapses and the boxes with the + sign are cell kernels. In the cortex signals are supposed to be tested for feasibility. When a signal is accepted the contact areas in the synapses are updated according to the formulas below in agreement with the Hebbian theory. The figure shows a 2-dimensional computer simulation of Gaussian adaptation according to the last formula in the preceding section. [[Image:brain2.GIF]] ''m'' and ''W'' are updated according to: : ''m''<sub>1</sub> = 0.9 ''m''<sub>1</sub> + 0.1 ''x''1; ''m''<sub>2</sub> = 0.9 ''m''<sub>2</sub> + 0.1 ''x''<sub>2</sub>; : ''w''<sub>11</sub> = 0.9 ''w''<sub>11</sub> + 0.1 ''y''<sub>1</sub>''g''<sub>1</sub>; ''w''<sub>12</sub> = 0.9 ''w''<sub>12</sub> + 0.1 ''y''<sub>1</sub>''g''<sub>2</sub>; : ''w''<sub>21</sub> = 0.9 ''w''<sub>21</sub> + 0.1 ''y''<sub>2</sub>''g''<sub>1</sub>; ''w''<sub>22</sub> = 0.9 ''w''<sub>22</sub> + 0.1 ''y''<sub>2</sub>''g''<sub>2</sub>; As can be seen this is very much like a small brain ruled by the theory of [[Hebbian learning]] (Kjellström, 1996, 1999 and 2002). ==Gaussian adaptation and free will== Gaussian adaptation as an evolutionary model of the brain obeying the [[Hebbian theory]] of associative learning offers an alternative view of [[free will]] due to the ability of the process to maximize the mean fitness of signal patterns in the brain by climbing a mental landscape in analogy with phenotypic evolution. Such a random process gives us lots of freedom of choice, but hardly any will. An illusion of will may, however, emanate from the ability of the process to maximize mean fitness, making the process goal seeking. I. e., it prefers higher peaks in the landscape prior to lower, or better alternatives prior to worse. In this way an illusive will may appear. A similar view has been given by Zohar 1990. See also Kjellström 1999. ==A theorem of efficiency for random search== The efficiency of Gaussian adaptation relies on the theory of information due to Claude E. Shannon (see [[information content]]). When an event occurs with probability ''P'', then the information &minus;log(''P'') may be achieved. For instance, if the mean fitness is ''P'', the information gained for each individual selected for survival will be &minus;log(''P'') – on the average - and the work/time needed to get the information is proportional to 1/''P''. Thus, if efficiency, E, is defined as information divided by the work/time needed to get it we have: : ''E'' = &minus;''P'' log(''P''). This function attains its maximum when ''P'' = 1/''e'' = 0.37. The same result has been obtained by Gaines with a different method. ''E'' = 0 if ''P'' = 0, for a process with infinite mutation rate, and if ''P'' = 1, for a process with mutation rate = 0 (provided that the process is alive). This measure of efficiency is valid for a large class of [[random search]] processes provided that certain conditions are at hand. 1 The search should be 