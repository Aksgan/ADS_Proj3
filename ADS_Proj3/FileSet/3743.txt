be built by hand. A very popular approach has been ''Probabilistic Record Linkage'' (''PRL''). In this approach, a large set of pairs of records are human-labeled as being matching or differing pairs. Then statistics are calculated from the agreement of fields on matching and differing records to determine weights on each field. During execution, the agreement or disagreement weight for each field is added to get a combined score that represents the probability that the records refer to the same entity. Often there is one threshold above which a pair is considered a match, and another threshold below which it is considered not to be a match. Between the two thresholds a pair is considered to be "possibly a match", and dealt with accordingly (e.g., human reviewed, linked, or not linked, depending on the application). In recent years, a variety of machine learning techniques have been used in record linkage. It has been recognized that Probabilistic Record Linkage is equivalent to the "[[Naive Bayes]]" algorithm in the field of machine learning, and suffers from the same assumption of the independence of its features, which is typically not true. Higher accuracy can often be achieved by using various other machine learning techniques, including a single-layer [[Perceptron]]. Regardless of whether rule-based, PRL or machine learning techniques are used, ''normalization'' of the data is very important. Names are often spelled differently in different sources (e.g., "Wm. Smith", "William Smith", "William J. Smith", "Bill Smith", etc.), dates can be recorded various ways ("1/2/73", "1973.1.2", "Jan 2, 1973"), and places can be recorded differently as well ("Berkeley, CA", "Berkeley, Alameda, California, USA", etc.). By normalizing these into a common format and using comparison techniques that handle additional variation, much more consistency can be achieved, resulting in higher accuracy in any record linkage technique. Record linkage typically involves two main steps: ''blocking'' and ''scoring''. The ''blocking'' phase attempts to find groups of records that are similar in some way. For example, a query might select all individuals with the same surname and zip code. Of course many of these individuals would not be the same real person (false positives), and some records for the same individual might have different values for the surname (due to typos, married name, etc.) or zipcode (e.g., because they moved). Thus robust systems often use multiple blocking passes to group data in various ways in order to come up with groups of records that should be compared to each other. Once a set of potential matches is identified, a ''scoring'' algorithm is used to calculate a score indicating how likely the two records are to really represent the same real entity. By using labeled test data, score thresholds can be selected that yield a desired trade-off between ''recall'' and ''precision''. (''Recall'' is the percent of true matching pairs that get a score above a given threshold, and ''precision'' is the percent of pairs with a score above a given threshold that are really a match). ==History of RL theory== The initial idea goes back to [[Halbert L. Dunn]] in 1946<ref>{{cite journal | first = Halbert L. | last = Dunn | authorlink = Halbert L. Dunn | title = Record Linkage | journal = [[American Journal of Public Health]] | year = 1946 | month = December | volume = 36 | issue = 12 | pages = ''pp.'' 1412&ndash;1416 | url = http://www.ajph.org/cgi/reprint/36/12/1412 | format = PDF | accessdate = 2008-05-31 | doi = 10.2105/AJPH.36.12.1412}}</ref>. In the 1950s, Howard Borden Newcombe laid the probabilistic foundations of modern record linkage theory. In 1969, [[Ivan Fellegi]] and Alan Sunter formalized these ideas and proved that the probabilistic decision rule they described was optimal when the comparison attributes are conditionally independent. Their pioneering work "A Theory For Record Linkage"<ref>{{cite journal | first = Ivan | last = Fellegi | authorlink = Ivan Fellegi | coauthors = Sunter, Alan | title = A Theory for Record Linkage | journal = [[Journal of the American Statistical Association]] | year = 1969 | month = December | volume = 64 | issue = 328 | pages = ''pp.'' 1183&ndash;1210 | id = {{JSTOR|2286061}}. | doi = 10.2307/2286061 | url = http://jstor.org/stable/2286061 }}</ref> is, still today, the mathematical tool for many record linkage applications. Since the late 1990s, various machine learning techniques have been developed that can, under favorable conditions, be used to estimate the conditional probabilities required by the Fellegi-Sunter (FS) theory. Several researchers have reported that the conditional independence assumption of the FS algorithm is often violated in practice; however, published efforts to explicitly model the conditional dependencies among the comparison attributes have not resulted in an improvement in record linkage quality. {{Fact|date=May 2007}} ==Mathematical model== In an application with two files, A and B, denote the rows (''records'') by <math>\alpha (a)</math> in file A and <math>\beta (b)</math> in file B. Assign <math>K</math> ''characteristics'' to each record. The set of records that represent identical entities is defined by <math> M = \left\{ (a,b); a=b; a \in A; b \in B \right\} </math> and the complement of set <math>M</math>, namely set <math>U</math> representing different entities is defined as <math> U = \{ (a,b); a \neq b; a \in A, b \in B \} </math>. A vector, <math>\gamma</math> is defined, that contains the coded agreements and disagreements on each characteristic: <math> \gamma \left[ \alpha ( a ), \beta ( b ) \right] = \{ \gamma^{1} \left[ \alpha ( a ) , \beta ( b ) \right] ,..., \gamma^{K} \left[ \alpha ( a ), \beta ( b ) \right] \} </math> where <math>K</math> is a subscript for the characteristics (sex, age, marital status, etc.) in the files. The conditional probabilities of observing a specific vector <math>\gamma</math> given <math>(a, b) \in M</math>, <math>(a, b) \in U</math> are defined as <math> m(\gamma) = P \left\{ \gamma \left[ \alpha (a), \beta (b) \right] | (a,b) \in M \right\} = \sum_{(a, b) \in M} P \left\{\gamma\left[ \alpha(a), \beta(b) \right] \right\} \cdot P \left[ (a, b) | M\right] </math> and <math> u(\gamma) = 