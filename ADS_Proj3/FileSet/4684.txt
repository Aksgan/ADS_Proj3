[[Cost driver]] {{Multicol-break}} * [[Semi variable cost]] * [[Fixed cost]] * [[Total cost]] * [[Contribution margin]] {{Multicol-end}} ==Notes== {{reflist|2}} ==References== *<cite name="Garrison,Noreen,Brewer">{{cite book|last=Garrison|first=Ray H|coauthors=Eric W. Noreen, Peter C. Brewer|title=Managerial Accounting|publisher=McGraw-Hill Irwin|edition=13e|isbn=978-0-07-337961-6|url=http://www.mhhe.com/garrison13e|date=2009}}</cite> [[Category:Costs]] [[Category:Management accounting]] [[Category:Production economics]] [[ar:تكلفة متغيرة]] [[ca:Cost variable]] [[cs:Variabilní náklady]] [[de:Variable Kosten]] [[es:Costo variable]] [[eu:Kostu aldakor]] [[fr:Coût proportionnel]] [[hr:Varijabilni troškovi]] [[he:עלות משתנה]] [[hu:Változó költség]] [[mk:Варијабилни трошоци]] [[nl:Variabele kosten]] [[ja:変動費用]] [[pl:Koszty zmienne]] [[pt:Custeio direto]] [[ru:Переменные затраты]] [[sk:Variabilné náklady]] [[sv:Rörlig kostnad]] [[vi:Chi phí khả biến]] [[zh:變動成本]]</text> </page> <page> <id>39242</id> <title>Variational Bayesian methods</title> <text>'''Variational Bayesian''' methods, also called ''ensemble learning'', are a family of techniques for approximating [[intractable]] [[integral]]s arising in [[Bayesian inference]] and [[machine learning]]. They are typically used in complex [[statistical model]]s consisting of observed variables (usually termed "data") as well as unknown [[parameter]]s and [[latent variable]]s, with various sorts of relationships among the three types of [[random variable]]s, as might be described by a [[graphical model]]. As is typical in Bayesian inference, the parameters and latent variables are grouped together as "unobserved variables". Variational Bayesian methods can provide an analytical approximation to the [[posterior probability]] of the unobserved variables, and also to derive a [[lower bound]] for the [[marginal likelihood]] (i.e. "evidence"{{clarify|date=October 2010}}) of several models (i.e. the [[marginal probability]] of the data given the model, with marginalization performed over unobserved variables), with a view to performing [[model selection]].{{clarify|date=October 2010}} It is an alternative to [[Monte Carlo sampling]] methods for taking a fully Bayesian approach to [[statistical inference]] over complex [[probability distribution|distributions]] that are difficult to directly evaluate or [[sample (statistics)|sample]] from. Variational Bayes can be seen as an extension of the EM ([[expectation-maximization]]) algorithm from [[maximum a posteriori estimation]] (MAP estimation) of the single most probable value of each parameter to fully Bayesian estimation which computes(an approximation to) the entire [[posterior distribution]] of the parameters and latent variables. ==Mathematical derivation== In [[variational]] inference, the posterior distribution over a set of unobserved variables <math>\mathbf{Z} = \{Z_1 \dots Z_n\}</math> given some data <math>\mathbf{X}</math> is approximated by a variational distribution, <math>Q(\mathbf{Z})</math>: :<math>P(\mathbf{Z}|\mathbf{X}) \approx Q(\mathbf{Z}).</math> The distribution <math>Q(\mathbf{Z})</math> is restricted to belong to a family of distributions of simpler form than <math>P(\mathbf{Z}|\mathbf{X})</math>, selected with the intention of making <math>Q(\mathbf{Z})</math> similar to the true posterior, <math>P(\mathbf{Z}|\mathbf{X})</math>. The lack of similarity is measured in terms of a dissimilarity function <math>d(Q; P)</math> and hence inference is performed by selecting the distribution <math>Q(\mathbf{Z})</math> that minimizes <math>d(Q; P)</math>. One choice of dissimilarity function that makes this minimization tractable is the [[Kullback-Leibler divergence]] (KL divergence) of ''P'' from ''Q'', defined as :<math>D_{\mathrm{KL}}(Q || P) = \sum_\mathbf{Z} Q(\mathbf{Z}) \log \frac{Q(\mathbf{Z})}{P(\mathbf{Z}|\mathbf{X})}.</math> The [[logarithm|log]] [[marginal likelihood|evidence]]{{clarify|date=October 2010}} can be written as{{clarify|date=October 2010}} :{| |- |<math>\log P(\mathbf{X})\!</math> |<math>= D_{\mathrm{KL}}(Q||P) - \sum_\mathbf{Z} Q(\mathbf{Z}) \log \frac{Q(\mathbf{Z})}{P(\mathbf{Z},\mathbf{X})}</math> |- | |<math>= D_{\mathrm{KL}}(Q||P) + \mathcal{L}(Q)</math>. |} As the log evidence is fixed with respect to <math>Q</math>, maximising the final term <math>\mathcal{L}(Q)</math> minimizes the KL divergence of <math>P</math> from <math>Q</math>. By appropriate choice of <math>Q</math>, <math>\mathcal{L}(Q)</math> becomes tractable to compute and to maximize. Hence we have both an analytical approximation <math>Q</math> for the posterior <math>P(\mathbf{Z}|\mathbf{X})</math>, and a lower bound <math>\mathcal{L}(Q)</math> for the evidence <math>\log P(\mathbf{X})</math>. ==In practice== The variational distribution <math>Q(\mathbf{Z})</math> is usually assumed to factorize over some [[partition of a set|partition]] of the latent variables, i.e. for some partition of the latent variables <math>\mathbf{Z}</math> into <math>\mathbf{Z}_1 \dots \mathbf{Z}_M</math>, :<math>Q(\mathbf{Z}) = \prod_{i=1}^M q_i(\mathbf{Z}_i|\mathbf{X})</math> It can be shown using the [[calculus of variations]] (hence the name "variational Bayes") that the "best" distribution <math>q_j^{*}</math> for each of the factors <math>q_j</math> (in terms of the distribution minimizing the KL divergence, as described above) can be expressed as: :<math>q_j^{*}(\mathbf{Z}_j|\mathbf{X}) = \frac{e^{\operatorname{E}_{i \neq j} [\ln p(\mathbf{Z}, \mathbf{X})]}}{\int e^{\operatorname{E}_{i \neq j} [\ln p(\mathbf{Z}, \mathbf{X})]}\, d\mathbf{Z}_j}</math> where <math>\operatorname{E}_{i \neq j} [\ln p(\mathbf{Z}, \mathbf{X})]</math> is the [[expected value|expectation]] of the [[joint probability]] of the data and latent variables, taken over all variables not in the partition. In practice, we usually work in terms of logarithms, i.e.: :<math>\ln q_j^{*}(\mathbf{Z}_j|\mathbf{X}) = \operatorname{E}_{i \neq j} [\ln p(\mathbf{Z}, \mathbf{X})] + \text{const.}</math> The constant in the above expression is related to the [[normalizing constant]] (the denominator in the expression above for <math>q_j^{*}</math>) and is usually reinstated by inspection, as the rest of the expression can usually be recognized as being a known type of distribution (e.g. [[Gaussian distribution|Gaussian]], [[gamma distribution|gamma]], etc.). Using the properties of expectations, the expression <math>\operatorname{E}_{i \neq j} [\ln p(\mathbf{Z}, \mathbf{X})]</math> can usually be simplified into a function of the fixed [[hyperparameter]]s of the [[prior distribution]]s over the latent variables and of expectations (and sometimes higher [[moment (mathematics)|moment]]s such as the [[variance]]) of latent variables not in the current partition (i.e. latent variables not included in <math>\mathbf{Z}_j</math>). This creates circular dependencies between the parameters of the distributions over variables in one partition and the expectations of variables in the other partitions. This naturally suggests an [[iterative]] algorithm, much like EM (the [[expectation-maximization]] algorithm), in which the expectations (and possibly higher moments) of the latent variables are initialized in some fashion (perhaps randomly), and then the parameters of each distribution are computed in turn using the current values of the expectations, after which the expectation of the newly computed distribution is set appropriately according to the computed parameters. An algorithm of this sort is guaranteed to [[limit of a sequence|converge]].<ref>S. Boyd and L. Vandenberghe, ''Convex Optimization'', 2004</ref> Furthermore, if the distributions in question are part of the [[exponential family]], which is usually the case, convergence will be to a [[global maximum]], since the exponential family is [[convex function|convex]].<ref>Christopher Bishop, ''Pattern Recognition and Machine Learning'', 2006</ref> In other words, for each of the partitions of variables, by simplifying the expression for the distribution over the partition's variables and examining the distribution's functional dependency on the variables in question, the family of the distribution can usually be determined (which in turn determines the value of the constant). The formula for the distribution's parameters will be expressed in terms of the prior distributions' hyperparameters (which are known constants), but also in terms of expectations of functions of variables in other partitions. Usually these expectations can be simplified into functions of expectations 