a slightly different mindset. It took them time to get to the point where they were able to crack the German codes in a timely fashion. ==U.S. troops== The Americans were relative newcomers to cryptography when they entered the war, but they did have their star players. One was [[Parker Hitt]], who before the war had been an [[Army Signal Corps]] instructor. He was one of the first to try to bring [[United States Army|US Army]] cryptology into the 20th century, publishing an influential short work on the subject in 1915. He was assigned to France in an administrative role, but his advice was eagerly sought by colleagues working in operational cryptology. Another Signal Corps officer who would make his mark on cryptology was [[Joseph Mauborgne]], who in 1914, as a [[First Lieutenant|first lieutenant]], had been the first to publish a solution to the Playfair cipher. When the Americans began moving up to the front in numbers in early 1918, they adopted trench codes and became very competent at their construction, with a [[Captain (land)|Captain]] [[Howard R. Barnes]] eventually learning to produce them at a rate that surprised British colleagues. The Americans adopted a series of codes named after rivers, beginning with "Potomac". They learned to print the codebooks on paper that burned easily and degraded quickly after a few weeks, when the codes would presumably be obsolete, while using a font that was easy to read under trench conditions. ==Communications discipline== However, American codemakers were often frustrated by the inability or refusal of combat units to use the codes—or worse, to use them properly. A soldier engaged in combat doesn't always feel the need to do things "by the book" even when there are very good reasons to do so, and generals on the front line felt that they had other things to worry about. One codemaker suggested that the best way to address the problem was to publicly hang a few offenders, but he lacked the authority to do so. The [[United Kingdom of Great Britain and Ireland|British]] and French were already familiar with such problems in "communications discipline". They hadn't completely solved the problems either, but they had at least managed to get it through the heads of most of their signalmen that if they didn't have time to properly encrypt a message, they shouldn't bother trying; send the message unencrypted, or "in the clear". A partially or badly encrypted message could undermine a cipher or code system, sometimes completely, which made an unencrypted message far preferable. {{DerivedGoebelCrypto}} {{Portal|World War I}} {{World War I}} {{DEFAULTSORT:Trench Code}} [[Category:History of cryptography]] [[Category:Signals intelligence of World War I]]</text> </page> <page> <id>38252</id> <title>Trie</title> <text>[[Image:trie example.svg|thumb|right|250px|A trie for keys "A", "to", "tea", "ted", "ten", "i", "in", and "inn".]] In [[computer science]], a '''trie''', or '''prefix tree''', is an [[ordered tree data structure|ordered tree]] [[data structure]] that is used to store an [[associative array]] where the keys are usually [[string (computer science)|string]]s. Unlike a [[binary search tree]], no node in the tree stores the key associated with that node; instead, its position in the tree shows what key it is associated with. All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the [[string (computer science)|empty string]]. Values are normally not associated with every node, only with leaves and some inner nodes that correspond to keys of interest. The term trie comes from re'''trie'''val. Following the [[etymology]], the inventor, [[Edward Fredkin]], pronounces it {{IPA-en|ˈtriː|}} "tree".<ref name = DADS>{{cite web|url=http://www.nist.gov/dads/HTML/trie.html|title=trie|first=Paul E.|last=Black|date=2009-11-16|work=Dictionary of Algorithms and Data Structures|publisher=[[National Institute of Standards and Technology]]|archiveurl=http://www.webcitation.org/5pqUULy24|archivedate=2010-05-19}}</ref><ref name="Liang1983"/> However, it is pronounced {{IPA-en|ˈtraɪ|}} "try" by other authors.<ref name=DADS/><ref name="Liang1983"/><ref name = KnuthVol3>{{cite book|last=Knuth|first=Donald|authorlink=Donald Knuth|title=The Art of Computer Programming Volume 3: Sorting and Searching|edition=2nd|year=1997|publisher=Addison-Wesley|isbn=0201896850|page=492|chapter=6.3: Digital Searching}}</ref> In the example shown, keys are listed in the nodes and values below them. Each complete English word has an arbitrary integer value associated with it. A trie can be seen as a [[deterministic finite automaton]], although the symbol on each edge is often implicit in the order of the branches. It is not necessary for keys to be explicitly stored in nodes. (In the figure, words are shown only to illustrate how the trie works.) Though it is most common, tries need not be keyed by character strings. The same algorithms can easily be adapted to serve similar functions of ordered lists of any construct, e.g., permutations on a list of digits or shapes. In particular, a '''bitwise trie''' is keyed on the individual bits making up a short, fixed size of bits such as an integer number or pointer to memory. == Advantages relative to other search algorithms == <gallery caption="A series of graphs showing how different algorithms scale with number of items" widths="400px" heights="240px" > File:BitwiseTreesScaling.png|How Fredkin tries scale<br/>(in this case, nedtries which is an '''in-place''' implementation and therefore has a much steeper curve than a dynamic memory based trie implementation) File:RedBlackTreesScaling.png|How red black trees scale<br/>(in this case, the BSD rbtree.h, and clearly showing the classic O(log N) behaviour) File:HashTableScaling.png|How hash tables scale<br/>(in this case, uthash, and when averaged shows the classic O(1) behaviour) </gallery> Unlike most other algorithms, tries have the peculiar feature that the time to insert, or to delete or to find is almost identical because the code paths followed for each are almost identical. As a result, for situations where code is inserting, deleting and finding in equal measure tries can handily beat [[binary search trees]] or even [[hash table]]s, as well as being better for the CPU's instruction and branch caches. The following are the main advantages of tries over [[binary search tree]]s (BSTs): * Looking up keys is faster. Looking up a key of length ''m'' takes worst case [[Big-O notation|O]](''m'') time. A BST performs O([[Logarithm|log]](''n'')) comparisons of keys, where ''n'' is the number of elements in the tree, because lookups depend on the depth of the tree, which is logarithmic in the number of keys if the tree is balanced. Hence in 