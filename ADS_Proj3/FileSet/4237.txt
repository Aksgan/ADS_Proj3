of the last document is reached. On the output, one has what is called a bigrams table. Each row in the table may be called a tuple, storing the first word, and then the second word. Additional characteristics may be stored about each bigram, such as its frequency (the total count of the times the whole bigram was discovered) in whatever body of texts was used to generate the table. The table may also store the frequency of each individual word. For example, this sentence contains bigrams like "for example" and "this sentence". From the table an algorithm can garner the probability of the second word following the first word by assessing each word's frequency and the frequency of the bigrams and perhaps other criteria like the total number of bigrams or the total number of bigrams where the first word is the same. For example, one would be able to state conclusions like: the presence of the word '''post''' indicates a probability of the following word being '''office''' of 50%, because the sequence occurred this way 50% of the time in the text (where '''post''' occurred with and without '''office''' following it). In the bi-gram relationship, the first word can be understood as the context that describes the second word. It qualifies the second word, providing additional insight into its meaning. Humans use the technique of examining the context of words to determine the meaning of written words quite frequently. Words can have multiple meanings, called senses. The bigram-provided context provides a way to distinguish which sense of the word is used (well, most of the time, as literary puns and the like are the obvious exception). As mentioned elsewhere, the stemming algorithm can use this context as additional information to assist in determining the outcome of the algorithm (whether or not the current word is stemmed, which of one or more possible stems is appropriate, whether to use substitution or stripping, whether the word is a verb or noun or some other lexical category, etc). Because the algorithm used probabilities in determining the outcome, it is a stochastic algorithm. Entering into the realm of stochastic algorithms may greatly complicate the stemming algorithm, making it much harder to develop, distribute and maintain. There is considerable debate over its benefits. If a simple suffix stripper can get to about 80% accuracy from a few days or weeks of programming, then how much more valuable is it that a context-driven n-gram stemmer can achieve 90% accuracy given several months of work? There are also many problems with stochastic stemmers. What if the body of text used to derive the probabilities of the bigrams is not a representative sample of all documents ever written in a given language (and it is likely not)? What if the algorithm's bigram table 'becomes corrupted', by learning from 'bad text'? Reconfiguring a brute force algorithm that employs a lookup table is as simple as going in and removing a row from the table. But how does one reconfigure a probabilistic network (this is analogous to some of the problems behind neural networks)? Such obstacles are researched in computer science. ===Hybrid approaches=== Hybrid approaches use two or more of the approaches described above in unison. A simple example is a [[probabilistic suffix tree|suffix tree]] algorithm which first consults a lookup table using brute force. However, instead of trying to store the entire set of relations between words in a given language, the lookup table is kept small and is only used to store a minute amount of "frequent exceptions" like "ran => run". If the word is not in the exception list, apply suffix stripping or lemmatisation and output the result. ===Affix stemmers=== In [[linguistics]], the term [[affix]] refers to either a [[prefix]] or a [[suffix]]. In addition to dealing with suffixes, several approaches also attempt to remove common prefixes. For example, given the word ''indefinitely'', identify that the leading "in" is a prefix that can be removed. Many of the same approaches mentioned earlier apply, but go by the name '''affix stripping'''. ===Matching algorithms=== Such algorithms use a stem database (for example a set of documents that contain stem words). These stems, as mentioned above, are not necessarily valid words themselves (but rather common sub-strings, as the "brows" in "browse" and in "browsing"). In order to stem a word the algorithm tries to match it with stems from the database, applying various constraints, such as on the relative length of the candidate stem within the word (so that, for example, the short prefix "be", which is the stem of such words as "be", "been" and "being", would not be considered as the stem of the word "beside"). ==Language challenges== While much of the early academic work in this area was focused on the English language (with significant use of the Porter Stemmer algorithm), many other languages have been investigated.<ref>[http://www.clef-campaign.org/2007/working_notes/DolamicCLEF2007.pdf] Dolamic, Savoy: Stemming Approaches for East European Languages (CLEF 2007)</ref><ref>[http://portal.acm.org/citation.cfm?doid=1141277.1141523 ] Savoy: Light stemming approaches for the French, Portuguese, German and Hungarian languages (SAC 2006) ISBN 1-59593-108-2</ref><ref>[http://www3.interscience.wiley.com/cgi-bin/abstract/10049677/ABSTRACT?CRETRY=1&SRETRY=0] Popovic & Willett: The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data (1992) Journal of the [[American Society for Information Science]]</ref><ref>[http://staff.science.uva.nl/~mdr/Publications/Files/clef2005-proc-adhoc.pdf] Stemming in Hungarian at CLEF 2005</ref><ref>[http://InformationR.net/ir/12-3/paper315.html] Viera, A.F.G. & Virgil, J.: Uma revisão dos algoritmos de radicalização em língua portuguesa (2007) Information Research, 12(3) paper 315.</ref> Hebrew and Arabic are still considered difficult research languages for stemming. English stemmers are fairly trivial (with only occasional problems, such as "dries" being the third-person singular present form of the verb "dry", "axes" being the plural of "axe" as well as "axis"); but stemmers become harder to design as the morphology, orthography, and character encoding of the target language becomes more complex. For example, an Italian stemmer is more complex than an English one (because of more possible verb inflections), a Russian one is more complex (more possible noun declensions), a Hebrew one is even more complex (due to non-catenative morphology and a writing system without vowels; and the requirement 