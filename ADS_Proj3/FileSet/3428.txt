is increased primarily by shrinking the circuitry, and modern processors are pushing physical size and heat limits. These twin barriers have flipped the equation, making multiprocessing practical even for small systems. The cost or complexity of serial algorithms is estimated in terms of the space (memory) and time (processor cycles) that they take. Parallel algorithms need to optimize one more resource, the communication between different processors. There are two ways parallel processors communicate, shared memory or message passing. [[Shared memory]] processing needs additional [[Lock (computer science)|locking]] for the data, imposes the overhead of additional processor and bus cycles, and also serializes some portion of the algorithm. [[Message passing]] processing uses channels and message boxes but this communication adds transfer overhead on the bus, additional memory need for queues and message boxes and latency in the messages. Designs of parallel processors use special buses like crossbar so that the communication overhead will be small but it is the parallel algorithm that decides the volume of the traffic. Another problem with parallel algorithms is ensuring that they are suitably [[Load balancing (computing)|load balanced]]. For example, checking all numbers from one to a hundred thousand for primality is easy to split amongst processors; however, some processors will get more work to do than the others, which will sit idle until the loaded processors complete. A subtype of parallel algorithms, ''[[distributed algorithms]]'' are algorithms designed to work in [[cluster computing]] and [[distributed computing]] environments, where additional concerns beyond the scope of "classical" parallel algorithms need to be addressed. ==See also== * [[Multiple-agent system]] (MAS) * [[Neural network]] * [[Parallel computing]] ==External links== *[http://www-unix.mcs.anl.gov/dbpp/ Designing and Building Parallel Programs page at the US Argonne National Laboratories] [[Category:Parallel computing]] [[Category:Concurrent algorithms]] [[Category:Distributed algorithms]] [[ar:خوارزمية متوازية]] [[de:Paralleler Algorithmus]] [[es:Algoritmo paralelo]] [[fa:الگوریتم موازی]] [[hu:Párhuzamos algoritmus]] [[ja:並列アルゴリズム]] [[pl:Algorytm równoległy]] [[ru:Параллельный алгоритм]] [[uk:Паралельний алгоритм]]</text> </page> <page> <id>28414</id> <title>Parallel computing</title> <text>{{Programming paradigms}}'''Parallel computing''' is a form of [[computing|computation]] in which many calculations are carried out simultaneously,<ref>Almasi, G.S. and A. Gottlieb (1989). [http://portal.acm.org/citation.cfm?id=1011116.1011127 ''Highly Parallel Computing'']. Benjamin-Cummings publishers, Redwood City, CA.</ref> operating on the principle that large problems can often be divided into smaller ones, which are then solved [[Concurrency (computer science)|concurrently]] ("in parallel"). There are several different forms of parallel computing: [[bit-level parallelism|bit-level]], [[instruction level parallelism|instruction level]], [[data parallelism|data]], and [[task parallelism]]. Parallelism has been employed for many years, mainly in [[high performance computing|high-performance computing]], but interest in it has grown lately due to the physical constraints preventing [[frequency scaling]].<ref>S.V. Adve et al. (November 2008). [http://www.upcrc.illinois.edu/documents/UPCRC_Whitepaper.pdf "Parallel Computing Research at Illinois: The UPCRC Agenda"] (PDF). Parallel@Illinois, University of Illinois at Urbana-Champaign. "The main techniques for these performance benefits – increased clock frequency and smarter but increasingly complex architectures – are now hitting the so-called power wall. The computer industry has accepted that future performance increases must largely come from increasing the number of processors (or cores) on a die, rather than making a single core go faster."</ref> As power consumption (and consequently heat generation) by computers has become a concern in recent years,<ref>Asanovic et al. Old [conventional wisdom]: Power is free, but transistors are expensive. New [conventional wisdom] is [that] power is expensive, but transistors are "free".</ref> parallel computing has become the dominant paradigm in [[computer architecture]], mainly in the form of [[Multi-core (computing)|multicore processor]]s.<ref name="View-Power">Asanovic, Krste et al. (December 18, 2006). [http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.pdf "The Landscape of Parallel Computing Research: A View from Berkeley"] (PDF). University of California, Berkeley. Technical Report No. UCB/EECS-2006-183. "Old [conventional wisdom]: Increasing clock frequency is the primary method of improving processor performance. New [conventional wisdom]: Increasing parallelism is the primary method of improving processor performance ... Even representatives from Intel, a company generally associated with the 'higher clock-speed is better' position, warned that traditional approaches to maximizing performance through maximizing clock speed have been pushed to their limit."</ref> Parallel computers can be roughly classified according to the level at which the hardware supports parallelism—with [[multi-core]] and [[Symmetric multiprocessing|multi-processor]] computers having multiple processing elements within a single machine, while [[Computer cluster|clusters]], [[Massive parallel processing|MPPs]], and [[Grid computing|grids]] use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks. [[Parallel algorithm|Parallel computer programs]] are more difficult to write than sequential ones,<ref>[[David A. Patterson (scientist)|Patterson, David A.]] and [[John L. Hennessy]] (1998). ''Computer Organization and Design'', Second Edition, Morgan Kaufmann Publishers, p. 715. ISBN 1558604286.</ref> because concurrency introduces several new classes of potential [[software bug]]s, of which [[race condition]]s are the most common. [[Computer networking|Communication]] and [[Synchronization (computer science)|synchronization]] between the different subtasks are typically one of the greatest obstacles to getting good parallel program performance. The [[speedup|speed-up]] of a program as a result of parallelization is observed as [[Amdahl's law]]. ==Background== Traditionally, computer software has been written for serial computation. To solve a problem, an [[algorithm]] is constructed and implemented as a serial stream of instructions. These instructions are executed on a [[central processing unit]] on one computer. Only one instruction may execute at a time—after that instruction is finished, the next is executed.<ref name="llnltut">{{cite web |url=http://www.llnl.gov/computing/tutorials/parallel_comp/ |title=Introduction to Parallel Computing |accessdate=2007-11-09 |author=Barney, Blaise |publisher=Lawrence Livermore National Laboratory}}</ref> Parallel computing, on the other hand, uses multiple processing elements simultaneously to solve a problem. This is accomplished by breaking the problem into independent parts so that each processing element can execute its part of the algorithm simultaneously with the others. The processing elements can be diverse and include resources such as a single computer with multiple processors, several networked computers, specialized hardware, or any combination of the above.<ref name="llnltut" /> [[Frequency scaling]] was the dominant reason for improvements in computer performance from the mid-1980s until 2004. The runtime of a program is equal to the number of instructions multiplied by the average time per instruction. Maintaining everything else constant, increasing the clock frequency decreases the average time it takes to execute an instruction. An increase in frequency thus decreases runtime for all [[CPU bound|computation-bounded]] programs.<ref>[[John L. Hennessy|Hennessy, John L.]] and [[David A. Patterson (scientist)|David A. Patterson]] (2002). ''Computer 