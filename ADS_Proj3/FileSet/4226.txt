Testing the distributional hypothesis: The influence of context on judgements of semantic similarity]. In Proceedings of the 23rd Annual Conference of the Cognitive Science Society, pages 611-616. * Pantel, P., and Lin, D. (2002). Discovering word senses from text. In ''Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining'', pages 613–619. * Terra, E., and Clarke, C.L.A. (2003). Frequency estimates for statistical word similarity measures. In ''Proceedings of the Human Language Technology and North American Chapter of Association of Computational Linguistics Conference 2003 (HLT/NAACL 2003)'', pages 244–251. * Turney, P.D. (2000). Learning algorithms for keyphrase extraction. ''Information Retrieval'', 2(4), 303-336. [http://www.citebase.org/abstract?id=oai:arXiv.org:cs/0212020 OAI arXiv.org:cs/0212020] * Turney, P.D. (2001). Answering subcognitive Turing Test questions: A reply to French. ''Journal of Experimental and Theoretical Artificial Intelligence'', 13(4), 409-419. [http://www.citebase.org/abstract?id=oai:arXiv.org:cs/0212015 OAI arXiv.org:cs/0212015] * Turney, P.D. (2003). Coherent keyphrase extraction via Web mining, In ''Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03)'', Acapulco, Mexico, 434-439. [http://www.citebase.org/abstract?id=oai:arXiv.org:cs/0308033 OAI arXiv.org:cs/0308033] * Turney, P.D. (2004). Word sense disambiguation by Web mining for word co-occurrence probabilities. In ''Proceedings of the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (SENSEVAL-3)'', Barcelona, Spain, pp. 239–242. [http://www.citebase.org/abstract?id=oai:arXiv.org:cs/0407065 OAI arXiv.org:cs/0407065] * Turney, P.D. (2006), Similarity of semantic relations. ''Computational Linguistics'', 32(3), 379-416. [http://www.citebase.org/abstract?id=oai:arXiv.org:cs/0608100 OAI arXiv.org:cs/0608100] * Turney, P.D., and Littman, M.L. (2003). Measuring praise and criticism: Inference of semantic orientation from association, ''ACM Transactions on Information Systems (TOIS)'', 21(4), 315-346. [http://www.citebase.org/abstract?id=oai:arXiv.org:cs/0309034 OAI arXiv.org:cs/0309034] * Turney, P.D., and Littman, M.L. (2005). Corpus-based learning of analogies and semantic relations. ''Machine Learning'', 60(1–3):251–278. [http://www.citebase.org/abstract?id=oai:arXiv.org:cs/0508103 OAI arXiv.org:cs/0508103] * Turney, P.D., Littman, M.L., Bigham, J., and Shnayder, V. (2003). Combining independent modules to solve multiple-choice synonym and analogy problems. In ''Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-03)'', Borovets, Bulgaria, pp. 482–489. [http://www.citebase.org/abstract?id=oai:arXiv.org:cs/0309035 OAI arXiv.org:cs/0309035] * Weaver, W. (1955). Translation. In W.N. Locke and D.A. Booth (eds.), ''Machine Translation of Languages'', Cambridge, MA: MIT Press. ISBN 0-8371-8434-7 {{DEFAULTSORT:Statistical Semantics}} [[Category:Artificial intelligence applications]] [[Category:Computational linguistics]] [[Category:Information retrieval]] [[Category:Semantics]] [[Category:Statistical natural language processing]] [[Category:Fields of application of statistics]] [[bg:Статистическа семантика]] [[eo:Statistika semantiko]]</text> </page> <page> <id>35663</id> <title>Statistical significance</title> <text>In [[statistics]], a result is called '''statistically significant''' if it is unlikely to have occurred by [[Randomness|chance]]. The phrase ''[[Statistical hypothesis testing|test of significance]]'' was coined by [[Ronald Fisher]].<ref name="Fisher1925">"Critical tests of this kind may be called tests of significance, and when such tests are available we may discover whether a second sample is or is not significantly different from the first." — R. A. Fisher (1925). ''Statistical Methods for Research Workers'', Edinburgh: Oliver and Boyd, 1925, p.43.</ref> As used in statistics, ''significant'' does not mean ''important'' or ''meaningful'', as it does in everyday speech. For example, a study that included tens of thousands of participants might be able to say with great [[Confidence interval#Meaning of the term "confidence"|confidence]] that residents of one city were more intelligent than people of another city by 1/20 of an [[Intelligence quotient|IQ point]]. This result would be statistically significant, but the difference is small enough to be utterly unimportant. Many researchers urge that tests of significance should always be accompanied by [[Effect size|effect-size]] statistics, which approximate the size and thus the practical importance of the difference. The amount of evidence required to accept that an event is unlikely to have arisen by chance is known as the '''significance level''' or critical [[p-value]]: in traditional [[Ronald Fisher|Fisherian]] [[statistical hypothesis testing]], the p-value is the probability of observing data at least as extreme as that observed, ''given that the null hypothesis is true''. If the obtained p-value is small then it can be said either the [[null hypothesis]] is false or an unusual event has occurred. It is worth stressing that p-values do not have any repeat sampling interpretation. An alternative statistical hypothesis testing framework is the [[Neyman–Pearson lemma|Neyman–Pearson]] frequentist school which requires both a null and an alternative hypothesis to be defined and investigates the repeat sampling properties of the procedure, i.e. the probability that a decision to reject the null hypothesis will be made when it is in fact true and should not have been rejected (this is called a "false positive" or [[Type I error]]) and the probability that a decision will be made to accept the null hypothesis when it is in fact false ([[Type II error]]). More typically, the significance level of a test is such that the probability of mistakenly rejecting the null hypothesis is ''no more than'' the stated probability. This allows the test to be performed using non-significant statistics which has the advantage of reducing the computational burden while wasting some information. It is worth stressing that Fisherian p-values are philosophically different from Neyman–Pearson Type I errors. This confusion is unfortunately propagated by many statistics textbooks.<ref>Raymond Hubbard, M.J. Bayarri, ''[http://ftp.isds.duke.edu/WorkingPapers/03-26.pdf P Values are not Error Probabilities]''. A working paper that explains the difference between Fisher's evidential p-value and the Neyman–Pearson Type I error rate <math>\alpha</math>.</ref> == Use in practice == The significance level is usually denoted by the Greek symbol α ([[Alpha (letter)|lowercase alpha]]). Popular levels of significance are 10% (0.1), 5% (0.05), 1% (0.01) and 0.1% (0.001). If a [[Statistical hypothesis testing|''test of significance'']] gives a p-value lower than the α-level, the null hypothesis is thus rejected. Such results are informally referred to as 'statistically significant'. For example, if someone argues that "there's only one chance in a thousand this could have happened by coincidence," a 0.001 level of statistical significance is being implied. The lower the significance level, the stronger the evidence required. Choosing level of significance is an arbitrary task, but for many applications, a level of 5% is chosen, for no better reason than that it is conventional.<ref>{{cite journal|author=Stigler S|title=Fisher and the 5% level|journal=Chance|volume=21|issue=4|year=2008|page=12|doi=10.1007/s00144-008-0033-3}}</ref><ref>{{cite book|author=Fisher RA|year=1925|title=Statistical Methods for Research Workers|edition=first|location=Edinburgh|publisher=Oliver & Boyd}}</ref> In some situations it is convenient to express the statistical significance as 1 &minus; α. In general, when interpreting a stated significance, one must be careful to note what, precisely, is being tested statistically. Different α-levels trade off countervailing 