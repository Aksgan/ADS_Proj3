''randomly'' selected points to serve as the cut line. Practically this technique often results in nicely balanced trees. Given a list of ''n'' points, the following [[algorithm]] will construct a balanced ''k''d-tree containing those points. '''function''' kdtree (''list of points'' pointList, ''int'' depth) { '''if''' pointList '''is empty''' '''return''' '''nil'''; '''else''' { ''// Select axis based on depth so that axis cycles through all valid values'' '''var''' ''int'' axis := depth '''mod''' k; ''// Sort point list and choose median as pivot element'' '''[[Selection algorithm|select]]''' median '''by''' axis '''from''' pointList; ''// Create node and construct subtrees'' '''var''' ''tree_node'' node; node.location := median; node.leftChild := kdtree(points '''in''' pointList '''before''' median, depth+1); node.rightChild := kdtree(points '''in''' pointList '''after''' median, depth+1); '''return''' node; } } It is common that points "after" the median include only ones that are greater than or equal to the median. Another approach is to define a "superkey" function that compares the points in other dimensions. Lastly, it may be acceptable to let points equal to the median lie on either side. This algorithm implemented in the [[Python (programming language)|Python programming language]] is as follows: <source lang="python"> class Node:pass def kdtree(pointList, depth=0): if not pointList: return # Select axis based on depth so that axis cycles through all valid values k = len(pointList[0]) # assumes all points have the same dimension axis = depth % k # Sort point list and choose median as pivot element pointList.sort(key=lambda point: point[axis]) median = len(pointList) // 2 # choose median # Create node and construct subtrees node = Node() node.location = pointList[median] node.leftChild = kdtree(pointList[0:median], depth+1) node.rightChild = kdtree(pointList[median+1:], depth+1) return node </source> [[Image:Kdtree 2d.svg|right|thumb|300px|The resulting ''k''d-tree decomposition.]] [[Image:Tree 0001.svg|right|thumb|300px|The resulting ''k''d-tree.]] Example usage would be: pointList = [(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)] tree = kdtree(pointList) The tree generated is shown on the right. This algorithm creates the [[invariant (computer science)|invariant]] that for any node, all the nodes in the left [[subtree]] are on one side of a splitting [[plane (mathematics)|plane]], and all the nodes in the right subtree are on the other side. Points that lie on the splitting plane may appear on either side. The splitting plane of a node goes through the point associated with that node (referred to in the code as ''node.location''). ===Adding elements === {{Expand section|date=November 2008}} One adds a new point to a ''k''d-tree in the same way as one adds an element to any other [[binary search tree|search tree]]. First, traverse the tree, starting from the root and moving to either the left or the right child depending on whether the point to be inserted is on the "left" or "right" side of the splitting plane. Once you get to the node under which the child should be located, add the new point as either the left or right child of the leaf node, again depending on which side of the node's splitting plane contains the new node. Adding points in this manner can cause the tree to become unbalanced, leading to decreased tree performance. The rate of tree performance degradation is dependent upon the spatial distribution of tree points being added, and the number of points added in relation to the tree size. If a tree becomes too unbalanced, it may need to be re-balanced to restore the performance of queries that rely on the tree balancing, such as nearest neighbour searching. ===Removing elements=== To remove a point from an existing ''k''d-tree, without breaking the invariant, the easiest way is to form the set of all nodes and leaves from the children of the target node, and recreate that part of the tree. Another approach is to find a replacement for the point removed.<ref>Chandran, Sharat. [http://www.cs.umd.edu/class/spring2002/cmsc420-0401/pbasic.pdf Introduction to kd-trees]. University of Maryland Department of Computer Science.</ref> First, find the node R that contains the point to be removed. For the base case where R is a leaf node, no replacement is required. For the general case, find a replacement point, say p, from the sub-tree rooted at R. Replace the point stored at R with p. Then, recursively remove p. For finding a replacement point, if R discriminates on x (say) and R has a right child, find the point with the minimum x value from the sub-tree rooted at the right child. Otherwise, find the point with the maximum x value from the sub-tree rooted at the left child. ===Balancing=== Balancing a ''k''d-tree requires care. Because ''k''d-trees are sorted in multiple dimensions, the [[tree rotation]] technique cannot be used to balance them &mdash; this may break the invariant. Several variants of balanced kd-tree exists. They include divided kd-tree, pseudo kd-tree, K-D-B-tree, hB-tree and Bkd-tree. Many of these variants are [[adaptive k-d tree]]. ===Nearest neighbor search=== <!-- Incomplete and wafty description of the KD-NN-algorithm --> [[Image:KDTree-animation.gif|thumb|300px|Animation of NN searching with a KD Tree in 2D]] The nearest neighbor (NN) algorithm aims to find the point in the tree which is nearest to a given input point. This search can be done efficiently by using the tree properties to quickly eliminate large portions of the search space. Searching for a nearest neighbor in a ''k''d-tree proceeds as follows: # Starting with the root node, the algorithm moves down the tree recursively, in the same way that it would if the search point were being inserted (i.e. it goes right or left depending on whether the point is greater or less than the current node in the split dimension). # Once the algorithm reaches a leaf node, it saves that node point as the "current best" # The algorithm unwinds the recursion of the tree, performing the following steps at each node: ## If the current node is closer than the current best, then it becomes the current best. ## The algorithm checks whether there could be any points on the other side of the splitting plane that are closer to the search point than the current best. In concept, this is done by intersecting the splitting [[hyperplane]] with a [[hypersphere]] around the search 