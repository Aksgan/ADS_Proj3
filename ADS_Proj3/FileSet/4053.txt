1 & 0\\ 0 & 1 & 0 & 0\\ 0 & 0 & 0 & -1\\ 1 & 0 & 0 & 0\end{bmatrix} \cdot \begin{bmatrix} 0 & 0 & 0 & 1\\ 0 & 1 & 0 & 0\\ 1 & 0 & 0 & 0\\ 0 & 0 & -1 & 0\end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0\\ 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 1\end{bmatrix} </math> and :<math> \begin{bmatrix} 0 & 0 & \sqrt{0.2} & 0 & -\sqrt{0.8}\\ 1 & 0 & 0 & 0 & 0\\ 0 & 1 & 0 & 0 & 0\\ 0 & 0 & 0 & 1 & 0\\ 0 & 0 & \sqrt{0.8} & 0 & \sqrt{0.2} \end{bmatrix} \cdot \begin{bmatrix} 0 & 1 & 0 & 0 & 0\\ 0 & 0 & 1 & 0 & 0\\ \sqrt{0.2} & 0 & 0 & 0 & \sqrt{0.8}\\ 0 & 0 & 0 & 1 & 0\\ -\sqrt{0.8} & 0 & 0 & 0 & \sqrt{0.2}\end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 & 0\\ 0 & 1 & 0 & 0 & 0\\ 0 & 0 & 1 & 0 & 0\\ 0 & 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 0 & 1\end{bmatrix}. </math> It should also be noted that this particular singular value decomposition is not unique. Choosing <math>V</math> such that :<math> V^* = \begin{bmatrix} 0 & 1 & 0 & 0 & 0\\ 0 & 0 & 1 & 0 & 0\\ \sqrt{0.2} & 0 & 0 & 0 & \sqrt{0.8}\\ \sqrt{0.4} & 0 & 0 & \sqrt{0.5} & -\sqrt{0.1}\\ -\sqrt{0.4} & 0 & 0 & \sqrt{0.5} & \sqrt{0.1} \end{bmatrix} </math> is also a valid singular value decomposition. == Singular values, singular vectors, and their relation to the SVD == A non-negative real number σ is a '''[[singular value]]''' for ''M'' if and only if there exist unit-length vectors ''u'' in ''K''<sup>''m''</sup> and ''v'' in ''K''<sup>''n''</sup> such that :<math>Mv = \sigma u \,\mbox{ and } M^{*} u = \sigma v. \,\!</math> The vectors ''u'' and ''v'' are called '''left-singular''' and '''right-singular vectors''' for σ, respectively. In any singular value decomposition :<math>M = U\Sigma V^{*} \,\!</math> the diagonal entries of Σ are equal to the singular values of ''M''. The columns of ''U'' and ''V'' are, respectively, left- and right-singular vectors for the corresponding singular values. Consequently, the above theorem states that: * An ''m'' &times; ''n'' matrix ''M'' has at least one and at most ''p'' = min(''m'',''n'') distinct singular values. * It is always possible to find a unitary basis for ''K''<sup>''m''</sup> consisting of left-singular vectors of ''M''. * It is always possible to find a unitary basis for ''K''<sup>''n''</sup> consisting of right-singular vectors of ''M''. A singular value for which we can find two left (or right) singular vectors that are linearly independent is called ''degenerate''. Non-degenerate singular values always have unique left and right singular vectors, up to multiplication by a unit phase factor ''e''<sup>'''i'''φ</sup> (for the real case up to sign). Consequently, if all singular values of ''M'' are non-degenerate and non-zero, then its singular value decomposition is unique, up to multiplication of a column of ''U'' by a unit phase factor and simultaneous multiplication of the corresponding column of ''V'' by the same unit phase factor. Degenerate singular values, by definition, have non-unique singular vectors. Furthermore, if ''u''<sub>1</sub> and ''u''<sub>2</sub> are two left-singular vectors which both correspond to the singular value σ, then any normalized linear combination of the two vectors is also a left singular vector corresponding to the singular value σ. The similar statement is true for right singular vectors. Consequently, if ''M'' has degenerate singular values, then its singular value decomposition is not unique. ==Applications of the SVD== ===Pseudoinverse=== The singular value decomposition can be used for computing the [[Moore-Penrose pseudoinverse|pseudoinverse]] of a matrix. Indeed, the pseudoinverse of the matrix ''M'' with singular value decomposition <math> M = U\Sigma V^*</math> is :<math> M^+ = V \Sigma^+ U^*, \,</math> where Σ<sup>+</sup> is the pseudoinverse of Σ, which is formed by replacing every nonzero entry by its [[Multiplicative inverse|reciprocal]] and transposing the resulting matrix. The pseudoinverse is one way to solve [[linear least squares]] problems. ===Solving homogeneous linear equations=== A set of [[homogeneous linear equation]]s can be written as <math> \mathbf{A} \, \mathbf{x} = \mathbf{0} </math> for a matrix <math> \mathbf{A} </math> and vector <math> \mathbf{x} </math>. A typical situation is that <math> \mathbf{A} </math> is known and a non-zero <math> \mathbf{x} </math> is to be determined which satisfies the equation. Such an <math> \mathbf{x} </math> belongs to <math> \mathbf{A} </math>'s [[Kernel (matrix)|null space]] and is sometimes called a (right) null vector of <math> \mathbf{A} </math>. <math> \mathbf{x} </math> can be characterized as a right singular vector corresponding to a singular value of <math> \mathbf{A} </math> that is zero. This observation means that if <math> \mathbf{A} </math> is a [[square matrix]] and has no vanishing singular value the equation has no non-zero <math> \mathbf{x} </math> as a solution. It also means that if there are several vanishing singular values, any linear combination of the corresponding right singular vectors is a valid solution. Analogously to the definition of a (right) null vector, a non-zero <math> \mathbf{x} </math> satisfying <math> \mathbf{x}^* \, \mathbf{A} = \mathbf{0} </math>, with <math> \mathbf{x}^*</math> denoting the conjugate transpose of <math> \mathbf{x} </math>, is called a left null vector of <math> \mathbf{A} </math>. ===Total least squares minimization=== A [[total least squares]] problem refers to determining the vector <math> \mathbf{x} </math> which minimizes the [[Vector_norm#p-norm|2-norm]] of a vector <math> \mathbf{A} \, \mathbf{x} </math> under the constraint <math> \| \mathbf{x} \| = 1 </math>. The solution turns out to be the right singular vector of <math> \mathbf{A} </math> corresponding to the smallest singular value. ===Range, null space and rank=== Another application of the SVD is that it provides an explicit representation of the [[Column space|range]] and [[null 