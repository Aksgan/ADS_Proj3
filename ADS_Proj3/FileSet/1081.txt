were not smokers, 40 people—4 percent—also went into remission with no chemotherapy. The data, as it stands, suggests that smokers are as likely as non-smokers to go into remission without chemotherapy. The result is not what the researchers desire, so they reduce the sample size to 1,000 patients, to see if that produces different results. The new data retains the 90 percent smoker rate (900). In this sample, 36 people—about 4 percent—go into remission without chemotherapy. However, the new sample of non-smoking patients (100) retains 16 of the 40 people from the original sample who went into remission without chemotherapy. That is 16 percent of the new sample size. The researchers therefore claim that non-smokers with lung cancer are four times more likely to go into remission without chemotherapy than smokers are. By reducing the sample size without regard to [[statistical significance]], after the original sample suggested there is no difference in untreated remission rates, the researchers have produced numbers that seem to bear out the desired result. ===Example in meteorology=== In [[meteorology]], dataset A is often weather data up to the present, which ensures that, even subconsciously, subset B of the data could not influence the formulation of the hypothesis. Of course, such a discipline necessitates waiting for new data to come in, to show the formulated theory's [[predictive power]] versus the [[null hypothesis]]. This process ensures that no one can accuse the researcher of hand-tailoring the predictive model to the data on hand, since the upcoming weather is not yet available. Suppose that observers note that a particular town appears to be a [[cancer cluster]], but lack a firm hypothesis of why this is so. However, they have access to a large amount of [[demographic data]] about the town and surrounding area, containing measurements for the area of hundreds or thousands of different variables, mostly uncorrelated. Even if all these variables are independent of the cancer incidence rate, it is highly likely that at least one variable will be significantly correlated with the cancer rate across the area. While this may suggest a hypothesis, further testing using the same variables but with data from a different location is needed to confirm. Note that a [[p-value]] of 0.01 suggests that 1% of the time a result at least that extreme would be obtained by chance; if hundreds or thousands of hypotheses (with mutually relatively uncorrelated independent variables) are tested, then one is more likely than not to get at least one null hypothesis with a p-value less than 0.01. ==Remedies== The practice of looking for patterns in data is legitimate; the vice of applying a statistical test of significance (hypothesis testing) to the same data from which the pattern was learned is wrong. One way to construct hypotheses while avoiding the problems of data dredging is to conduct randomized out-of-sample tests. The researcher collects a data set, then randomly partitions it into two subsets, A and B. Only one subset - say, subset A - is examined for creating hypotheses. Once a hypothesis has been formulated, it must be tested on subset B, which was not used to construct the hypothesis. Only where such a hypothesis is ''also'' supported by B is it reasonable to believe that the hypothesis might be valid. Another remedy for data dredging is to record the number of all significance tests conducted during the experiment and simply multiply the final significance level by this number (the [[Bonferroni correction]]); however, this is a very conservative metric. The use of a [[false discovery rate]] is a more sophisticated approach that has become a popular method for control of multiple hypothesis tests. Ultimately, the statistical significance of a test and the statistical confidence of a finding are joint properties of data and the method used to examine the data. Thus, if someone says that a certain event has probability of 20% ± 2% 19 times out of 20, this means that if the probability of the event is estimated ''by the same method'' used to obtain the 20% estimate, the result will be between 18% and 22% with probability 0.95. No claim of statistical significance can be made by only looking, without due regard to the method used to assess the data. ==See also== * [[Base rate fallacy]] * [[Bonferroni inequalities]] * [[False discovery rate]] * [[Multiple comparisons]] * [[Pareidolia]] * [[Predictive analytics]] ==References== {{reflist}} * {{Cite journal |last=Ioannidis |first=John P.A. |authorlink=John P. A. Ioannidis |title=Why Most Published Research Findings Are False |journal=[[PLoS Medicine]] |volume=2 |issue=8 |pages=e124 |publisher=Public Library of Science |location=San Francisco |date=August 30, 2005 |url=http://www.plosmedicine.org/article/info%3Adoi%2F10.1371%2Fjournal.pmed.0020124 |issn=1549-1277 |doi=10.1371/journal.pmed.0020124 |accessdate= 2009-11-29 |pmid=16060722}} ==External links== * [http://data-snooping.martinsewell.com/ A bibliography on data-snooping bias] {{DEFAULTSORT:Data Dredging}} [[Category:Bias]] [[Category:Cognitive biases]] [[Category:Data mining]] [[Category:Experimental design]] [[Category:Hypothesis testing]]</text> </page> <page> <id>9364</id> <title>Data drilling</title> <text>{{Otheruses|drill down}} {{unreferenced|date=February 2008}} '''Data drilling''' (also '''drill-down''') refers to any of various operations and transformations on tabular, relational, and multidimensional data. The term has widespread use in various contexts, but is primarily associated with specialized [[software]] designed specifically for [[data analysis]]. == Common data drilling operations == There are certain operations that are common to applications that allow data drilling. Among them are: '''[[Query]] operations''': * tabular query * pivot query === Tabular query === Tabular query operations consist of standard operations on data tables. Among these operations are: * search * sort * filter (by value) * filter (by extended function or condition) * transform (e.g., by adding or removing columns) Consider the following example: '''Fred and Wilma table (Fig 001)''': gender , fname , lname , home male , fred , chopin , Poland male , fred , flintstone , bedrock male , fred , durst , usa female , wilma , flintstone , bedrock female , wilma , rudolph , usa female , wilma , webb , usa male , fred , johnson , usa The preceding is an example of a simple flat file table formatted as comma-separated values. The table includes first name, last name, gender and home country for various people named fred 