standard and [[API]], lib[[ZPAQ]], which is released into [[Public domain]], for the creation of data compressor, [[PAQ]], which is released in [[GPL]]v3, and won the [[Hutter prize]], and many other compression tools. He is working as a technology strategist at Ocarina Networks, (a division of Dell) specializing in data compression research. He is a researcher on [[Data compression]]. ==Related articles== [[Phil Katz]] <br /> [[ZPAQ]] <br /> [[PAQ]] <br /> [[Data compression]] ==References== [[http://mattmahoney.net/]] Matt mahoney's Website. [[Category:Data compression]]</text> </page> <page> <id>23770</id> <title>Matt Woolsey</title> <text>{{BLP unsourced|date=July 2010}} {{Orphan|date=February 2009}} Also '''Matthew Woolsey,''' born December 29, 1982 in [[San Francisco]] is a [[writer]], new media programmer and designer, and consultant. Woolsey's writing on topics including [[finance]], [[economic development]], [[drug trafficking]], [[politics]], book reviews, [[baseball]], [[real estate]] and [[biomechanics]] has appeared [[Forbes Magazine]], [[ABC News]], [[Moscow Times]], [[Los Angeles Times]], [[International Herald Tribune]], [[San Francisco Chronicle]], [[USA Today]], [[Forbes Asia]], [[AOL]], [[Yahoo!]], [[MSN]] & [[Rediff]]. Academic papers on the subject of [[new media]], [[education]] and [[digital literacy]] have appeared in [[Educause]]'s ''ECAR Journal For Applied Research'' and [[Ohio State University]]'s ''Theory into Practice'' journal, published through the College of Education and Human Ecology. Woolsey is a director of the [[New Media Consortium]], an international consulting company founded in 1993, which consists of more than 250 colleges, universities, museums, corporations, and other learning-focused organizations, including [[Yale University]], [[Princeton University]], [[University of Michigan]], [[Apple Computer]], [[Adobe Systems]], [[SF MoMA]] et al. dedicated to the exploration and use of new media and new technologies. His design and programming work with True North Studios, based in [[Bainbridge Island, Washington]] for [[Sun Microsystems]], [[U.S. Library of Congress]], [[The Walt Disney Company|Walt Disney Studios]] and [[Sybase]], has earned awards from [[Macromedia]], [[MacHome Magazine]] and [[Oppenheim Toy Portfolio]]. {{Persondata <!-- Metadata: see [[Wikipedia:Persondata]]. --> | NAME =Woolsey, Matt | ALTERNATIVE NAMES = | SHORT DESCRIPTION = | DATE OF BIRTH = 1982 | PLACE OF BIRTH = | DATE OF DEATH = | PLACE OF DEATH = }} {{DEFAULTSORT:Woolsey, Matt}} [[Category:1982 births]] [[Category:Technology writers]] [[Category:Living people]]</text> </page> <page> <id>23774</id> <title>Matthew Hennessy</title> <text>'''Matthew Hennessy''' is an Irish [[computer scientist]] who has contributed especially to [[concurrency (computer science)]], [[process calculi]] and programming language [[semantics]]. Hennessy was Professor of [[Computer Science]] at the Department of Informatics, [[University of Sussex]], [[England]], until 2008.<ref>[http://www.informatics.sussex.ac.uk/users/matthewh/ Matthew Hennessy home page], [[University of Sussex]], UK.</ref> Since then, Hennessy has held a professorship at the Department of [[Computer Science]], [[Trinity College, Dublin]]. Hennessy's research interests are in the area of the semantic foundations of [[programming language|programming]] and [[specification language]]s, particularly involving [[distributed computing]], including [[mobile computing]].<ref>{{dblp name| id=h/Hennessy:Matthew | name=Matthew Hennessy}}</ref> He also has an interest in [[verification]] tools. == Books == Matthew Hennessy has written a number of books: * Hennessy, Matthew. ''A Distributed Pi-Calculus''. [[Cambridge University Press]], Cambridge, UK, 2007. ISBN 0-521-87330-4. * Hennessy, Matthew. ''Algebraic Theory of Processes''. [[The MIT Press]], Cambridge, Massachusetts, 1988. ISBN 0-262-58093-4. * Hennessy, Matthew. ''The Semantics of Programming Languages: An Elementary Introduction using Structural Operational Semantics''. [[John Wiley and Sons]], New York, 1990. ISBN 0-471-92772-4. ==References== {{reflist}} ==External links== * [https://www.cs.tcd.ie/Matthew.Hennessy/ Matthew Hennessy Trinity College Dublin home page] {{Persondata <!-- Metadata: see [[Wikipedia:Persondata]]. --> | NAME = Hennessy, Matthew | ALTERNATIVE NAMES = | SHORT DESCRIPTION = | DATE OF BIRTH = | PLACE OF BIRTH = | DATE OF DEATH = | PLACE OF DEATH = }} {{DEFAULTSORT:Hennessy, Matthew}} [[Category:Year of birth missing (living people)]] [[Category:Irish computer scientists]] [[Category:Formal methods people]] [[Category:Computer science writers]] [[Category:Academics of the University of Sussex]] [[Category:Fellows of Trinity College, Dublin]] [[Category:Living people]] {{Ireland-scientist-stub}} {{Compu-bio-stub}}</text> </page> <page> <id>23792</id> <title>Maverick.NET</title> <text>'''Maverick.NET''' is a [[Maverick_Framework|Maverick]] port from [[Java (software platform)|Java]] to [[C Sharp (programming language)|C#]] for its integration in the [[.NET Framework|.NET platform]]. As many other similar tools targeted to this development platform, Maverick.NET also works with [[Mono (software)|Mono]]. ==See also== *[[Maverick_Framework|Maverick]] *[[Struts]] ==External links== *[http://mavnet.sourceforge.net/ Official project page] {{software-stub}} [[Category:.NET framework]] [[Category:Free software programmed in C Sharp]] [[es:Maverick.NET]]</text> </page> <page> <id>23817</id> <title>Maximum entropy probability distribution</title> <text>{{Refimprove|date=August 2009}} In [[statistics]] and [[information theory]], a '''maximum entropy probability distribution''' is a [[probability distribution]] whose [[information entropy|entropy]] is at least as great as that of all other members of a specified class of distributions. According to the [[principle of maximum entropy]], if nothing is known about a distribution except that it belongs to a certain class, then the distribution with the largest entropy should be chosen as the default. The motivation is twofold: first, maximizing entropy minimizes the amount of prior information built into the distribution; second, many physical systems tend to move towards maximal entropy configurations over time. == Definition of entropy == {{further|[[Entropy (information theory)]]}} If ''X'' is a [[discrete random variable]] with distribution given by :<math>\operatorname{Pr}(X=x_k) = p_k \quad\mbox{ for } k=1,2,\ldots</math> then the entropy of ''X'' is defined as :<math>H(X) = - \sum_{k\ge 1}p_k\log p_k .</math> If ''X'' is a [[continuous random variable]] with [[probability density]] ''p''(''x''), then the entropy of ''X'' is sometimes defined as.<ref>Williams, D. (2001) ''Weighing the Odds'' Cambridge UP ISBN 0-521-00618-x (pages 197-199)</ref><ref>Bernardo, J.M., Smith, A.F.M. (2000) ''Bayesian Theory'.' Wiley. ISBN 0-471-49464-x (pages 209, 366)</ref><ref>O'Hagan, A. (1994) ''Kendall's Advanced Theory of statistics, Vol 2B, Bayesian Inference'', Edward Arnold. ISBN 0-340-52922-9 (Section 5.40)</ref> :<math>H(X) = - \int_{-\infty}^\infty p(x)\log p(x) dx</math> where ''p''(''x'') log ''p''(''x'') is understood to be zero whenever ''p''(''x'') = 0. In connection with maximimum entropy distributions, this form of definition is often the only one given, or at least it is taken as the standard form. However, it is recognisable as the special case ''m''=1 of the more general definition :<math>H^c(p(x)\|m(x)) = -\int p(x)\log\frac{p(x)}{m(x)}\,dx,</math> which is discussed in the articles [[Entropy (information theory)]] and [[Principle of maximum entropy]]. The base of the [[logarithm]] is not important as long as the same one is used consistently: change of base merely results in a rescaling of the entropy. Information theoreticians may prefer to use base 2 in order to express the entropy in [[bit]]s; mathematicians and physicists will often prefer the [[natural logarithm]], resulting in a unit of [[Nat (information)|nat]]s or 