example it was possible to build a LOCUS cluster containing both PDP-11/45 and VAX 750 machines, but instruction sets used were not identical, so two versions of each object program would be needed<ref group="note">Rather like Apple [[Fat binary]] files</ref> The solution was to replace the files that needed to be different on a per node basis by special hidden directories. These directories would then contain the different versions of the file. When a user accessed one of these hidden directories the system would check the users ''context'' and open the appropriate file. For example, if the user was running on one of the PDP-11/45's and typed the command <code>/bin/who</code> then the system would find that <code>/bin/who</code> was actually a hidden directory and run the command <code>/bin/who/45</code>. Another user on a VAX node who typed <code>/bin/who</code> would run the command <code>/bin/who/vax</code>. ===Devices=== LOCUS provided remote access to I/O devices. <!-- Yes, do go on --> ===Processes=== LOCUS provided a single process space. Processes could be created on any node on the system. Both the Unix [[Fork (operating system)|fork]] and [[exec (operating system)|exec]] calls would examine an ''advice list'' which determined on which node the process would be run. LOCUS was designed to work with heterogeneous nodes, (e.g. a mix of VAX 750s and PDP 11/45s) and could decide to execute a process on a different node if it needed a particular instruction set. As an optimization a ''run'' call was added which was equivalent to a combined fork and exec, thus avoiding the overhead of copying the process memory image to another node before overwriting it by the new image.<ref group="note">''run'' is the same operation as [[Spawn (computing)|spawn]] on [[Microsoft Windows|Windows]] systems.</ref> ===Pipes=== Processes could use [[Pipeline (Unix)|pipes]] for inter node communication, including [[named pipe]]s, ===Partitioning=== The LOCUS system was designed to be able to cope with [[network partitioning]] - one or more nodes becoming disconnected from the rest of the system. As the file system was [[Replication (computer science)|replicated]] the disconnected nodes could continue to access files. When the nodes were reconnected any files modified by the disconnected nodes would be merged back into the system. For some file types (for example mailboxes) the system would perform the merge automatically, for others the user would be informed (by mail) and tools were provided to allow access to the different versions of the file. ==Notes== <references group="note"/> ==References== <references/> [[Category:Proprietary operating systems]] [[Category:Cluster computing]]</text> </page> <page> <id>20999</id> <title>LPBoost</title> <text>'''Linear Programming Boosting''' ('''LPBoost''') is a [[supervised classification|supervised classifier]] from the [[Boosting]] family of classifiers. LPBoost maximizes a ''margin'' between training samples of different classes and hence also belongs to the class of margin-maximizing supervised classification algorithms. Consider a classification function :<math> f: \mathcal{X} \to \{ -1, 1 \}, </math> which classifies samples from a space <math>\mathcal{X}</math> into one of two classes, labelled 1 and -1, respectively. LPBoost is an algorithm to ''learn'' such a classification function given a set of training examples with known class labels. LPBoost is a [[machine learning]] technique and especially suited for applications of joint classification and feature selection in structured domains. == LPBoost overview == As in all Boosting classifiers, the final classification function is of the form :<math>f(\boldsymbol{x}) = \sum_{j=1}^{J} \alpha_j h_j(\boldsymbol{x}),</math> where <math>\alpha_j</math> are non-negative weightings for ''weak'' classifiers <math>h_j: \mathcal{X} \to \{-1,1\}</math>. Each individual weak classifier <math>h_j</math> may be just a little bit better than random, but the resulting linear combination of many weak classifiers can perform very well. LPBoost constructs <math>f</math> by starting with an empty set of weak classifiers. Iteratively, a single weak classifier to add to the set of considered weak classifiers is selected, added and all the weights <math>\boldsymbol{\alpha}</math> for the current set of weak classifiers are adjusted. This is repeated until no weak classifiers to add remain. The property that all classifier weights are adjusted in each iteration is known as ''totally-corrective'' property. Early Boosting methods, such as [[AdaBoost]] do not have this property and converge slower. == Linear program == More generally, let <math>\mathcal{H}=\{h(\cdot;\omega) | \omega \in \Omega\}</math> be the possibly infinite set of weak classifiers, also termed ''hypotheses''. One way to write down the problem LPBoost solves is as a [[linear program]] with infinitely many variables. The primal linear program of LPBoost, optimizing over the non-negative weight vector <math>\boldsymbol{\alpha}</math>, the non-negative vector <math>\boldsymbol{\xi}</math> of slack variables and the ''margin'' <math>\rho</math> is the following. :<math>\begin{array}{cl} \underset{\boldsymbol{\alpha},\boldsymbol{\xi},\rho}{\min} & -\rho + D \sum_{n=1}^{\ell} \xi_n\\ \textrm{sb.t.} & \sum_{\omega \in \Omega} y_n \alpha_{\omega} h(\boldsymbol{x}_n ; \omega) + \xi_n \geq \rho,\qquad n=1,\dots,\ell,\\ & \sum_{\omega \in \Omega} \alpha_{\omega} = 1,\\ & \xi_n \geq 0,\qquad n=1,\dots,\ell,\\ & \alpha_{\omega} \geq 0,\qquad \omega \in \Omega,\\ & \rho \in {\mathbb R}. \end{array}</math> Note the effects of slack variables <math>\boldsymbol{\xi} \geq 0</math>: their one-norm is penalized in the objective function by a constant factor <math>D</math>, which -- if small enough -- always leads to a primal feasible linear program. Here we adopted the notation of a parameter space <math>\Omega</math>, such that for a choice <math>\omega \in \Omega</math> the weak classifier <math>h(\cdot ; \omega): \mathcal{X} \to \{-1,1\}</math> is uniquely defined. When the above linear program was first written down in early publications about Boosting methods it was disregarded as intractable due to the large number of variables <math>\boldsymbol{\alpha}</math>. Only later it was discovered that such linear programs can indeed be solved efficiently using the classic technique of [[column generation]]. === Column Generation for LPBoost === In a [[linear program]] a ''column'' corresponds to a primal variable. [[Delayed Column Generation|Column generation]] is a technique to solve large linear programs. It typically works in a restricted problem, dealing only with a subset of variables. By generating primal variables iteratively and on-demand, eventually the original unrestricted problem with all variables is recovered. By cleverly choosing the columns to generate the problem can be solved such that while still guaranteeing the obtained solution to be optimal for the original full problem, only a small fraction of columns has to be created. ==== LPBoost dual problem ==== Columns in the primal linear program corresponds to 