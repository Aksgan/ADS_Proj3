had finished my enthusiastic speech, one of the most seasoned instructors in the audience raised his hand and made his own short speech, which began by conceding that positive reinforcement might be good for the birds, but went on to deny that it was optimal for flight cadets. He said, “On many occasions I have praised flight cadets for clean execution of some aerobatic maneuver, and in general when they try it again, they do worse. On the other hand, I have often screamed at cadets for bad execution, and in general they do better the next time. So please don’t tell us that reinforcement works and punishment does not, because the opposite is the case.” This was a joyous moment, in which I understood an important truth about the world: because we tend to reward others when they do well and punish them when they do badly, and because there is regression to the mean, it is part of the human condition that we are statistically punished for rewarding others and rewarded for punishing them. I immediately arranged a demonstration in which each participant tossed two coins at a target behind his back, without any feedback. We measured the distances from the target and could see that those who had done best the first time had mostly deteriorated on their second try, and vice versa. But I knew that this demonstration would not undo the effects of lifelong exposure to a perverse contingency.}} UK law enforcement policies have encouraged the visible sitting of static or mobile [[speed camera]]s at [[accident blackspot]]s. This policy was justified by a perception that there is a corresponding reduction in serious [[road traffic accidents]] after a camera is set up. However, statisticians have pointed out that, although there is a net benefit in lives saved, failure to take into account the effects of regression to the mean results in the beneficial effects being overstated. It is thus claimed that some of the money currently spent on traffic cameras could be more productively directed elsewhere.<ref>[http://www.timesonline.co.uk/tol/news/uk/article766659.ece The Times, 16 December 2005 Speed camera benefits overrated]</ref> Statistical analysts have long recognized the effect of regression to the mean in sports; they even have a special name for it: the “[[Sophomore slump|Sophomore Slump]]”. For example, [[Carmelo Anthony]] of the [[National Basketball Association|NBA]]’s [[Denver Nuggets]] had an outstanding rookie season in 2004. It was so outstanding, in fact, that he couldn’t possibly be expected to repeat it: in 2005, Anthony’s numbers had dropped from his rookie season. The reasons for the “sophomore slump” abound, as sports are all about adjustment and counter-adjustment, but luck-based excellence as a rookie is as good a reason as any. Regression to the mean in sports performance may be the reason for the “[[Sports Illustrated Cover Jinx]]” and the “[[Madden Curse]]”. [[John Hollinger]] has an alternate name for the phenomenon of regression to the mean: the “fluke rule”, while [[Bill James]] calls it the “Plexiglas Principle”. Because popular lore has focused on “regression toward the mean” as an account of declining performance of athletes from one season to the next, it has usually overlooked the fact that such regression can also account for improved performance. For example, if one looks at the [[batting average]] of [[Major League Baseball]] players in one season, those whose batting average was above the league mean tend to regress downward toward the mean the following year, while those whose batting average was below the mean tend to progress upward toward the mean the following year.<ref>For an illustration see [[Nate Silver]], “Randomness: Catch the Fever!”, [http://www.baseballprospectus.com/article.php?articleid=1897''[[Baseball Prospectus]]'', May 14, 2003].</ref> ==Definition for simple linear regression of data points== This is the definition of regression toward the mean that closely follows [[Sir Francis Galton]]'s original usage.<ref name="galton1886" /> Suppose there are ''n'' data points {''y''<sub>''i''</sub>, ''x''<sub>''i''</sub>}, where ''i'' = 1, 2, …, ''n''. We want to find the equation of the '''regression line''', ''i.e.'' the straight line : <math> y = \alpha + \beta x, \,</math> which would provide a <!-- maybe indefinite article here will aggravate some of the grammar purists, but it attempts to convey the idea that there could be many different ways to define "best" fit --> “best” fit for the data points. (Note that a straight line may not be the appropriate regression curve for the given data points.) Here the “best” will be understood as in the [[Ordinary least squares|least-squares]] approach: such a line that minimizes the sum of squared residuals of the linear regression model. In other words, numbers ''α'' and ''β'' solve the following minimization problem: : Find <math>\min_{\alpha,\,\beta}Q(\alpha,\beta)</math>, where <math>Q(\alpha,\beta) = \sum_{i=1}^n\hat{\varepsilon}_i^{\,2} = \sum_{i=1}^n (y_i - \alpha - \beta x_i)^2\ </math> Using simple [[calculus]] it can be shown that the values of ''α'' and ''β'' that minimize the objective function ''Q'' are : <math>\begin{align} & \hat\beta = \frac{ \sum_{i=1}^{n} (x_{i}-\bar{x})(y_{i}-\bar{y}) }{ \sum_{i=1}^{n} (x_{i}-\bar{x})^2 } = \frac{ \overline{xy} - \bar{x}\bar{y} }{ \overline{x^2} - \bar{x}^2 } = \frac{ \operatorname{Cov}[x,y] }{ \operatorname{Var}[x] } = r_{xy} \frac{s_y}{s_x}, \\ & \hat\alpha = \bar{y} - \hat\beta\,\bar{x}, \end{align}</math> where ''r<sub>xy</sub>'' is the [[Correlation#Sample_correlation|sample correlation coefficient]] between ''x'' and ''y'', ''s<sub>x</sub>'' is the [[standard deviation]] of ''x'', and ''s<sub>y</sub>'' is correspondingly the standard deviation of ''y''. Horizontal bar over a variable means the sample average of that variable. For example: <math style="height:1.5em">\overline{xy} = \tfrac{1}{n}\textstyle\sum_{i=1}^n x_iy_i\ .</math> Substituting the above expressions for <math>\hat\alpha</math> and <math>\hat\beta</math> into <math> y = \alpha + \beta x, \,</math> yields fitted values : <math> \hat y = \hat\alpha + \hat\beta x, \,</math> which yields : <math>\frac{ \hat y-\bar{y}}{s_y} = r_{xy} \frac{ x-\bar{x}}{s_x} </math> This shows the role ''r''<sub>''xy''</sub> plays in the regression line of standardized data points. If -1 < ''r''<sub>''xy''</sub> < 1 , then we say that the data points exhibit regression toward the mean. In other words, if linear regression is the appropriate model for a set of data points whose [[Correlation#Sample_correlation|sample correlation coefficient]] is not perfect, then there is regression toward the mean. The predicted (or fitted) standardized value of y is closer 