neither XFN nor FNS were ever popular nor widely used. FNS was last included in Solaris 9 and was not included with Solaris 10. ==External links and references== * [http://iforce.sun.com/protected/solaris10/adoptionkit/general/fns.txt Overview of FNS] (Solaris 9 [[man page]]) * [http://iforce.sun.com/protected/solaris10/adoptionkit/general/xfn.txt Overview of the XFN interface] (Solaris 9 man page) * [http://www.findarticles.com/p/articles/mi_m0HPJ/is_n6_v46/ai_17990730 X/Open Federated Naming - specification for uniform naming interfaces between multiple naming systems] (Elizabeth A. Martin, ''Hewlett-Packard Journal'', December 1995) * [http://docs.sun.com/app/docs/doc/802-1999/ Federated Naming Service Guide for Solaris 2.5] (Sun Microsystems) [[Category:Sun Microsystems software]] [[Category:Identity management]] [[Category:Solaris software]]</text> </page> <page> <id>13399</id> <title>Federation (information technology)</title> <text>A '''Federation''' is multiple computing and/or network providers agreeing upon standards of operation in a collective fashion. The term may be used when describing the inter-operation of two distinct, formally disconnected, [[telecommunications network]]s that may have different internal structures. The term may also be used when groups attempt to delegate collective authority of development to prevent [[Fork (software development)|fragmentation]]. In a telecommunication inter-connection, the internal ''[[modus operandi]]'' of the different systems is irrelevant to the existence of a federation. Joining two distinct networks: *[[Yahoo!]] and [[Microsoft]] announced that [[Yahoo! Messenger]] and [[MSN Messenger]] would be interoperable.<ref>{{cite news|url=http://www.betanews.com/article/Microsoft_Yahoo_to_Link_IM_Networks/1129075667|title=Microsoft, Yahoo to Link IM Networks|date=October 12, 2005|author=Nate Mook|publisher=[[Beta News]]}}</ref> Collective authority: *The [[MIT X Consortium]] was founded in 1988 to prevent fragmentation in development of the [[X Window System]]. Today every major platform besides Windows is distributed with largely compatible X server implementations.<ref>At least most [[GNU/Linux distribution]]s,<!--non-GNU systems rarely have X11--> [[UNIX]], [[BSD]], and [[Mac OS X|Mac OSX]]</ref> *[[OpenID]], a form of [[federated identity]] In networking systems, to be '''federated''' means users are able to send messages from one network to the other. This is not the same as having a client that can operate with both networks, but interacts with both independently. For example [[Google]] allowing [[GMail]] users to log into their [[AOL Instant Messenger]] (AIM) accounts from GMail is not federation. One cannot send messages from [[GTalk]] or [[Extensible Messaging and Presence Protocol|XMPP]] (which Google/GTalk '''is''' federated with) to AIM, and vice versa.<ref>{{cite web|url=https://mail.google.com/support/bin/answer.py?answer=61024|title=About AIM in Gmail|publisher=[[Google]]|date=2009-10-13}}</ref> ==See also== *[[Google Wave Federation Protocol]] ==References== <references/> [[Category:Interoperability]] [[Category:Interoperable communications]] {{Telecomm-stub}}</text> </page> <page> <id>13417</id> <title>Feedforward neural network</title> <text>[[Image:Feed forward neural net.gif|right|thumb|400px|In a feed forward network information always moves one direction; it never goes backwards.]] A '''feedforward neural network''' is an [[artificial neural network]] where connections between the units do ''not'' form a [[directed cycle]]. This is different from [[recurrent neural networks]]. The feedforward neural network was the first and arguably simplest type of artificial neural network devised. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network. ==Single-layer perceptron== {{main|Perceptron}} The earliest kind of neural network is a ''single-layer perceptron'' network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. In this way it can be considered the simplest kind of feed-forward network. The sum of the products of the weights and the inputs is calculated in each node, and if the value is above some threshold (typically 0) the neuron fires and takes the activated value (typically 1); otherwise it takes the deactivated value (typically -1). Neurons with this kind of [[activation function]] are also called ''[[Artificial neurons]]'' or ''linear threshold units''. In the literature the term ''[[perceptron]]'' often refers to networks consisting of just one of these units. A similar neuron was described by [[Warren McCulloch]] and [[Walter Pitts]] in the 1940s. A perceptron can be created using any values for the activated and deactivated states as long as the threshold value lies between the two. Most perceptrons have outputs of 1 or -1 with a threshold of 0 and there is some evidence {{Fact|date=April 2009}} that such networks can be trained more quickly than networks created from nodes with different activation and deactivation values. Perceptrons can be trained by a simple learning algorithm that is usually called the ''[[delta rule]]''. It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of [[gradient descent]]. Single-unit perceptrons are only capable of learning [[linearly separable]] patterns; in 1969 in a famous [[monograph]] entitled ''[[Perceptrons_(book)|Perceptrons]]'' [[Marvin Minsky]] and [[Seymour Papert]] showed that it was impossible for a single-layer perceptron network to learn an [[XOR function]]. It is often believed that they also conjectured (incorrectly) that a similar result would hold for a multi-layer perceptron network. However, this is not true, as both Minsky and Papert already knew that multi-layer perceptrons were capable of producing an XOR Function. (See the page on [[Perceptrons_(book)|Perceptrons]] for more information.) Although a single threshold unit is quite limited in its computational power, it has been shown that networks of parallel threshold units can approximate any continuous function from a compact interval of the real numbers into the interval [-1,1]. This very recent result can be found in [ [[Peter Auer]], [[Harald Burgsteiner]] and [[Wolfgang Maass]]: A learning rule for very simple universal approximators consisting of a single layer of perceptrons, 2008]. A multi-layer neural network can compute a continuous output instead of a [[step function]]. A common choice is the so-called [[logistic function]]: : <math>y = \frac{1}{1+e^{-x}}</math> (In general form, f(X) is in place of x, where f(X) is an [[analytic function]] in set of x's.) With this choice, the single-layer network is identical to the [[logistic regression]] model, widely used in [[statistical model]]ing. The [[logistic function]] is also known as the [[sigmoid function]]. It has a continuous derivative, which allows it to be used in backpropagation. This function is also preferred because its derivative is easily calculated: : <math>y' = y(1-y)</math> (times <math>df/dX</math>, in general form, according to the [[Chain Rule]]) ==Multi-layer perceptron== {{main|Multilayer perceptron}} [[Image:XOR perceptron net.png|thumb|right|250px|A two-layer neural network capable of calculating XOR. The numbers within the neurons represent each neuron's explicit threshold (which can be factored out so 