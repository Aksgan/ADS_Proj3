number of total values ought to be known * '''Uniqueness''': Related to the number of duplicates in the data == The process of data cleansing == * '''Data Auditing''': The data is audited with the use of [[statistical]] methods to detect anomalies and contradictions. This eventually gives an indication of the characteristics of the anomalies and their locations. * '''Workflow specification''': The detection and removal of anomalies is performed by a sequence of operations on the data known as the workflow. It is specified after the process of auditing the data and is crucial in achieving the end product of high quality data. In order to achieve a proper workflow, the causes of the anomalies and errors in the data have to be closely considered. If for instance we find that an anomaly is a result of typing errors in data input stages, the layout of the [[Keyboard (computing)|keyboard]] can help in manifesting possible solutions. * '''Workflow execution''': In this stage, the workflow is executed after its specification is complete and its correctness is verified. The implementation of the workflow should be efficient even on large sets of data which inevitably poses a trade-off because the execution of a data cleansing operation can be computationally expensive. * '''Post-Processing and Controlling''': After executing the cleansing workflow, the results are inspected to verify correctness. Data that could not be corrected during execution of the workflow are manually corrected if possible. The result is a new cycle in the data cleansing process where the data is audited again to allow the specification of an additional workflow to further cleanse the data by automatic processing. == Popular methods used == * '''Parsing''': [[Parsing]] in data cleansing is performed for the detection of syntax errors. A parser decides whether a string of data is acceptable within the allowed data specification. This is similar to the way a parser works with [[grammars]] and [[languages]]. * '''Data Transformation''': Data Transformation allows the mapping of the data from their given format into the format expected by the appropriate application. This includes value conversions or translation functions as well as normalizing numeric values to conform to minimum and maximum values. * '''Duplicate Elimination''': Duplicate detection requires an [[algorithm]] for determining whether data contains duplicate representations of the same entity. Usually, data is sorted by a key that would bring duplicate entries closer together for faster identification. * '''Statistical Methods''': By analyzing the data using the values of [[mean]], [[standard deviation]], [[range (statistics)|range]], or [[clustering]] algorithms, it is possible for an expert to find values that are unexpected and thus erroneous. Although the correction of such data is difficult since the true value is not known, it can be resolved by setting the values to an average or other statistical value. Statistical methods can also be used to handle missing values which can be replaced by one or more plausible values that are usually obtained by extensive data augmentation algorithms. == Existing tools == Before computer automation data about individuals or organizations were maintained and secured as paper records, dispersed in separate business or organizational units. Information systems concentrate data in computer files that can potentially be accessed by large numbers of people and by groups outside of organization. == Challenges and problems == * '''Error Correction and loss of information''': The most challenging problem within data cleansing remains the correction of values to remove duplicates and invalid entries. In many cases, the available information on such anomalies is limited and insufficient to determine the necessary transformations or corrections leaving the deletion of such entries as the only plausible solution. The deletion of data though, leads to loss of information which can be particularly costly if there is a large amount of deleted data. * '''Maintenance of Cleansed Data''': Data cleansing is an expensive and time consuming process. So after having performed data cleansing and achieving a data collection free of errors, one would want to avoid the re-cleansing of data in its entirety after some values in data collection change. The process should only be repeated on values that have changed which means that a cleansing lineage would need to be kept which would require efficient data collection and management techniques. * '''Data Cleansing in Virtually Integrated Environments''': In virtually integrated sources like [[IBM]]â€™s DiscoveryLink, the cleansing of data has to be performed every time the data is accessed which considerably decreases the response time and efficiency. * '''Data Cleansing Framework''': In many cases it will not be possible to derive a complete data cleansing graph to guide the process in advance. This makes data cleansing an [[iterative]] process involving significant exploration and interaction which may require a framework in the form of a collection of methods for error detection and elimination in addition to data auditing. This can be integrated with other data processing stages like integration and maintenance. == See also == * [[Extract, transform, load]] (ETL) * [[Data quality]] * [[Data quality assurance]] * [[Record linkage]] == References == {{reflist|2}} ==Sources== * [[Jiawei Han|Han, J.]], Kamber, M. ''Data Mining: Concepts and Techniques'', Morgan Kaufmann, 2001. ISBN 1-55860-489-8. * Kimball, R., Caserta, J. ''The Data Warehouse ETL Toolkit'', Wiley and Sons, 2004. ISBN 0-7645-6757-8. * Muller H., Freytag J., ''Problems, Methods, and Challenges in Comprehensive Data Cleansing'', Humboldt-Universitat zu Berlin, Germany. * Rahm, E., Hong, H. ''Data Cleaning: Problems and Current Approaches'', University of Leipzig, Germany. == External links == *[http://www.wisegeek.com/what-is-data-cleansing.htm What is Data Cleansing?] *[http://www.computerworld.com/action/article.do?command=viewArticleBasic&articleId=78230 ''Computerworld: Data Scrubbing''] (February 10, 2003) {{DEFAULTSORT:Data Cleansing}} [[Category:Data management]] [[Category:Business intelligence]] [[de:Datenbereinigung]] [[es:Limpieza de datos]]</text> </page> <page> <id>9351</id> <title>Data compression</title> <text>{{redirect|Source coding|the term in computer programming|Source code}} In [[computer science]] and [[information theory]], '''data compression''' or '''source coding''' is the process of [[encoding]] [[information]] using fewer [[bit]]s (or other information-bearing units) than an [[code|unencoded]] representation would use, through use of specific [[encoding]] schemes. In computing, [[data deduplication]] is a specialized data compression technique for eliminating coarse-grained redundant data, typically to improve storage utilization. Compression is useful because it helps reduce 